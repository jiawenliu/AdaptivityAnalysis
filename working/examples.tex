%
We present four examples, illustrating $\THESYSTEM$.
% The first example is an .
% The second examples is an data analysis algorithm used for data-base attack in real-world.
% The last example example is a path-sensitive algorithm, where the $\THESYSTEM$ analysis result is over-approximated. 
Then we show our implementation of $\THESYSTEM$ and its experimental results on $18$ examples including these four examples.
%
\subsection{Examples}
%
\begin{example}[Multiple Rounds Algorithm]
\label{ex:multiplerounds}
%
We look at an advanced adaptive data analysis algorithm - multiple rounds algorithm, as in Figure~\ref{fig:multi_graphs}(a).
%
%
%   We have seen the two round algorithm in Section~\ref{subsec:loop-syntax}. We show the multiple-round algorithm, which is an advanced algorithm.
%  \\
%
% The multiple rounds algorithm starts from an initialized empty tracking list $I$, two scores called Nscore $ns=0$ and Cscore $cs=0$, initialzied to $0$. It goes $k$ rounds and at every round, the two scores $ns$ and $cs$ are updated by the result $a$ of a query $q(f(I))$. The function $f( I)$ specifies a complex linear query using the updated tracking list $I$. The tracking list $I$ is updated by the two scores via a function $update(I,ns,cs)$ at every round. This update function mainly compares $ns$ and $cs$, when $ns \geq cs$, certain elements are added to the tracking list $I$. An implementation of the algorithm is presented in Figure~\ref{fig:multi_code}(a), in which the round number $k$ are set to $3$, and we use $update\_cscore(a)$ and $update\_nscore(a)$ to simplify the complex update on Cscore and Nscore respectively, for the sake of simplicity.
% The multiple round algorithm is presented in Figure~\ref{fig:multi_graphs}(a).
% It starts from an initialized empty tracking list $I$, a score called Nscore $ns=0$, another score Cscore $cs=0$.
% with a hidden database $D$.
% % a score called Nscore $ns=0$ , another score Cscore $cs=0$. There is a hidden database $X$ as well.
% % It goes $k$ rounds and every round, the two scores $ns$ and $cs$ are updated by a query result. 
% % Then the list $I$ is updated by the two scores for every round. After the $r$ rounds, the algorithm returns the columns of the hidden database $X$ not specified in the tracking list $I$, which is $X\setminus I$. 
% It goes $k$ rounds and at every round, the two scores $ns$ and $cs$ are updated by a query result. 
% Then the tracking list is updated by the two scores for every round.  
% % Then the list $I$ is updated by the two scores for every round. 
% After the $r$ rounds, the algorithm returns the columns of the hidden database $D$ not specified in the tracking list $I$, which is $D \setminus I$. 
% \\
% The algorithm is written in the {\tt Query While} language as $\kw{multipleRounds(k)} $ taking 
% two parameters $k$ and $c$ for 
% number of iterations and the distribution sampling primitive $c$.
It takes the user input $k$ which decides the 
number of iterations.
% and the distribution sampling primitive $c$.
It starts from an initialized empty tracking list $I$,
% a score called Nscore $ns=0$ , another score Cscore $cs=0$. There is a hidden database $X$ as well.
% It goes $k$ rounds and every round, the two scores $ns$ and $cs$ are updated by a query result. 
% Then the list $I$ is updated by the two scores for every round. After the $r$ rounds, the algorithm returns the columns of the hidden database $X$ not specified in the tracking list $I$, which is $X\setminus I$. 
{ goes $k$ rounds and at every round, tracking list $I$ is updated by a query result of $\query(\chi[I])$.
% Then the list $I$ is updated by the two scores for every round. 
After $r$ rounds, the algorithm returns the columns of the hidden database $D$ not specified in the tracking list $I$.
% The $\mathrel{\mathsf{update}} ( {I}, (a, p))$ function takes $I, a, p$ as input and compute the updated results for $I$.
% $\mathsf{update}$ function is used here to simplify the complex update computation of Nscore, Cscore and the tracking list $I$.
We use functions $\kw{updnscore}(p,a)$,
$\kw{updcscore}(p,a)$,$\kw{update}(I,ns,cs)$ to simplify the complex update computations of $Nscore$, $Cscore$ and the tracking list $I$, 
which will not affect our analysis.%
}
% It uses a loop for the $k$ rounds computation and. 
% We use functions $\kw{updnscore}(p,a)$,
% $\kw{updcscore}(p,a)$,$\kw{update}(I,ns,cs)$ to simplify the complex update computation of Nscore, Cscore and the tracking list $I$. It will not change our analysis because these functions provides enough information through their arguments.
% As described in the two round algorithm, the multi-round algorithm has a loop as well.
% compare to two round algorithm

% and the tracking list $I$. It will not change our analysis because these functions provides enough information through their arguments.
% As described in the two round algorithm, the multi-round algorithm has a loop as well.
% compare to two round algorithm
{The interesting part here is the query asked in each iteration is not independent any more. 
The query in one iteration $j$ now depends on the tracking list $I$ from its previous iteration $j-1$, which is updated by the query result in the same iteration $j-1$. The connection between queries from different iterations, 
 which means these queries are adaptively chosen according to our discussion in overview.
}
% in comparison with the two rounds one, is that the query asked in each iteration is not independent(non-adaptive) anymore.
% For example, the query $q^{j}$ at iteration $j$ now may depend on the tracking list $I$, which comes from the previous iteration $j-1$. Additionally, this list $I$ at iteration $j-1$ is updated by the query result $q^{j-1}$ at the same iteration. Intuitively, we can see the connection between queries from different iterations, which means these queries are adaptively chosen according to our Theorem~\ref{thm:gaussiannoise2}.

% the result of the query from previous iteration,
% so that the query ask at the $j^{th}$ iteration is
% $q(p, I)$.
%
% In $MR$, the tracking list $I$ is initialized to an empty list. It appears inside the function of query $q(f(p,I))$ and updated in each iteration. 
% by the result of query in that iteration. It uses an update function $\eupdt$. 
% The input of this function is $a, p$, where $a$ is the result of the query in current iteration.
% \\
% By assuming a specific database $D = [[1, 1], [0, 0], [1, 1], [1, 1]]$,
% \todo{The adaptivity through dependency graph}
% \jl{
% We first show its query-based dependency graph in Figure~\ref{fig:multi_graphs}(a), the execution trace $t_{mr}$ is generated as follows.}

The program-based dependency graph is presented  in Figure~\ref{fig:multi_graphs}(b). Its execution-based dependency graph has the same graph, except different weight so we do not show it again. We can simply replaces $k$ with a function $w_k$ which takes a trace and returns the value of $k$ in this trace. The weight $1$ is replaced as a constant function $w_1$ taking whatever trace and returns $1$ for the execution-based dependency graph. For consistence, we use $w_k$ and $w_1$ for all the examples in this section.
% Each vertex corresponds to a labeled variable in program,
% and annotated with its weights and query annotation. 
% For example the vertex on the top of the graph, $a^6:{}^k_1$
% corresponds
% to the variable $a$ assigned by the result of the query request in the labeled command
% $\clabel{\assign{a}{\query(I)}}^6$ at line 6 with weight $k$ and query annotation $1$.
% The same for other vertices.
% The edges are constructed by the dependency relation 
% checking all the possible program execution trace.
% For example taking an arbitrary initial trace,
% $\trace_0 = [(k, in, K, \bullet)]$, where $k$ is the 
% initial value of input variable $k$ given by user,
% we observe the execution trace as
% $
% % \trace_0 \tracecat
% [ 
% (j, 0, K, \bullet),
% % (I, 1, [], \bullet),
% % (ns, 2, 0, \bullet),
% % (cs, 3, 0, \bullet),
% \cdots,
% (j>0, 4, \etrue, \bullet),
% (j, 5, 1, \bullet),
% (a, 6, v_1, []),
% \cdots,
% (I, 9, [1], \bullet),
% \cdots,
% (a, 6, v_2, [1]),
% \cdots,
% (I, 9, [1,1], \bullet), 
% (j>0, 4, \efalse, \bullet)
% ]$.
% Then, by modifying the event $(I, 9, [1], \bullet)$ into 
% $(I, 9, [1, 0], \bullet)$ in the first loop iteration, 
%  and continuously executing the next command, 
%  we 
%  observe this execution,
% % trace 
% we obtain another execution trace:
% $
% % \trace_0 \tracecat
% [ 
% (j, 0, 2, \bullet),
% % (I, 1, [], \bullet),
% % (ns, 2, 0, \bullet),
% % (cs, 3, 0, \bullet),
% \cdots,
% (j>0, 4, \etrue, \bullet),
% (j, 5, 1, \bullet),
% (a, 6, v_1, []),
% \cdots,
% (I, 9, [1], \bullet),
% \cdots,
% (a, 6, v_2', [1, 0]),
% \cdots,
% (I, 9, [1,1], \bullet),
% (j>0, 4, \efalse, \bullet)
% ]$.
% In the two traces, the value assigned to $a$ at line 6 changed from the event $(a, 6, v_2, [1])$ into $(a, 6, v_2', [1, 0])$.
% So we construct the directed edge from $a^6$ to $I^9$ and same way for all the other edges.
% For the weight, we observe the occurrence time of the label for each 
% labeled variable over all possible execution traces.
% % Given $k \in \mathbb{N}$, we observe the infinite  
% For labeled variables $j^0$, $I^1$, $ns^2$ and $cs^3$,
% which are not involved in any while loop,
% for any initial trace $\trace_0 \in \mathcal{T}$, 
% these labeled command will be evaluated at most once.
% % we observe only one occurrence time 
% % over all possible execution trace.
% So we assign these labeled variable
% with weight $1: \trace_0 \to 1$, as their superscript 
% on the graph in Figure~\ref{fig:multi_graphs}(b).
% In the same way for labeled variables $j^5$, $a^6$, $ns^7$, $cs^8$ and $I^9$,
% which are involved in while loop,
% % given the initial value $K$ for input $k$, 
% % we observe $K$ occurrence times
% % for labels inside the loop.
% % So, 
% we assign the labeled variables 
%  of the weight $k$. We abuse the notation $k$ as a function, such that for an initial trace $\trace_0 \in \mathcal{T}$,
%  $k(\trace_0) = \env(\trace_0) k$.
% %  in its 
% % execution-based dependency graph, 
% as shown in the superscript on these vertices
% in Figure~\ref{fig:multi_graphs}(b).
As the adaptivity definition in our formal adaptivity model in Definition~\ref{def:trace_adapt},
there is a finite walk along the dashed arrows,
$a^{6} \to I^9 \to ns^{7} \to  \cdots \to ns^7$ , 
where every vertex is visited $w_k(\trace_0)$ times for an initial trace $\trace_0 \in \mathcal{T}_0(c)$.
There is one vertex $a^{6}$ visited $w_k(\trace_0)$ times with query annotation 1, 
So we have the adaptivity with $\trace_0$ for this program as $w_k(\trace_0)$.

{
Next, we show {$\THESYSTEM$} providing the tight upper bound for this example.
% variable-based weighted dependency graph in Figure\ref{fig:multi_graphs}(b). We use a short in the graph, such as $a_1^{3}$ for $a_1^{(5, [4:3])}$ and so on. We show the most weighted path in the graph, which is the red dashed path as usual. Along the red dashed path, $3$ weighted nodes $a_1^{3},a_1^{2},a_1^{1} $, correspond to our queries $q_c, q_b$ and $q_a$ respectively. This is our intuition to estimate one graph in Figure~\ref{fig:multi_graphs}(b), to upper bound another graph(Figure~\ref{fig:multi_graphs})(a). Here, we simplify the estimated graph by omitting some variables such $ns_1$, $cs_1$ in  Figure~\ref{fig:multi_graphs}(b).  Every query node in the query-based dependency graph has a corresponding node(variable the query is associated) in the variable-based dependency graph generated by our analysis algorithm {\THESYSTEM}. 
% program-based dependency graph Graph as an approximation of the graph in Figure~\ref{fig:multi_graphs}(b).
% We omit the program-based dependency graph Graph for this example, because it 
% has identical vertices, edges and query annotation to the  execution-based dependency graph in Figure~\ref{fig:multi_graphs}(b),
% % except using the initial value $K$ as weights rather than 
% % except having 
% as well as the symbolic input variable $k$ 
% % rather than its initial value $K$ 
% as weights for 
% the vertices involved inside while loop, specifically, $j^0$, $I^1$, $ns^2$ and $cs^3$.
% as shown in the superscript on the vertex.
% ant this graph has identical topology to the Execution-Based dependency graph as in Figure\ref{fig:multi_graphs}(b). 
% We use a short in the graph, such as $a_1^{3}$ for $a_1^{(5, [4:3])}$ and so on. We show the most weighted path in the graph, which is the red dashed path as usual. 
If first finds a path  
% along 
$a^{6}: {}^k_1 \to I^9:{}^k_0 \to ns^7:{}^k_0$ with three weighted vertices, and then $\pathsearch$ approximate this path to a walk, in which $a^6,I^9, ns^{7}$ is visited $k$ times. So the estimated adaptivity is $k$. We know for any initial trace $\trace_0$ where $\config{\trace_0, k} \earrow v$ and 
$w_k(\trace_0) = v$. So $k$ from {$\THESYSTEM$} is a tight bound.
% correspond to our queries $q_c, q_b$ and $q_a$ respectively. 
% This is our intuition to estimate one graph in Figure~\ref{fig:multi_graphs}(b), to upper bound another graph(Figure~\ref{fig:multi_graphs})(a). 
% Here, we simplify the estimated graph by omitting some variables such $ns_1$, $cs_1$ in  Figure~\ref{fig:multi_graphs}(b).  
% Every query node in the query-based dependency graph has a corresponding node(variable the query is associated) in the variable-based dependency graph generated by our analysis algorithm {\THESYSTEM}. 
% And this path corresponds to the finite walk where 
% and every vertex is visited $w$ times where $\config{\trace_0, k} \earrow w$,
% is the longest finite walk with the 
% maximal query length.
% % Then, by summing up the number of query vertices showing up in this walk,
% % the query length is $k$, where $k$ is the program's adaptivity.
% % we have the maximal query length 
% $\THESYSTEM$ computes $k$ as upper bound for program's adaptivity $k$ and we have
}
\end{example}

%
\begin{figure}
\centering
\begin{subfigure}{0.25\textwidth}
    \small{
    $
\begin{array}{l}
\kw{multipleRounds(k, c)} \triangleq\\
    \clabel{\assign{j}{k}}^0;
    \clabel{\assign{I}{[]}}^1; \\
    \clabel{\assign{ns}{0}}^2; 
    \clabel{\assign{cs}{0}}^3; \\
    \ewhile ~ \clabel{j > 0}^{4} ~ \edo ~ \\
    \Big(
    \clabel{\assign{j}{j-1}}^{5} ;
    \clabel{\assign{a}{\query(I)}}^6; \\
    \clabel{\assign{ns}{\kw{updnscore}(ns, a)}}^7; \\
    \clabel{\assign{cs}{\kw{updcscore}(cs, a)}}^8; \\
    \clabel{\assign{I}{\kw{updI}(I, ns, cs)}}^9
    \Big) 
\end{array}
    $
    }
    \caption{}
\end{subfigure}
%
    % \begin{subfigure}{.3\textwidth}
    %     \begin{centering}
    %  %   \todo{abstract-cfg for two round}
    %  \begin{tikzpicture}[scale=\textwidth/18cm,samples=200]
    %  \draw[] (-5, 10) circle (0pt) node{{ $0$}};
    %  \draw[] (0, 10) circle (0pt) node{{ $1$}};
    %  \draw[] (0, 7) circle (0pt) node{\textbf{$2$}};
    %  \draw[] (0, 4) circle (0pt) node{{ $3$}};
    %  \draw[] (0, 1) circle (0pt) node{{ $4$}};
    %  \draw[] (-5, 1) circle (0pt) node{{ $5$}};
    %  % Counter Variables
    %  \draw[] (3, 7) circle (0pt) node {\textbf{$6$}};
    %  \draw[] (3, 4) circle (0pt) node {{ $ex$}};
    %  %
    %  % Control Flow Edges:
    %  \draw[ thick, -latex] (-4.5, 10)  -- node [above] {$a \leq 0$}(-0.5, 10);
    %  \draw[ thick, -latex] (0, 9.5)  -- node [left] {$j \leq k$} (0, 7.5) ;
    %  \draw[ thick, -latex] (0, 6.5)  -- node [left] {$\top$}  (0, 4.5);
    %  \draw[ thick, -latex] (0, 3.5)  -- node [right] {$x \leq \max(\dbdom)$} (0, 1.5) ;
    %  \draw[ thick, -latex] (0, 1)  -- node [above] {$j \leq j - 1$} (-4.5, 1) ;
    %  \draw[ thick, -latex] (-5, 1.5)  -- node [left] {$a \leq a + x$} (-0.5, 7)  ;
    %  \draw[ thick, -latex] (0.5, 7)  -- node [above] {$\top$}  (2.5, 7);
    %  \draw[ thick, -latex] (3, 6.5)  -- node [left] {$\top$} (3, 4.5) ;
    %  \end{tikzpicture}
    %   \caption{}
    %     \end{centering}
    %     \end{subfigure}
%      \begin{subfigure}{.3\textwidth}
%         \begin{centering}
%         \begin{tikzpicture}[scale=\textwidth/25cm,samples=200]
% % Variables Initialization
% \draw[] (-5, 1) circle (0pt) node{{ $I^1: {}^{w_1}_{0}$}};
% \draw[] (-5, 7) circle (0pt) node{{$ns^2: {}^{w_1}_{0}$}};
% \draw[] (-5, 4) circle (0pt) node{{ $cs^3: {}^{w_1}_{0}$}};
% % Variables Inside the Loop
%      \draw[] (0, 10) circle (0pt) node{{ $a^6: {}^{w}_{1}$}};
%      \draw[] (0, 7) circle (0pt) node{{ $ns^7: {}^{w}_{0}$}};
%      \draw[] (0, 4) circle (0pt) node{{ $cs^8: {}^{w}_{0}$}};
%      \draw[] (0, 1) circle (0pt) node{{ $I^9: {}^{w}_{0}$}};
%      % Counter Variables
%      \draw[] (5, 9) circle (0pt) node {{$j^0: {}^{w_1}_{0}$}};
%      \draw[] (5, 6) circle (0pt) node {{ $j^5: {}^{w}_{0}$}};
%      %
%      % Value Dependency Edges:
%      \draw[ thick, -latex,] (0, 1.5)  -- (0, 3.5) ;
%      \draw[ ultra thick, -latex, densely dotted,] (0, 7.5)  -- (0, 9.5) ;
%      \draw[ thick, -Straight Barb] (1., 4) arc (120:-200:1);
%      \draw[ thick, -Straight Barb] (6., 6.5) arc (150:-150:1);
%      \draw[ thick, -Straight Barb] (1, 7.5) arc (220:-100:1);
%      \draw[ thick, -latex] (5, 6.5)  -- (5, 8.5) ;
%      % Value Dependency Edges on Initial Values:
%      \draw[ thick, -latex,] (-1, 1)  -- (-4.5, 1) ;
%      \draw[ thick, -latex,] (-1, 4)  -- (-4.5, 4) ;
%      \draw[ thick, -latex,] (-1, 7)  -- (-4.5, 7) ;
%      %
%      \draw[ ultra thick, -latex, densely dotted,] (-0.5, 9.5)  to  [out=-130,in=130]  (-0.5, 1);
%      \draw[ ultra thick, -latex, densely dotted,] (-0.5, 1.5)  to  [out=-250,in=250]  (-0.5, 6.5);
%      % Control Dependency
%     %  \draw[ thick,-latex] (1.5, 7)  -- (4, 9) ;
%     %  \draw[ thick,-latex] (1.5, 4)  -- (4, 9) ;
%      \draw[ thick,-latex] (1.5, 7)  -- (4, 6) ;
%      \draw[ thick,-latex] (1.5, 4)  -- (4, 6) ;
%      \draw[ thick,-latex] (1.5, 1)  -- (4, 6) ;
%      \draw[ thick,-latex] (1.5, 10)  -- (4, 6) ;
%      \end{tikzpicture}
%      \caption{}
%         \end{centering}
%         \end{subfigure}
        \begin{subfigure}{.6\textwidth}
        \begin{centering}
        \begin{tikzpicture}[scale=\textwidth/25cm,samples=200]
% Variables Initialization
\draw[] (-7, 1) circle (0pt) node{{ $I^1: {}^1_{0}$}};
\draw[] (-7, 7) circle (0pt) node{{$ns^2: {}^{1}_{0}$}};
\draw[] (-7, 4) circle (0pt) node{{ $cs^3: {}^{1}_{0}$}};
% Variables Inside the Loop
     \draw[] (0, 10) circle (0pt) node{{ $a^6: {}^{k}_{1}$}};
     \draw[] (0, 7) circle (0pt) node{{ $ns^7: {}^{k}_{0}$}};
     \draw[] (0, 4) circle (0pt) node{{ $cs^8: {}^{k}_{0}$}};
     \draw[] (0, 1) circle (0pt) node{{ $I^9: {}^{k}_{0}$}};
     % Counter Variables
     \draw[] (7, 9) circle (0pt) node {{$j^0: {}^{1}_{0}$}};
     \draw[] (7, 6) circle (0pt) node {{ $j^5: {}^{k}_{0}$}};
     %
     % Value Dependency Edges:
     \draw[ thick, -latex,] (0, 1.5)  -- (0, 3.5) ;
     \draw[ ultra thick, -latex, densely dotted,] (0, 7.5)  -- (0, 9.5) ;
     \draw[ thick, -Straight Barb] (1.4, 4) arc (120:-200:1);
     \draw[ thick, -Straight Barb] (8.5, 6.5) arc (150:-150:1);
     \draw[ thick, -Straight Barb] (1, 7.5) arc (220:-100:1);
     \draw[ thick, -latex] (7, 6.5)  -- (7, 8.5) ;
     % Value Dependency Edges on Initial Values:
     \draw[ thick, -latex,] (-1.5, 1)  -- (-5.5, 1) ;
     \draw[ thick, -latex,] (-1.5, 4)  -- (-5.5, 4) ;
     \draw[ thick, -latex,] (-1.5, 7)  -- (-5.5, 7) ;
     %
     \draw[ ultra thick, -latex, densely dotted,] (-1, 9.5)  to  [out=-130,in=130]  (-1, 1.5);
     \draw[ ultra thick, -latex, densely dotted,] (-0.8, 1.7)  to  [out=-230,in=230]  (-0.5, 6.5);
     % Control Dependency
    %  \draw[ thick,-latex] (1.5, 7)  -- (4, 9) ;
    %  \draw[ thick,-latex] (1.5, 4)  -- (4, 9) ;
     \draw[ thick,-latex] (1.5, 7)  -- (5.8, 6) ;
     \draw[ thick,-latex] (1.5, 4)  -- (5.8, 6) ;
     \draw[ thick,-latex] (1.5, 1)  -- (5.8, 6) ;
     \draw[ thick,-latex] (1.5, 10)  -- (5.8, 6) ;
     \end{tikzpicture}
     \caption{}
        \end{centering}
        \end{subfigure}
    \vspace{-0.4cm}
    \caption{(a) The simplified multiple rounds example (b) The program-based dependency graph from $\THESYSTEM$}
    \vspace{-0.5cm}
    \label{fig:multi_graphs}
\end{figure}
%
\begin{example}[Linear Regression Algorithm with Gradient Decent Optimization]
\label{ex:linearregression}
    The linear regression algorithm with gradient decent Optimization works well 
    in our $\THESYSTEM$ as well.
            %   \[
            %   %
            %   \begin{array}{l}
            %   \kw{linearRegression(step, rate)} \triangleq \\
            %          \clabel{ a \leftarrow 0}^{0} ; \\
            %          \clabel{ c \leftarrow 0}^{1} ; \\
            %           \clabel{\assign{j}{\kw{step}} }^{2} ; \\
            %         %   \clabel{\assign{d}{10000000} }^{2} ; \\
            %           \ewhile ~ \clabel{j > 0}^{3} ~ \edo ~ \\
            %           \Big(
            %               \clabel{\assign{da}{\query(-2 * (\chi[1] - (\chi[0]\times a + c)) \times (\chi[0]))} }^{4}  ; \\
            %               \clabel{\assign{dc}{\query(-2 * (\chi[1] - (\chi[0]\times a + c)))} }^{5}  ; \\
            %               \clabel{\assign{a}{a - \kw{rate} * da} }^{6}  ; \\
            %               \clabel{\assign{c}{c - \kw{rate} * dc} }^{7}  ; \\
            %            \clabel{\assign{j}{j-1}}^{8} 
            %         %   \clabel{a \leftarrow x :: a}^{6} 
            %           \Big);
            %       \end{array}
            %   \]
              %
              %
                   %
\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
    \centering
    {\small
        \[
        \begin{array}{l}
            \kw{linearRegressionGD(k, rate)} \triangleq \\
                   \clabel{ a \leftarrow 0}^{0} ; 
                   \clabel{ c \leftarrow 0}^{1} ; 
                    \clabel{\assign{j}{\kw{k}} }^{2} ; \\
                  %   \clabel{\assign{d}{10000000} }^{2} ; \\
                    \ewhile ~ \clabel{j > 0}^{3} ~ \edo ~ \\
                    \Big(
                        \clabel{\assign{da}{\query(-2 * (\chi[1] - (\chi[0]\times a + c)) \times (\chi[0]))} }^{4}  ; \\
                        \clabel{\assign{dc}{\query(-2 * (\chi[1] - (\chi[0]\times a + c)))} }^{5}  ; \\
                        \clabel{\assign{a}{a - \kw{rate} * da} }^{6}  ; 
                        \clabel{\assign{c}{c - \kw{rate} * dc} }^{7}  ; \\
                     \clabel{\assign{j}{j-1}}^{8} 
                  %   \clabel{a \leftarrow x :: a}^{6} 
                    \Big);
                \end{array}
        \]
        }
     \caption{}
        \end{subfigure}
        \begin{subfigure}{.5\textwidth}
            \begin{centering}
            \begin{tikzpicture}[scale=\textwidth/25cm,samples=200]
    % Variables Initialization
    \draw[] (-6, 1) circle (0pt) node{{ $a^0: {}^1_{0}$}};
    \draw[] (-6, 4) circle (0pt) node{{ $c^1: {}^{1}_{0}$}};
    % Variables Inside the Loop
         \draw[] (0, 10) circle (0pt) node{{ $da^4: {}^{k}_{1}$}};
         \draw[] (0, 7) circle (0pt) node{{ $dc^5: {}^{k}_{0}$}};
         \draw[] (0, 4) circle (0pt) node{{ $a^6: {}^{k}_{0}$}};
         \draw[] (0, 1) circle (0pt) node{{ $c^7: {}^{k}_{0}$}};
         % Counter Variables
         \draw[] (7, 9) circle (0pt) node {{$j^0: {}^{1}_{0}$}};
         \draw[] (7, 6) circle (0pt) node {{ $j^8: {}^{k}_{0}$}};
         %
         % Value Dependency Edges:
         \draw[ thick, -latex,] (0, 1.5)  -- (0, 3.5) ;
         \draw[ thick, -Straight Barb] (1.8, 4.2) arc (220:-100:1);
         \draw[ thick, -Straight Barb] (7.5, 6.5) arc (150:-150:1);
         \draw[ thick, -latex] (6, 6.5)  -- (6, 8.5) ;
         \draw[ thick, -Straight Barb] (1.7, 1.) arc (120:-200:1);
         % Value Dependency Edges on Initial Values:
         \draw[ thick, -latex,] (-2, 1)  -- (-4.5, 1) ;
         \draw[ thick, -latex,] (-2, 4)  -- (-4.5, 4) ;
         %
         \draw[ ultra thick, -latex, densely dotted,] (-1, 1.5)  to  [out=-220,in=220]  (-1, 6.5);
         \draw[ ultra thick, -latex, densely dotted,] (-1, 4.5)  to  [out=-220,in=220]  (-1, 9.5);
         \draw[ ultra thick, -latex, densely dotted,]  (1, 6.2) to  [out=-60,in=60] (0.5, 1.5) ;
         \draw[ ultra thick, -latex, densely dotted,]  (1.2, 9.2)  to  [out=-50,in=50] (0.5, 4.5);
         % Control Dependency
        %  \draw[ thick,-latex] (1.5, 7)  -- (4, 9) ;
        %  \draw[ thick,-latex] (1.5, 4)  -- (4, 9) ;
         \draw[ thick,-latex] (1.8, 7)  -- (5.5, 6) ;
         \draw[ thick,-latex] (1.8, 4)  -- (5.5, 6) ;
         \draw[ thick,-latex] (1.8, 1)  -- (5.5, 6) ;
         \draw[ thick,-latex] (1.8, 10)  -- (5.5, 6) ;
         \end{tikzpicture}
         \caption{}
            \end{centering}
            \end{subfigure}
    \vspace{-0.5cm}
    \caption{(a) The linear regression algorithm 
    (b) The program-based dependency graph from $\THESYSTEM$}
    \vspace{-0.5cm}
    \label{fig:linear_regression}
\end{figure}
%
Analysis Result: $ \progA(\kw{linearRegressionGD(k, rate)}) = k$
\end{example} 
%
 
This linear regression algorithm 
% in order to
aims to
model a linear relationship between a dependent variable $y$,
% corresponding to the observed value in the column $\chi[1]$ in database, 
and an independent variable $x$, $y = a \times x + c$, specifically approximating the 
model parameter $a$ and $c$.
In order to have a good approximation on the model parameter 
$a$ and $c$, 
% corresponding to the observed value in the column $\chi[0]$ in database, 
it sends query to a training data set adaptively in every iteration.
This training data set contains two columns (can extend to higher dimensional data sets), first column is used as the observed value for the independent variable $x$,
second column is used as the observed label value for the dependent variable $y$.
This algorithm is written in our {\tt Query While} language in Figure~\ref{fig:linear_regression}(a) as $\kw{linearRegressionGD(k, rate)}$.
% taking the iteration number $\kw{step}$ 
% and the learning rate $\kw{step}$ for gradient decent optimization.
% As a reminder,
% the notation 
% $\chi[0]$ is used to access the first column of the training data set
% and $\chi[1]$ for 
% % is used to access 
% the second column when requesting a query.
% In this regression, the loss function is the mean square error between the approximated and the observed value from database,
% and the standard gradient decent algorithm is used for optimizing the loss function.
%

This linear regression algorithm starts from initializing the linear model parameters and the counter variable,
and then goes into the training iterations.
In each iteration, it computes the differential value w.r.t. parameter
$a$ and $c$ respectively,
through requesting two queries, $\query(-2 * (\chi[1] - (\chi[0]\times a + c)) \times (\chi[0]))$ and 
$\query(-2 * (\chi[1] - (\chi[0]\times a + c)))$
at line 4 and 5.
Then, it uses these two differential values stored in variable $da$ and $dc$ to update the linear model parameters $a$ and $c$.
%
Its the program-based dependency graph is shown in Figure~\ref{fig:linear_regression}(b). Its execution-based dependency graph share the same graph, only needs to change the weight, $k$ into $w_k$ and $1$ for $w_1$ as we do in the previous example.
% We omit the detail of how to 
% generate this graph, which is similar to the generation procedure in 
% Example~\ref{alg:multiRound}.
In the execution-based dependency graph, there are multiple walks having the same longest query length.
For example, the walk $c^7 \to dc^6 : \to c^7 \to \cdots \to dc^6$ along the 
dotted arrows, where each vertex is visited $w_k(\trace_0)$ times for an initial trace $\trace_0$.
% By counting the total occurrence time of vertices with annotation $1$ in this walk, we have this program's adaptivity $k$.
There is actually other walks having the same query length $k$, the 
walk $a^7 \to da^6  \to a^7 \to \cdots \to da^6 $ along the 
dotted arrows, where each vertex is visited $w_k(\trace_0)$ times.
% the dotted path corresponds to a finite walk with the longest query length and its adaptivity on this walk is $k$.
But it doesn't affect the adaptivity for this program, which is still the maximal query length $w_k(\trace_0)$ with respect to initial trace $\trace_0$.
Also, $\THESYSTEM$, estimates the adaptivity $k$ for this example. Similarly as the multiple round example, we can show it is a tight bound.
%
\begin{example}
[Over-approximation Algorithm]
\label{ex:overapproximate}
The $\THESYSTEM$ comes across an over-approximation on the estimation due to its path-insensitive nature. 
It occurs when the control flow can be decided in a particular way in front of conditional branches, while the static analysis fails to witness. 

We show the over-approximation, in Figure~\ref{fig:overappr_example}(a),
we call it a multiple rounds odd iteration algorithm. In this algorithm, at line 5 of every iteration, 
a query $\query(\chi[x])$ based on previous query results stored in $x$ is asked by the analyst like in the multiple rounds strategy. The difference is that only the query answers from the even iterations ($i =0, 2, \cdots $) are 
% used to $b$. 
used in the query 
in line 7, $\query(\chi[\ln(y)])$.
  Because the execution trace only updates 
%   $b$ using the query answers at odd iterations, so the answers from even iterations do not affect the queries at odd iterations. From the query-based dependency graph in Figure~\ref{fig:overappr_example}(b), we can see that there is no edge from queries at odd iterations (such as $q_1,q_3,q_5$) to queries at even iteration(such as $q_2,q_4$). The longest path is dashed with a length $3$.  However, {\THESYSTEM} fails to realize that odd iteration will always execute then branch and even iteration means else branch, so its dependency graph considers both branches for every iteration. In this sense, the dependency graph by {\THESYSTEM} is similar to the one in the multiple rounds strategy. We show the estimated graph in Figure~\ref{fig:overappr_example}(c). The estimated upper bound is then, $5$, instead of $3$. 
$x$ using the query answers in even iterations, so the answers from odd iterations do not affect the queries in even iterations. 
From the execution-based dependency graph in Figure~\ref{fig:overappr_example}(b), 
we can see that the weight for the vertex $y^5$ is 
$w_k/2$. a function which takes any initial trace $\trace_0$, return the value of $k/2$ evaluated in $\trace_0$.  
However, {\THESYSTEM} fails to realize that odd iteration will always execute the then branch and even iteration means else branch, so 
% its dependency 
it considers both branches for every iteration. 
In this sense, the weight estimated for $y^5$ and $p^6$ are both 
$k$ as in Figure~\ref{fig:overappr_example}(c).
As a result, {\THESYSTEM}  estimates the longest walk from Figure~\ref{fig:overappr_example}(c),
$y^5  \to x^7  \to y^5  \to \cdots \to x^7  $ with each vertex visited $k$ times,
as the dotted arrows. 
And the adaptivity computed 
% estimated from the program-based dependency graph graph from by finding the walk with the longest query length 
is $1 + 2 * k$, instead of $1 + k$. 
% We omitted the estimated graph, which is identical to the graph in Figure~\ref{fig:overappr_example}(b). 
%
{ \small
\begin{figure}
\centering
    \begin{subfigure}{0.33\textwidth}
\centering
\small{
    \[
    %
    \begin{array}{l}
        \kw{multipleRoundsOdd}(k) \triangleq \\
        \clabel{ \assign{j}{k}}^{0} ; 
        \clabel{ \assign{x}{\query(\chi[0])} }^{1} ; \\
            \ewhile ~ \clabel{j > 0}^{2} ~ \edo ~ 
            \Big(
             \clabel{\assign{j}{j-1}}^{3} ;\\
             \eif(\clabel{j \% 2 == 0}^{4}, \\
             \clabel{\assign{y}{\chi[x]}}^{5}, 
             \clabel{\assign{p}{\chi[x]}}^{6});\\                            
             \clabel{\assign{x}{\query(\chi(\ln(y)))} }^{7} \Big)
        \end{array}
    \]
}
 \caption{}
    \end{subfigure}
%
\begin{subfigure}{.31\textwidth}
    \begin{centering}
    \begin{tikzpicture}[scale=\textwidth/11cm,samples=200]
% Variables Initialization
\draw[] (5, 1) circle (0pt) node{{ $x^1: {}^{w_1}_{1}$}};
% Variables Inside the Loop
 \draw[] (0, 7) circle (0pt) node{{ $y^5: {}^{w_k/2}_{1}$}};
 \draw[] (0, 4) circle (0pt) node{{ $p^6: {}^{w_k/2}_{1}$}};
 \draw[] (0, 1) circle (0pt) node{{ $x^7: {}^{w_k}_{1}$}};
 % Counter Variables
 \draw[] (5, 7) circle (0pt) node {{$j^0: {}^{w_1}_{0}$}};
 \draw[] (5, 4) circle (0pt) node {{ $j^3: {}^{w_k}_{0}$}};
 %
 % Value Dependency Edges:
 \draw[ thick, -latex,]  (0, 3.5) -- (0, 1.5) ;
%  \draw[ thick, -Straight Barb] (1, 4.2) arc (220:-100:1);
 \draw[ thick, -Straight Barb] (6.5, 4.5) arc (150:-150:1);
 \draw[ thick, -latex] (5, 4.5)  -- (5, 6.5) ;
%  \draw[ thick, -Straight Barb] (1., 1.5) arc (120:-200:1);
 % Value Dependency Edges on Initial Values:
 \draw[ thick, -latex,] (1.5, 1)  -- (4, 1) ;
 %
 \draw[ ultra thick, -latex, densely dotted,] (-0.6, 1.5)  to  [out=-220,in=220]  (-0.5, 6.5);
\draw[ ultra thick, -latex, densely dotted,]  (0.5, 6.5) to  [out=-30,in=30] (0.6, 1.6) ;
%  \draw[ ultra thick, -latex, densely dotted,]  (0.5, 10)  to  [out=-50,in=50] (0.5, 4);
 % Control Dependency
 \draw[ thick,-latex] (1.5, 7)  -- (4, 6) ;
 \draw[ thick,-latex] (1.5, 4)  -- (4, 6) ;
 \draw[ thick,-latex] (1.5, 1)  -- (4, 6) ;
%  \draw[ thick,-latex] (1.5, 10)  -- (4, 6) ;
 \end{tikzpicture}
 \caption{}
    \end{centering}
    \end{subfigure}
    \begin{subfigure}{.31\textwidth}
        \begin{centering}
        \begin{tikzpicture}[scale=\textwidth/11cm,samples=200]
    % Variables Initialization
    \draw[] (5, 1) circle (0pt) node{{ $x^1: {}^1_{1}$}};
    % Variables Inside the Loop
     \draw[] (0, 7) circle (0pt) node{{ $y^5: {}^{k}_{1}$}};
     \draw[] (0, 4) circle (0pt) node{{ $\mathbf{p^6: {}^{k}_{1}}$}};
     \draw[] (0, 1) circle (0pt) node{{ $\mathbf{x^7: {}^{k}_{1}}$}};
     % Counter Variables
     \draw[] (5, 7) circle (0pt) node {{$j^0: {}^{1}_{0}$}};
     \draw[] (5, 4) circle (0pt) node {{ $j^3: {}^{k}_{0}$}};
     %
% Value Dependency Edges:
 \draw[ thick, -latex,]  (0, 3.5) -- (0, 1.5) ;
%  \draw[ thick, -Straight Barb] (1, 4.2) arc (220:-100:1);
 \draw[ thick, -Straight Barb] (6.5, 4.5) arc (150:-150:1);
 \draw[ thick, -latex] (5, 4.5)  -- (5, 6.5) ;
%  \draw[ thick, -Straight Barb] (1., 1.5) arc (120:-200:1);
 % Value Dependency Edges on Initial Values:
 \draw[ thick, -latex,] (1.5, 1)  -- (4, 1) ;
 %
 \draw[ ultra thick, -latex, densely dotted,] (-0.6, 1.5)  to  [out=-220,in=220]  (-0.5, 6.5);
\draw[ ultra thick, -latex, densely dotted,]  (0.5, 6.5) to  [out=-30,in=30] (0.6, 1.6) ;
%  \draw[ ultra thick, -latex, densely dotted,]  (0.5, 10)  to  [out=-50,in=50] (0.5, 4);
 % Control Dependency
 \draw[ thick,-latex] (1.5, 7)  -- (4, 6) ;
 \draw[ thick,-latex] (1.5, 4)  -- (4, 6) ;
 \draw[ thick,-latex] (1.5, 1)  -- (4, 6) ;
%  \draw[ thick,-latex] (1.5, 10)  -- (4, 6) ;
     \end{tikzpicture}
     \caption{}
        \end{centering}
        \end{subfigure}
        \vspace{-0.4cm}
\caption{(a) The multiple rounds odd example 
(b) The execution-based dependency graph
(c) The program-based dependency graph graph from $\THESYSTEM$.}
    \label{fig:overappr_example}
    \vspace{-0.5cm}
\end{figure}
}
%
\end{example}

\input{limitation}
% \input{impl_results}
