We would like to thank all the reviewers for their comments and suggestions.
We will take all of them in consideration when revising the paper. 

- Overview

We will follow the reviewer recommendations for improving the
presentation and to make the paper more accessible, using the extra
pages available for the final version.

Specifically, we will:
* add a simpler introductory example in the introduction,
* improve our presentation of notation by having a table summarizing it
* provide more discussion about the main technical definitions
* simplify the technical part of the paper, by relating the concepts we
introduce to known concepts and explaining the differences
* improve the description of Algorithm 1 explaining how this
 algorithm handles examples with high level multi loops and produces
 non-linear adaptivity upper bound 
* add a clear statement about the fact that AdaptFun is not
path-sensitive in the general case.
* improve the description of our evaluation, providing more details on the
  timeout and the motivations alternative implementation AdaptFun-I
* improve the caption of figure and tables
* improve the appendix to make it easier to find the results we omit
  in the paper.

- Detailed Response

---------------------------------------------------------------
Reviewer A

> As the authors mentioned, the paper's method works only for linear
> queries. Also, it does not seem to work on high-level multi loops.

To clarify, we consider only linear queries because it is the broader
class for which we have clear theoretical statements as the ones we
discuss in Section 2 (Theorems 2.1-3). Our system can be easily
adapted to handle also non-linear queries and we plan in the future to
explore in which situations we can control the generalization error
for this class of queries.
Our method can handle multi loops but as we discussed in Section 7,
our implementation does have performance issues with some examples
using high level multi-loops to estimate the weight. The bottleneck is
the reachability bound analysis. To overcome this, we implemented
another more efficient (but less precise) reachability bound analysis
(we called it AdaptFun-I) which can handle these examples more
efficiently. The results and performance of AdaptFun-I are presented
in Table 2. The evaluation shows that AdaptFun-I can be used as a
complement for AdaptFun for examples with "complicated" multi loops.

> Demonstrations in the paper are a bit confusing. For example, in
> semantics-based dependency graph, there are too much information
> packed in a small graph, and lambda tau.rho(tau)k notation is hard
> to understand at first sight.

Thanks for pointing this out, we will review our notation to make it
easier to understand (perhaps omitting some detail from the graph,
e.g. we could leave the initial trace implicit) and we will also
improve our explanations.


------------------------------------------------------------------
Reviewer B

Thanks for the suggestions. We will improve our presentation by having
a table summarizing the notation, a simpler example in the
introduction, more text description for Algorithm 1, more discussion
on the alternative implementation AdaptFun-I, a clear claim that
AdaptFun is not path-sensitive in the general case. Thank you also for
pointing out typos, we will fix them as well.

> I appreciated the citation of differential privacy work.  Work on
> program fairness properties and differential privacy in software
> topics (e.g., profiling) is loosely related and may be worth citing
> if space allows.

Thanks for pointing this out. We will add references to those works.

> In section 2, the co-domain for AdaptFun is defined to be any of
> [-1, 1], [0, 1], or [-R, +R].  Does this choice matter, whether for
> correctness or performance?

It does not matter for correctness. It might affect performance
for some specific set of queries but it is an aspect we didn't explore. 

> When reading the definition of "well-formed" graphs (end of section
> 4.2), I was left wondering: Is this restrictive in any way for
> real-world graphs?  Does the database being defined as finite make a
> difference here?

Our definition of "well-formed" graphs rules out programs with
non-terminating behaviors, such as ones with infinite loops. We
believe for our application, adaptive data analysis, this is not too
restrictive. Most of the programs perform some recursion either on the
database structure, which is in general finite, or on some other
quantity finite quantity.
The database being finite is an essential assumption in our work, as well
as in the literature we summarized in Section 2. If we could have
an infinite number of sample data, in most situations we wouldn't need
additional method to control the generalization error since this would
be vanishing when the number of samples n tends to infinity. 

> There is no direct proof provided for Lemma 5.1 (including in the
> appendix, from what I could decipher).  Even if trivial, it would
> be helpful to note something.

Sorry for the inconvenience. The proof for Lemma 5.1 is actually in
appendix Lemma B.2 in page 44. However, we realize that the lemma
number is different and actually the lemma in the appendix uses a
slightly different notation. We will fix the notation, the theorem
number and we will add a pointer to it in the paper.

> " I wasn't sure what conclusion to draw regarding AdaptFun-I (is it
>  the best? is the one case pathological?).  More could also be said
>  regarding the claim (855) that "examples with many nested loops are
>  not common".  Are they important real-world examples, though?  What
>  type of examples would fit this profile?"

The alternative implementation AdaptFun-I is used to analyze some
examples with complex high level multi loops that AdaptFun cannot
handle due to the performance bottleneck of the underlying
implementation of reachability bound analysis (show in Table 1, 3
examples whose running time is * meaning too long). AdaptFun-I
provides a less precise but efficient analysis for these examples,
using very little time (shown in table 2). The price to pay to use
AdaptFun-I is in precision. We think about it as a complement of
AdaptFun when the latter cannot handle "complex" multi loops. We don't
expect these complex multi loops to appear in many realistic examples,
but it is still an option we have.

-------------------------------------------------------------------------
Reviewer C

Thanks for your careful review and detailed suggestions. We will work
on improving the presentation of the paper, adjusting the font of
Figure 2, fixing typos, rephrasing the confusing sentences, and better
presenting the tables in the evaluation section.

> 3. Why is there an (l6, x3) edge in Figure 3b? Is the graph
> implicitly the transitive closure of data dependencies?

That is right, this edge appears because of the transitive closure of
data dependencies. Formally, the edge (l6, x3) in Fig 3b of the
semantic-based dependency graph shows up because of the DEP relation in 
definition 3.

> Given that the reachability bound analysis is based on difference
> constraints, is it correct for me to assume that the algorithm can
> at most derive linear bounds on the number of times each statement
> may be executed? In that case, I was surprised that the algorithm
> was able to derive quadratic adaptivity estimates (such as for
> loop2VD in Table 1). Can the authors please clarify?

Our reachability bound analysis uses difference constraints inspired
by previous work [37]. Indeed, this technique provides linear bounds
or weight (the weight used in our weighted dependency graph in Section
5.1.2). To get the estimation of an adaptivity upper bound, our
algorithm estimates the length of the longest path in the weighted
dependency graph by approximating the local adaptivity of every
Strong Connect Component (SCC) of the weighted graph by the number of
vertices times the minimum weight of all the vertex in a path of this
SCC. So, even if the reachability bound analysis gives linear weight k
for every statement in a loop (SCC in the graph), then the estimated
local adaptivity of this SCC is k * [number of vertices in the path of
this SCC]. If there are nested while loops, then the estimated
adaptivity for this nested while loop will be quadratic (k*k). In
table 1, loop2VD contains a nested loop of  the form:

j = k;
i=k;
while [j>0] {
j--;
while [i > 0] {
i--;
... }
... }

Our adaptBD will then return a quadratic adaptivity estimation. We
will explain this better in the evaluation section as well as in the
the discussion about the adaptBD algorithm in 5.3.

> but a more interesting question is how accurate the entire pipeline
> would have been if the ground truth adaptivity had been used instead
> of Aest? Given that the ground truth and Aest columns are identical
> in many cases in Table 1, I assume that not much accuracy is lost
> because AdaptFun is conservative?

Our evaluation shows that the adaptivity estimated by AdaptFun is
precise on several different examples. This shows that there wouldn't
be too much difference in terms of accuracy by using the result of
AdaptFun instead of the ground truth. However, as we discussed in
Section 6, AdaptFun is not path sensitive, so there are examples where
AdaptFun will give loose adaptivity bounds, with a corresponding loss
in accuracy with respect to the ground truth. These examples aren't
widespread in adaptive data analysis. Hence, we believe that the
result provided by AdaptFun would be in general a good estimate of the
ground truth.

> 9. Can you elaborate on why AdaptFun times out for the last three
> benchmark programs?

These three examples have multiple nested loops, which requires very
long time to do reachability bound analysis using difference
constraints [37]. This is the motivation we simplify the reachability
bound analysis in AdaptFun-I, which is not precise as AdaptFun but
will not time out, as shown in Table 2.


---------------------------------------------------------------------------------

Reviewer D

Thank you for your comments and suggestions.  We will revise the paper
to simplify the technical presentation. We will provide a brief
reminder on stats notation in section 2 and we will try to simplify
the notation in section 4 and 5.

> how the dependence graph, dataflow/reachability analyses,
> etc. differ from standard analyses on programs? Is the key
> contribution to recognize well-known compiler techniques are
> sufficient for this problem, or was it to adapt them for this
> particular problem of propagating adaptivity information?

As discussed in the paper, we see our key contribution as providing a
formal notion of adaptivity and an effective method to estimate
it. Our method builds on previously known compiler techniques but it
was not our goal to show that these compiler techniques are
sufficient. In fact, these techniques alone are not sufficient. In
broad strokes, we also had to integrate a resource analysis step in
the form of reachability-bound analysis, since adaptivity is a
quantitative notion of dependency, and we had to design a new
estimation algorithm computing the longest "query length".

> Overall the notion of a weighted dependence graph and finding paths
> in it, for some purpose (e.g. SIMD vectorization) is not a new
> concept,

We will include a discussion about other works using this approach,
including the work on SIMD vectorization, in the related works section.

> Serious problems kick in Sec 4: Definition 1 is hard to digest, for
> what seems a simple independence / non-existence
> condition. Definition 2 is where I stopped being able to read the
> formalism: many symbols are undefined, for example what "++" means?
> eg. how to read tau_1 ++ [ e1 ] ++ tau ++ [ e2 ]?

We will also make sure that all our symbols are defined (and we will
summarize the notation in a table as suggested by another
reviewer). However, we would like to stress that we say at line 410
that the symbol "++" stands for "the concatenation of two traces". So,
tau_1 ++ [ e1 ] ++ tau ++ [ e2 ] is a trace which is obtained by
concatenating 4 other traces, assuming associativity of concatenation,
two of which have only one event. We use this notation because the
events we are interested into, e1 and e2, may appear at different
places in the trace but we want the constraint that e1 comes before e2
and e2 is the last event of the trace.


