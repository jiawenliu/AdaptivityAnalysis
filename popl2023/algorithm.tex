In this section, we clarify the algorithm {\THESYSTEM} that analyzes the adaptivity of a target program in SSA loop language, which consists of three auxiliary algorithms: a variable estimation algorithm $\mathsf{VE}$, a matrix-vector based graph generating algorithm $\mathsf{GG}$ to generate the weighted variable-based dependency graph, and a path-searching algorithm $\mathsf{PS}$ to find the most weighted path in the graph. We do not show details of  $\mathsf{PS}$ as we use a standard graph algorithm.
We start with the ideas of {\THESYSTEM}, and illustrate the first two auxiliary algorithms, variable estimation algorithm $\mathsf{VE}$ and dependency graph generating algorithm $\mathsf{GG}$. 
In the end, we show the soundness of our algorithm with respect to adaptivity.  


\subsection{Ideas behind the Algorithm}
In consideration of the definition of adaptivity, the longest path in its query-based dependency graph, our analysis targeting a tight upper bound on the
adaptivity, is supposed to take care of paths (possible adaptivity candidates) in all the possible dependency graphs (per configuration).
To this end, {\THESYSTEM} aims to statically construct a weighted directed dependency graph, in which the nodes are annotated SSA variables and directed edges show when one annotated variable may depend on another. The weight of the node shows if the variable is assigned with a query request. Intuitively, every query request in the query-based dependency graph is assigned to variables that appear in the SSA variable-based dependency graph. \dg{I think the last sentence can be deleted.}

The algorithm {\THESYSTEM}, summarized in Figure~\ref{fig:adaptfun}, estimates the adaptivity from the weighted variable-based dependency graph. Given the input SSA program $c$ to be analyzed, the first step of {\THESYSTEM} is the variable estimation via the auxiliary algorithm $\mathsf{VE}$, which specifies the nodes of the variable-based dependency graph. The result of $\mathsf{VE}$ is stored in a global variable list $G$, fed to the next step. The second step is the graph generation, via a matrix-vector-based graph generating algorithm $\mathsf{GG}$. The matrix $M$ records the may-dependency between annotated variables in the global list $G$. It has size $|G| \times |G|$. The vector $V$ has the same size as $G$ and gives a weight to each variable in $G$. This weight is $1$ when the variable is assigned with a query request and $0$ otherwise. 

To be precise, the $i$th row, $j$th column of the matrix $M$, written $M[i][j]$, is  $1$ when there may be a dependency from variable $ G[i]$ to $G[j]$. Dually, $M[i][j] =0$ means no dependency. In a similar way, $V[i]=1$ means the variable $G[i]$ is assigned with a query request.
 
The SSA variable-based weighted dependency graph is constructed by the graph generating algorithm $\mathsf{GG}$. The final step is to find out the estimated adaptivity -- the most weighted path in the graph. We use another auxiliary algorithm $PS$ to find the path with the most weights in the graph.     
%
{\small
\begin{figure}
    \centering    
    \begin{tikzpicture}
    % {node distance = 2cm, auto}
  % nodes
%   \node[block] at (2,-6) (block6) {$f_6$};
\draw[very thick, dashed] (-1, 3.6)  -- (9.5, 3.6) -- (9.5,0.4) -- (-1,0.4) -- (-1,3.6);
  \node [block]at (1,3) [text width=6em](VG){ Variable Estimation} ;
  \node [block]at (1,1)[text width=6em](GG){Graph Generating};
    \node [block, line width=0.8mm, dashed,  minimum width=6.5em,minimum height=6.5em, rounded corners=1cm] at (4.5,2)[text width=6em](graph)
{Variable Based Weighted Dependency Graph};
    \node [block] at (7.5,2)[text width=3.5em](pathsearch)
{Most Weighted Path Search};
    \node[inner sep=0,minimum size=0,left of=VG, node distance = 3cm] (start) {};
\node[inner sep=0,minimum size=0,right of=pathsearch, node distance = 3cm] (end) {};
% edges
%   \draw[->] (VG.west) -- -(0.5,0);
  \path [line, thick] (graph) -- node [above] {} (pathsearch) ;
  \path [line, thick] (GG) -- node [below] {$M$, $V$} (graph);
  \path [line, thick]  (VG) -- node {Global Variables $G$}(GG); 
%   \draw [double distance=2.5mm, -{Latex[length=1.2cm,open]}, blue, line width=1mm] (start.east) to (VG.west);
%     \draw [double distance=.5mm, white, line width=1mm] (start.east) to ($(VG.west) - (11mm,0)$);
  \path [line, -{Implies},double, line width=0.7mm]  (start) -- node[text width=5em, above, text centered] {SSA Program $c^{s}$}(VG); 
  \path [line,  line width=0.7mm, -{Implies},double]  (pathsearch) -- node[text width=5em, above, text centered] {Estimated Adaptivity $Adapt$}(end); 
  %   \draw[vecArrow] (start) to (VG) [above] {ssa Program $c^{s}$}
%   % 2nd pass: copy all from 1st pass, and replace vecArrow with innerWhite
%   \draw[innerWhite] (start) to (VG);
 \end{tikzpicture} 
    \vspace{-0.3cm}
    \caption{The overview of {\THESYSTEM}}
    \label{fig:adaptfun}
    \vspace{-0.5cm}
\end{figure}
}
%
\subsection{Variable Estimation Algorithm}
We first show the algorithm $\mathsf{VE}$, which adds variables to the global variable list $G$. $\mathsf{VE}$ has the form $\ag{G; w; \ssa{c}}{ G'; w'} $, as shown in Figure~\ref{fig:ag}. The input of $\mathsf{VE}$ is a list of annotated variables $G$ collected before the program $\ssa{c}$, a loop map $w$ consistent with previous estimation, and an input SSA program $\ssa{c}$. The output of the algorithm is the updated global list $G'$, along with the updated loop maps $w$, for later estimation.   
{\footnotesize
\begin{figure}
 \begin{mathpar}
\inferrule
{
}
{ \ag{G ;w; \ssa{[\assign {x}{\expr}]^{l}}}{G ++ [\ssa{x}^{(l,w)}];w}
% G ;w; \ssa{[\assign {x}{\expr}]^{l}} \to G ++ [x^{(l,w)}];w 
}
~\textbf{ag-asgn}
\and
\inferrule
{
}
{ \ag{G ;w;  [ \assign{\ssa{x}}{q(\ssa{\expr})}]^{l}}{  G ++ [\ssa{x}^{(l,w)}] ; w} 
}~\textbf{ag-query}
%
\and 
%
\inferrule
{
\ag{G; w; \ssa{c_1}}{  G_1;w_1}
\and 
 \ag{G_1;w ; \ssa{c_2}}{  G_2; w_2}
 \\
 {G_3 = G_2 ++ \ssa{[\bar{x}^{(l,w)}]++ \ssa{[\bar{y}^{(l,w)}]}++ \ssa{[\bar{z}^{(l,w)}]} }}
}
{
\ag{G; w;
[\eif(\ssa{\bexpr},[ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ,[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}],[ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}], \ssa{ c_1, c_2)}]^{l} }{ G_3 ;w}
}~\textbf{ag-if}
%
%
%
% \and 
% %
% \inferrule
% {
% \ag{G; w; \ssa{c_1}}{ G_1; w_1}
% \and 
% \ag{G_1;w_1; \ssa{c_2}}{ G_2; w_2}
% }
% {
% \ag{G; w;
% \ssa{(c_1 ; c_2)}}{  G_2 ; w_2}
% }
% ~\textbf{ag-seq}
\and 
\inferrule
{
{G_0 = G \quad w_0 =w }
\and
\forall 0 \leq z < N. 
{ \ag{ G_z ++ \ssa{[\bar{x}^{(l, {w_z}+l)}]} ; (w_z+l); \ssa{c}}{ G_{z+1} ; w_{z+1}}  }
\\
{G_f = G_N ++ \ssa{[\bar{x}^{(l, w_N \setminus l)}]} }
\and
{ \ssa{\aexpr} =  {N}  }
}
{\ag{G; w; [\eloop ~ \ssa{\aexpr}, n, [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ~ \edo ~ \ssa{c}]^{l} }{ G_f; w_N\setminus l }
}~\textbf{ag-loop}
\end{mathpar}
\vspace{-0.4cm}
 \caption{The key rule of variable estimation algorithm  }
    \label{fig:ag}
    \vspace{-0.5cm}
\end{figure}
}

The algorithm adds variables to $G$ at assignments. In the case of expression assignment $\textbf{ag-asgn}$ and query request $\textbf{ag-query}$, the output global list is expanded by $\ssa{x}^{(l,w)}$. When it comes to if statements (rule $\textbf{ag-if}$), variables assigned in both branches, as well as generated variables $\bar{\ssa{x}},\bar{\ssa{y}},\bar{\ssa{z}}$ in $ [ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ,[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}],[ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}]$ are all added to $G$. Sequencing $\ssa{c_1;c_2}$ is handled as expected.

For loops, we assume that a loop counter is a natural number (rule 
$\textbf{ag-loop}$). The algorithm accounts for variables assigned in every iteration, including newly created variables in $\bar{\ssa{x}}$, and adds them to $G$ with annotations representing the iteration number. 

\subsection{Matrix and Vector based Algorithm}
Next, we describe the algorithm $GG$, which takes the list $G$ generated by the previous algorithm. $\mathsf{GG}$ has the form: $ \ad{\Gamma; \ssa{c} ;i }{ M; V;  i' } $. The input is a tuple consisting of three elements: (1) a 1-row-N-column matrix $\Gamma$ storing the (annotated) variables on which the execution of the current instruction is control dependent; this is needed for if statements. (2) the SSA program $\ssa{c}$ to be analyzed. (3) an index $i$ specifying the location of the first assigned variable of the program $\ssa{c}$ in the global list $G$. The output of $\mathsf{GG}$ also consists of three elements: (1) A matrix $M$ representing the may-dependencies from $\ssa{c}$. (2) A vector $V$ representing the (annotated) variables assigned with queries in $\ssa{c}$. (3) the index $i'$ that refers to the next position of the last assigned variable in $\ssa{c}$, if it exists. The existence of the index $i'$ helps to locate the first assigned variable when we need to analyze the continuation of $\ssa{c}$.  

We first define some functions which use the indices in $G$. 
The function $\mathsf{L(i)}$ generates a 1-column-N-rows  matrix, where only the $i$-th row is $1$ and all the other rows are $0$. This function is used to locate the right row when we calculate the matrix during assignment and query commands. 

The function $\mathsf{R(\ssa{e}, i)}$ generates a 1-row-N-column matrix. For every variable used in $\ssa{e}$, it finds the corresponding index $i$ in $G$ so that $G[i]$ maps to the variable and marks the $i$th column as $1$. If the variable is not found, we do not mark it. When we say $G[i]$ maps to a target variable, we take off the annotation of $G[i]$ and check if the left variable with no annotation is the same as the target variable. The extra argument $i$ is used to handle loops. For instance, a variable $y$ may appear many times in $G$, but with different annotations (iteration numbers). In this case, $i$ helps find the most recently assigned variable version of $y$ in $G$. It is used when analyzing assignment and query request commands. Thanks to our SSA language, where every variable is assigned once, our choice of the most recent assigned variable is reasonable because the variable used in the loop refers to the most recent assignment of itself. 

We define $M_1 ; M_2  :=  M_2 \cdot M_1 + M_1 + M_2 $, where $M_1 + M_2$ is the standard sum of two matrices.
We also define the operator $\uplus$ to combine two vectors.
\[
V_1 \uplus V_2  :=  \left\{
\begin{array}{ll}
1 & (V_1[i] = 1 \lor V_2[i] = 1) \land 1 \leq i \leq N \land |V_1| = |V_2|\\
0 & \mbox{otherwise}
\end{array}\right.
\]
For the sake of brevity, we also use some annotations when the algorithm $\mathsf{GG}$ handles the extra part $[ \bar{\ssa{x}}, \bar{\ssa{x_1}},\bar{\ssa{x_2}} ] $ in the if and loop statements. First, we give unique names for variables in lists $\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}$ respectively, as follows: \[ \forall 0 \leq z < |\bar{\ssa{x}}|. \bar{\ssa{x}}(z) = \ssa{x_z}, \bar{\ssa{x_1}}(z) = \ssa{x_{1z}}, \bar{\ssa{x_2}}(z) = \ssa{x_{2z}} \]
We treat every tuple $(\ssa{x_z},\ssa{x_{1z}},\ssa{x_{2z}} )$ in $[\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}} ]$ as the simple may dependency case : $\ssa{x_z}$ may depend on both $ \ssa{x_{1z}}$ and $\ssa{x_{2z}}$, just like $ \assign{\ssa{x_z} }{\ssa{x_{1z}} + \ssa{x_{2z}} }$, defined as follows.\\ 
$
 \ad{\Gamma; [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}} ] ; i }{ M; V_{\emptyset}; i + |\bar{\ssa{x}}| } 
  \triangleq { \forall 0 \leq z < |\bar{\ssa{x}}|.
  \ad{\Gamma;\assign{\ssa{x_z} }{\ssa{x_{1z}} + \ssa{x_{2z}} }; i+z }{ M_{x_z};  V_{\emptyset}; i+z+1} }$
   where $M = \sum_{o \leq z < |\bar{\ssa{x}}| }M_{x_z} $.
% \framebox{$ \ad{\Gamma; \ssa{c} ; i_1){M;V;i_2} $}
%
{\footnotesize
\begin{figure}
\begin{mathpar}
\inferrule
{M = \mathsf{L}(i) * ( \mathsf{R}(\ssa{\expr},i) + \Gamma )
}
{
 \ad{\Gamma;[\assign {\ssa{x}}{\ssa{\expr}} ]^{l}; i }{M; V_{\emptyset}; i+1 }
% \Gamma \vdash_{M, V_{\emptyset}}^{(i, i+1)} [\assign {\ssa{x}}{\ssa{\expr}} ]^{l}
}
~\textbf{ad-asgn}
\and
\inferrule
{M = \mathsf{L}(i) * ( \mathsf{R}(\ssa{\expr},i) + \Gamma )
\\
V= \mathsf{L}(i)
}
{ 
\ad{\Gamma;[ \assign{\ssa{x}}{q(\ssa{\expr})} ]^{l} ; i }{M;V;i+1}
%  \vdash^{(i, i+1)}_{M, V} [ \assign{\ssa{x}}{q(\ssa{\expr})} ]^{l} 
}~\textbf{ad-query}
%
\and 
%
\inferrule
{
{\ad{\Gamma + \mathsf{R}(\ssa{\bexpr}, i_1); \ssa{c_1} ; i_1 }{ M_1;V_1;i_2 }}
% \Gamma + \mathsf{R}(\bexpr, i_1) \vdash^{(i_1, i_2)}_{M_1, V_1} \ssa{c_1} 
% : \Phi \land \bexpr \Rightarrow \Psi
\and 
{\ad{\Gamma + \mathsf{R}(\ssa{\bexpr}, i_1);\ssa{c_2} ; i_2 }{ M_2; V_2 ;i_3}}
% \Gamma + \mathsf{R}(\ssa{\bexpr}, i_1) \vdash^{(i_2, i_3)}_{M_2, V_2} \ssa{c_2} 
% : \Phi \land \neg \bexpr \Rightarrow \Psi
\\
% { \forall 0 \leq j < |\bar{x}|. \bar{x}(j) = x_j, \bar{x_1}(j) = x_{1j}, \bar{x_2}(j) = x_{2j}  }
{\ad{\Gamma; [ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]; i_3 }{ M_x; V_{\emptyset}; i_3+|\bar{\ssa{x}}| }}
%
\and
%
{\ad{\Gamma; [ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}]; i_3+|\bar{\ssa{x}}| }{ M_y; V_{\emptyset}; i_3+|\bar{\ssa{x}}|+|\bar{\ssa{y}}| }}
%
\\
%
{\ad{\Gamma; [ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}]; i_3+|\bar{\ssa{x}}|+ |\bar{\ssa{y}}|}{ M_y; V_{\emptyset}; i_3+|\bar{\ssa{x}}|+|\bar{\ssa{y}}| + |\bar{\ssa{z}}| }}
\\
{M = (M_1+M_2)+ M_x+M_y +M_z }
}
{
\ad{\Gamma ; \eif([\ssa{\bexpr}]^{l},[ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ,[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}] , [ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}] , \ssa{ c_1, c_2)} ; i_1}{ M ;V_1 \uplus V_2  ; i_3+|\bar{x}|+|\bar{y}|+|\bar{z}| }
}~\textbf{ad-if}
%
%
%
\and 
%
\inferrule
{
{\ad{\Gamma; \ssa{c_1} ; i_1 }{ M_1 ; V_1; i_2 }  }
% \Gamma \vdash^{(i_1, i_2)}_{M_1, V_1} \ssa{c_1} 
% : \Phi \Rightarrow \Phi_1
\and 
{\ad{\Gamma;\ssa{c_2}; i_2}{M_2; V_2 ;i_3 }}
% \Gamma \vdash^{(i_2, i_3)}_{M_2, V_2} \ssa{c_2} 
% : \Phi_1 \Rightarrow \Psi 
}
{
\ad{\Gamma ; (\ssa{c_1 ; c_2} ) ; i_1}{(M_1 {;} M_2) ; V_1 \uplus V_2 ; i_3  }
% \Gamma \vdash^{(i_1, i_3)}_{M_1 {;} M_2, V_1 \uplus V_2}
% \ssa{c_1 ; c_2} 
% : \Phi \Rightarrow \Psi
}
~\textbf{ad-seq}
\and 
\inferrule
{
B= |\ssa{\bar{x}}| \and {A = |\ssa{c}|}
% \and
% {\Gamma \vdash^{(i, i+B)}_{M_{10}, V_{10}} [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] }
% \and
% {\Gamma \vdash^{(i+B,i+B+A )}_{M_{20}, V_{20}} \ssa{c} 
% }
\\
\forall 0 \leq j < N. 
{\ad{\Gamma;[\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]; i+ j*(B+A) }{M_{1j};V_{1j}; i+B+j*(B+A) }}
% {\Gamma \vdash^{(i+j*(B+A), i+B+j*(B+A))}_{M_{1j}, V_{1j}}  } [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]
\\
{
\ad{\Gamma;\ssa{c} ; i+B+j*(B+A)  }{M_{2j}; V_{2j}; i+B+A+j*(B+A) }
% \Gamma \vdash^{(i+B+j*(B+A),i+B+A+j*(B+A) )}_{M_{2j}, V_{2j}} \ssa{c} 
% : \Phi \land e_n = \lceil{z+1}\rceil \Rightarrow \Psi 
}
\\
{
\ad{\Gamma ; [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ; i+N*(B+A) }{M; V ;i+N*(B+A)+B}
% \Gamma \vdash^{(i+N*(B+A) ,i+N*(B+A)+B )}_{M, V} [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]
% : \Psi \Rightarrow \Phi \land e_N = \lceil{z}\rceil 
}
\\
{ \ssa{\aexpr} =  {N}  }
\and
{M' = M+ \sum_{0 \leq j <N}( M_{1j}+M_{2j})  }
\and
{V' = V \uplus \sum_{0 \leq j <N}( V_{1j} \uplus V_{2j})  }
}
{
\ad{\Gamma;\eloop ~ [\ssa{\aexpr}]^{l}, ~0, [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ~ \edo ~ \ssa{c}, i }{ M';V' ;i+N*(B+A)+B }
%  \vdash^{(i,   )}_{M', V'} 
% : \Phi \land \expr_N = \lceil { N} \rceil \Rightarrow \Phi \land \expr_N = \lceil{0}\rceil
}~\textbf{ad-loop}
\end{mathpar}
    \vspace{-0.4cm}
    \caption{The key rules of the graph generating algorithm}
    \label{fig:algo_ad}
    \vspace{-0.5cm}
\end{figure}
}
%

One key idea of algorithm $\mathsf{GG}$ is to track the indices $i$, $i'$ in the input and output to synchronize with the previous algorithm $\mathsf{VE}$: The index in $\mathsf{GG}$ increases in the same way as the global list expands after the analysis of a program $\ssa{c}$ by $\mathsf{VE}$, which helps $\mathsf{GG}$ record the dependency relation from the program $\ssa{c}$ in the right place of the matrix. For example, in the cases $\textbf{ad-asgn}$ and $\textbf{ad-query}$, the index increases by 1, which corresponds to the addition of one variable in the algorithm $\textbf{VE}$. The if and loop commands have the extra part $[\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] $ and the output index increases by also considering this part as we do in collecting $G$ in $\mathsf{VE}$.  

% Another interesting point is the construction of the matrix. The fundamental case is the assignment and query cases. We use a function $L(i)$ to generate a N-row-one-column matrix $L$ to guarantee the resulting matrix only has non-zeros at row $i$. The intuition behind is that one single assignment or query request can only reveal the dependency of its assigned variable (corresponding to one row of the matrix) to the variables used on the right hand sides. Thanks to the index $i$, we know which row this assignment should be in the matrix. The function $\mathsf{R}(\ssa{\expr},i)$ gets a one-row-N-column matrix marking the variables used in the right hand side. The $\Gamma$ is designed for the if command and we will discuss it later.
We show one simple example $sa$ to illustrate the construction of the matrix.     
\[
sa \triangleq
\begin{array}{l}
    \left[x_1 \leftarrow 2 \right]^1; 
    \left[x_2 \leftarrow x_1 + 2 \right]^2 ; 
    \left[x_3 \leftarrow x_1 + x_2 \right]^3
\end{array}
\]
In the program $sa$, only simple assignment is involved. When we assume an empty $\Gamma$, when analyzing the assignment at line $3$, the matrix is built as follows.
\[
\textbf{line3:} ~~
 \left[x_3 \leftarrow x_1 + x_2 \right]^3 :
 ~~~
\begin{blockarray}{cc}
\begin{block}{c[c]}
 x_1 & 0   \\
 x_2 & 0 \\
 x_3 & 1 \\
\end{block}
\end{blockarray}
*
\begin{blockarray}{ccc}
x_1 & x_2 & x_3 \\
\begin{block}{[ccc]}
1 & 1 & 0 \\
\end{block}
\end{blockarray}
= 
\begin{blockarray}{cccc}
& x_1 & x_2 & x_3\\
\begin{block}{c[ccc]}
x_1 & 0 & 0 & 0 \\
x_2 & 1 & 0 & 0 \\
x_3 & 1 & 1 & 0 \\
\end{block}
\end{blockarray}
\]
%
$\Gamma$ is needed to handle the case when the control flow diverges in an if statement
% $[\eif(\ssa{\bexpr},[ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ,[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}] , [ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}] , \ssa{ c_1, c_2)}]^{l} $
, where the execution of either branch may depend on the conditional predicate $\ssa{\bexpr}$. Following this intuition, the analysis of either branch considers the variables used in the conditional $\ssa{\bexpr}$, tracked in $\Gamma$. In the rule $\textbf{ad-if}$, the analysis of the two branches $\ssa{c_1}$ and $\ssa{c_2}$ is in $\Gamma + \mathsf{R}(\ssa{\bexpr}, i)$. Here, $\mathsf{R}(\ssa{\bexpr}, i) $ gives variables used in the conditional predicate $\ssa{\bexpr}$.   

We compose the matrices and vectors in sequencing in rule $\textbf{ad-seq}$. The non-zeros or, as we call them, the \emph{effect ranges}, of the matrix and the vector are decided by input and output indices. In rule $\textbf{ad-seq}$, the two programs $\ssa{c_1}$ and $\ssa{c_2}$ must have disjoint effect ranges $[i_1, i_2)$ and $[i_2,i_3)$, so it is safe to combine them without losing information. 
In rule \textbf{ad-seq}, we use $B= |\bar{\ssa{x}}|$ and $A= |{\ssa{c}}|$ to estimate the number of variables assigned in $\bar{\ssa{x}}$ and $\ssa{c}$. $|\ssa{c}|$ is defined with the help of the algorithm $\mathsf{VE}$, defined as $|\ssa{c}|= |G|$ when $\ag{[];\ssa{c};\emptyset }{ G; \emptyset }$. The algorithm then determines the number $N$ of loop iterations from the loop counter $\ssa{\aexpr}$. For every iteration, it first records the dependency relations between variables in $ [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]$ by constructing a corresponding matrix $M_{1j}$ ($j$ is the iteration number) and an empty vector $V_{1j}$, and analyzes the loop body $\ssa{c}$, resulting in a matrix $M_{2j}$ and vector $V_{2j}$. We know that for all possible iteration numbers $j$, $M_{1j}$ and $M_{2j}$ have disjoint effect ranges so we can safely combine them. Similarly, we can combine the vectors $V_{1j}$ and $V_{2j}$.   

End-to-end, {\THESYSTEM} yields a variable-based weighted dependency graph defined by edges $M$ and weights $V$. The definition of the estimated adaptivity is the weight of the most weighted path in the graph, defined as follows. 

\begin{defn}
[Estimated adaptivity]
Given a program $\ssa{c}$, the global list $G$, and $\ad{\Gamma; \ssa{c}; i_1}{M, V, i_2}$, the weighted dependency graph $G_{ssa}(M, V,G,i_1,i_2) = (Nodes, Edges, Weights)$ is defined as:
\\
Nodes $Vt = \{ G(j) \in \mathcal{LV} \mid i_1 \leq j < i_2 \}$
\\
Edges $E = \{ (G(j_1), G(j_2)) \in \mathcal{LV} \times \mathcal{LV} \mid M[j_1][j_2] \geq 1 \land  i_1 \leq j_1,j_2 < i_2   \}$
\\
 Weights $Wt = \{ (  G(j), 1 ) \in \mathcal{LV} \times \mathcal{N} \mid i_1 \leq j < i_2 \land V[j] = 1\}
        \cup \{ (  G(j), 0 ) \in \mathcal{LV} \times \mathcal{N} \mid i_1 \leq j < i_2 \land V[j] = 0 \} $.
        
Adaptivity of the program is then defined as:
\[
Adapt(M, V,G,i_1,i_2) := \max_{vt_1, vt_2 \in Vt}\{ \mathsf{Weight}( p(vt_1, vt_2), Wt) \},
\]
where $p(k, l)$ is the path in graph $G_{ssa}(M, V,G, i_1,i_2)$ starting from $k$ to $l$, and $\mathsf{Weight}(p(vt_1,vt_2), Wt)$ is the total sum of weights of nodes along the path $p(vt_1,vt_2)$.
\end{defn}        
%
% \subsection{Analysis on two round algorithm}
% We still use the two round example in Figure~\ref{fig:ssa_tworound} to illustrate how {\THESYSTEM} works.
%
% \[
% TR^{ssa}(k) \triangleq
% {
% \begin{array}{l}
%     % \left[j \leftarrow 0 \right]^1 ; \\
%     \clabel{a_1 \leftarrow []}^{1} ; \\
%     \clabel{\assign{j_1}{0} }^{2} ; \\
%     \eloop ~ \clabel{k}^{3} ~ \edo [(j_3, j_1,j_2),(a_3, a_1,a_2)]~ \\
%     \Big(
%     \clabel{ x_1 \leftarrow q(\chi(j_3)\cdot \chi(k))}^{4}  ; \\
%     \clabel{ \assign{j_2}{j_3+1} }^{5} ;\\
%     \clabel{a_2 \leftarrow x_1 :: a_3}^{6}       \Big);\\
%     \clabel{l_1 \leftarrow q(\mathrm{sign}\big (\sum_{i\in [k]} \chi(i)\times\ln\frac{1+a_3[i]}{1-a_3[i]} \big ))}^{7}\\
% \end{array}
% }
% \]
% {\THESYSTEM} first runs the algorithm $\mathsf{AG}$ to generate the global list $G$.
%
% \[G = \left[
% \begin{array}{l}
%     {a_1}^{(1,\emptyset)}, {i_1}^{(2,\emptyset)}, {i_3}^{(3,[3:1])} ,{a_3}^{(3,[3:1])} , {x_1}^{(4,[3:1])} , {a_2}^{(5,[3:1])} , {i_2}^{(6,[3:1])} ,
%   {i_3}^{(3,[3:2])} , {a_3}^{(3,[3:2])} , {x_1}^{(4,[3:2])} , \\ {a_2}^{(5,[3:2])} ,{i_2}^{(6,[3:2])},  
%   {i_3}^{(3,[3:3])} , {a_3}^{(3,[3:3])} , {x_1}^{(4,[3:3])} , {a_2}^{(5,[3:3])} ,{i_2}^{(6,[3:3])},
%   {i_3}^{(3,\emptyset)}, {a_3}^{(3,\emptyset)} , {l_1}^{(7,\emptyset)}    
% \end{array} \right] 
%   \]
%   There are in total $20$ elements in the generated $G$. We can notice that there are $5$ annotated variables for every iteration. One iteration contains both the new generated variable $i_3$, $a_3$, and the variables $x_1,a_2,i_2$, assigned in the loop body.   
%
% \[G_{k=2} = \left[ \begin{array}{l}
%      {a_1}^{(1,\emptyset)} , {j_1}^{(2,\emptyset)}, {j_3}^{(3,[2:1])} , {a_3}^{(3,[2:1])} , {x_1}^{(4,[2:1])} ,{j_2}^{(5,[2:1])} ,
%   {a_2}^{(6,[2:1])},  \\
%     {j_3}^{(3,[2:2])} , {a_3}^{(3,[2:2])} , {x_1}^{(4,[2:2])} ,{j_2}^{(5,[2:2])} ,
%   {a_2}^{(6,[2:2])},
%   {j_3}^{(3,\emptyset)} , {a_3}^{(3,\emptyset)} ,
%   {l_1}^{(7,\emptyset)} \\  
% \end{array}
%      \right] \]
%  For simplicity, we denote $a_1$ short for ${a_1}^{(1,\emptyset)}$. We use both ${a_3}^{1}$ and ${a_3}(1)$ as the short for ${a_3}^{(3,[3:1])}$ when the iteration number matters, similarly for other variables. Because of the property of ssa, we can uniquely find the variable even if we omit the line number.  Then the resulting matrix $M_{tr}$ and $V_{tr}$ of the algorithm $\mathsf{AD}$ as follows.
 %
% { \tiny
%  \[
% M_{tr} = 
% \begin{blockarray}{ccccccccccccccccccccc}
% % & a_1^{1} & a_3^{(2,1)} & x_1^{(3,1)} & a_2^{(4,1)}  & a_3^{(2,2)} & x_1^{(3,2)} & a_2^{(4,2)} & a_3^{2} & l_1^{5}& x & x\\
% & a_1 & i_1 & i_3^{1} & a_3^{1}  & x_1^{1} & a_2^{1} & i_2^{1} & i_3^{2} & a_3^{2}  & x_1^{2} & a_2^{2} & i_2^{2} &i_3^{3} & a_3^{3}  & x_1^{3} & a_2^{3} & i_2^{3} & i_3 & a_3 & l_1\\
% \begin{block}{c[cccccccccccccccccccc]}
%  a_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
%  i_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% i_3({1}) & 0 & 1 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% a_3({1}) & 1 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% x_1({1}) & 0 & 0 & 1 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% a_2({1}) & 0 & 0 & 0 & 1 & 1 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% i_2({1}) & 0 & 0 & 1 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% i_3({2}) & 0 & 1 & 0 & 0 & 0 & 0 & 1 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% a_3({2}) & 1 & 0 & 0 & 0 & 0 & 1 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% x_1({2}) & 0 & 0 & 0 & 0 & 0 & 0 & 0 &1 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% a_2({2}) & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% i_2({2}) & 0 & 0 & 0 & 0 & 0 & 0 & 0 &1 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% i_3({3}) & 0 & 1 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% a_3({3}) & 1 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% x_1({3}) & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% a_2({3}) & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 &0 \\
% i_2({3}) & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 &0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 &0 \\
% i_3 & 0 & 1 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 &0 \\
% a_3 & 1 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 &0 \\
% l_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 &0 \\
% \end{block}
% \end{blockarray}
% ~ , V_{tr} = \left [ \begin{matrix}
% a_1 &  0 \\
% i_1 & 0 \\
% i_3({1}) & 0 \\
% a_3({1}) & 0 \\
% x_1({1}) &  1 \\
% a_2({1}) & 0 \\
% i_2({1}) & 0 \\
% i_3({2}) & 0 \\
% a_3({2}) & 0 \\
% x_1({2}) &  1 \\
% a_2({2}) & 0 \\
% i_2({2}) & 0 \\
% i_3({3}) & 0 \\
% a_3({3}) & 0 \\
% x_1({3}) &  1 \\
% a_2({3}) & 0 \\
% i_2({3}) & 0 \\
% i_3 & 0 \\
% a_3 &  0 \\
% l_1 & 0 \\
% \end{matrix} \right ]
% \]
% }
% The graph is built as in Figure~\ref{fig:ssa_tworound}. The most weighted path has the weight $2$.
%
%% a graph is better here
%
\subsection{ Soundness of {\THESYSTEM}}
We would like to show that the query-based dependency graph generated from the trace of the execution of the target SSA program is a subgraph of the variable-based dependency graph generated by ${\THESYSTEM}$, and the total number of queries asked in the program implementing an adaptive data analysis is also bounded by our algorithm.

We first define when a query-based dependency graph whose nodes are queries is a subgraph of a variable-based dependency graph whose nodes are variables. Intuitively, this happens when there is a mapping of nodes from the first graph to the second.
\begin{defn}
[Subgraph]
Given a query-based dependency graph $G_{s} = (V_1, E_1)$, a variable-based dependency graph $G_{ssa} = (V_2, E_2)$, $G_{s} \subseteq G_{ssa}$ iff:
$\exists f$ so that \\
1. for every $v \in V_1$, $f(v) \in V_2$. 
\\
2. $\forall e=(v_i, v_j) \in E_1$, there exists a path 
% $g(e)$ 
from $f(v_i)$ to $f(v_2)$ in $G_{ssa}$.
\end{defn}

We now show the soundness of {\THESYSTEM}. We use some new definitions. $G \vDash M, V$ means that $G$ matches $M$, $V$ in size. For example, if the cardinality of $G$ is $N$, a matched matrix $M$ is of size $N\times N$ and a good vector $V$ has the same size as $G$. The assumption $G; w \vDash (\ssa{c}, i_1,i_2)$ checks that the variables assigned in $\ssa{c}$ estimated by $\mathsf{VE}$ match the variables in $G$ from index $i_1$ to $i_2$.

\begin{thm}
[Soundness of {\THESYSTEM}]
Given $ \ad{\Gamma; \ssa{c}; i_1 }{M; V;i_2}$,  for any global list $G$,  loop maps $w$ such that $G ;w \vDash (\ssa{c}, i_1, i_2) \land G \vDash (M,V)$, let $K$ be the number of queries made during the execution of the piece of program $\ssa{c}$ and |V| be the number of non-zeros in $V$. 
% $|.|_{low} $ is the annotation erasure, which turns a ssa form program $\ssa{c}$ to its low-level version.
Then,
\[
K \leq |V| \land \forall D, \ssa{m}.\, G_{s}(\ssa{c},D,\ssa{m},w) \subseteq G_{ssa}(M, V,G,i_1, i_2)
\]      
\end{thm}

\begin{coro}
Given $ \ad{\Gamma; \ssa{c}; i_1 }{M; V;i_2}$,  for any global list $G$,  loop maps $w$ such that $G ;w \vDash (\ssa{c}, i_1, i_2) \land G \vDash (M,V)$,
\[
K \leq |V| \land A_s(\ssa{c}, D, \ssa{m}, w) \leq Adapt(M, V,G,i_1, i_2)
\]      
\end{coro}
We have already shown in Lemma~\ref{lem:same_adapt} that for every transformation of a program $c$ to $\ssa{c}$, the adaptivity of $c$ and $\ssa{c}$ is equal. It follows that the bound estimated by {\THESYSTEM} on $\ssa{c}$ is an upper bound on the adaptivity of $c$.  

% \mg{I think we should connect this theorem with our definition of adaptivity, rather than just with the graph. Can we write a corollary about this?}