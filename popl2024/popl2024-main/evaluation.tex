% \subsection{The Implementation Evaluation Results }
% \label{sec:adapt-impleval}

\paragraph{Set Up}
We implemented $\THESYSTEM$ as a tool that takes a labeled command as input 
and outputs two upper bounds on the program adaptivity and the number of query requests respectively.
This implementation consists of an 
abstract control flow graph generation,
edge estimation (as presented in Section~\ref{sec:alg_edgegen}), and weight estimation (as presented in Section~\ref{sec:alg_weightgen}) in Ocaml, 
and the adaptivity computation algorithm shown in Section~\ref{sec:alg_adaptcompute} in Python.
The OCaml program takes the labeled command as input and outputs the program-based dependency graph and
the abstract transition graph,
feeds into the Python program and the Python program provides the adaptivity upper bound and the query number as the final output.


\paragraph{Accuracy and Performance}
\input{eval-table}
We evaluated this implementation on $23$ example programs with the evaluation results shown in Table~\ref{tb:adapt-imp}.

In this table,
the first column is the name of each program.
For each program $c$, the second column is its intuitive adaptivity rounds,
and the third column is the output of the $\THESYSTEM$ implementation, which consists of two expressions.
The first one is the upper bound for adaptivity and the second one is the 
upper bound for the total number of query requests in the program. And the last column is the performance evaluation w.r.t. the program size.

The last column is the performance evaluation.
The time contains three parts. The first part is the running time of the Ocaml code, which parses the program and generates the $\progG(c)$.
The second and third parts are the running times of the reachability bound analysis algorithm
and the adaptivity computation algorithm, $\pathsearch(c)$.

The first six programs are adapted from real-world data analysis algorithms.
The first two programs $\kw{twoRounds(k)}$, $ \kw{multiRounds(k)}$ are the same as Fig.~\ref{fig:overview-example}(a) and Fig.~\ref{fig:multipleRounds}(a).
$\THESYSTEM$ computes tight adaptivity bound for the first three examples.
For the fourth program $\kw{multiRoundsO(k)}$, $\THESYSTEM$ outputs an over-approximated upper bound $1 + 2*k$ 
% for the $A(c)$, which is consistent with our expectation 
as discussed in Example~\ref{ex:multiRoundsO}. 
The fifth program is the evaluation results for the example in Example~\ref{ex:multiRoundsS}, where $\THESYSTEM$ outputs the tightly bound for $A(c)$ but $A(c)$ is a loose definition of the program's actual adaptivity rounds.
%

The next eleven programs from Table~\ref{tb:adapt-imp} rows 7 to 18 are handcrafted programs based on the code pieces extracted from the C library. They all have small sizes but complex structures in order to test the programs under different situations including
data, control dependency,
the multiple paths nested loop with related counters, etc.
The names of these programs obey the convention that,
$\kw{if}$ means there is an if control in the program;
$\kw{loop}$ means there is while loop and $\kw{loop2}$ represents two levels nested loop in the program;
$\kw{C}$ denotes Control;
$\kw{D}$ for Dependency; $\kw{V}$ for Variable;
$\kw{M}$ for Multiple; $\kw{P}$ for Path and $\kw{R}$ for Related.


The algorithm computes the tight bound for examples from line six, $\kw{ifCD()}$ to line fourteen, $\kw{loop2MPRV(k)}$
and over-approximate the \emph{adaptivity} for the $15^{th}$ and $16^{th}$ examples in the table due to path-insensitivity.

The last six programs are synthesized programs composed of the previous programs in order to test the performance limitation when the input program is large. 
From the evaluation results, the performance bottleneck is the reachability bound analysis algorithm.


By implementing the bound analysis algorithm in Section~\ref{sec:alg_weightgen} (adapted from \cite{SinnZV17}), we are unable to evaluate the $\kw{Jumbo}$ in a reasonable time period.

\paragraph{Alternative Implementations}
\input{eval-table-alternatives}
Alternatively, we implement three different versions of {\THESYSTEM} and show our alternative implement evaluation result in Table~\ref{tb:adapt-imp-alternatives}.
 
1. In column {\THESYSTEM}-I, the implementation replaces the reachability bound analysis algorithm in Section~\ref{sec:alg_weightgen} with a light reachability bound analysis algorithm and computes the \emph{adaptivity} for
$\kw{jumboS}, \kw{jumbo}$ and $\kw{big}$ effectively.
The results show that the alternative implementation computes the tight bound for all the examples from line:1 to line:14
and over-approximate the \emph{adaptivity} for $15^{th}$ and $16^{th}$ due to path-insensitivity similar to the
$\THESYSTEM$.
For the $17^{th}$ example ($\kw{loop2RC}$), {\THESYSTEM} gives a tight upper bound while the alternative implementation gives a loose bound, so we keep both implementations.

2. In {\THESYSTEM}-II, we remove the reaching definition analysis from Section~\ref{sec:alg_edgegen}.
The Definition~\ref{def:feasible_flowsto} is replaced by a simple data flow analysis without looking into the variable liveness.
As a result, the estimated data dependency result contains the dependency relation between variables that is ``dead''. In other words, it over-approximates the variable may-dependency relation.

3. In {\THESYSTEM}-III, we remove the control flow analysis from Definition~\ref{def:feasible_flowsto}.
The estimated data dependency only considers the data flow.
However, the adaptivity should consider both the data and control flow, results produced from this version are unsound.

Overall, {\THESYSTEM} gives the accurate estimated
adaptivity upper bound and our adaptivity formalization also gives precise adaptivity definition w.r.t. the intuitive \emph{adaptivity rounds}.
% and analysis framework $\THESYSTEM$.


\paragraph{Effectiveness Evaluation}
\input{eval-table-generalization}
We evaluate our framework also on real-world programs from
three benchmark sets, including TensorFlow, Sklearn, Pytorch, and research paper~\cite{Jamieson2015TheAO}.

These programs include 
four programs from our examples in Table~\ref{tb:adapt-imp},
three programs implementing the strategies from~\cite{Jamieson2015TheAO},
4 data analysis programs 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark
and
7 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}~\cite{TensorflowBenchmark}
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}~\cite{PytorchBenchmark}
benchmarks.
% \hyperlink{https://homes.cs.washington.edu/~jamieson/resources/kevinJamieson\_Dissertation.pdf}{thesis}.

% The 15 data analysis programs 
% from sklearn benchmark includes 12 programs with $O(n)$-adaptivity rounds
% and 3 programs with $O(n*m)$-adapvitiy rounds.
% % for the database classification.
% These $O(n)$-rounds adaptivity programs are
% the
% implementations of the decision tree, logistic regression, naive Gaussian inference classifiers
% with $O(n)$ fitting depth,
% and the grid search hyperparameter selecting algorithm for each classifier with 
% hyperparameter space of constant size $c$.
% The $O(n*m)$-rounds adaptivity programs are
% the implementations of the decision tree, logistic regression, naive Gaussian inference models equipped with one v.s. rest model with $O(n)$ fitting depth for classifying the dataset with $O(m)$ classes.
% These programs are evaluated for classifying a demography dataset from 
% the USA census database~\cite{CensusDatabase}.

For each program, we show in Table~\ref{tb:adapt-generalization} the generalization errors when running without a mechanism
and running with different mechanisms over uniformly generated training data.
The average generalization error of each program is measured by root-mean-square error.
The root-mean-square error without any mechanism is shown in column ``rmse without mechanism''.
Re-running these programs by equipping them with different mechanisms, we present the results
in the columns ``rmse with mechanisms''.
After adaptivity analysis by {\THESYSTEM}, we highlight the rmse produced by the mechanism chosen by our tool.

In short, all these data analysis programs behave overfitting when running over randomly generated data sets. 
To obtain good data analysis result with low generalization error,
one needs to run programs multiple times with different mechanisms.
This takes much longer time than just training a data analysis program with one run.
However, by using our tool in the first step, an analyst can directly equip the program with a chosen mechanism to get good
analysis results.


We first evaluate four example programs from Table~\ref{tb:adapt-imp} as the baseline evaluation to show that our tool can choose the mechanism that performs best in reducing the generalization error as we claimed.
$\kw{twoRound}$ is trained to classifying a uniformly generated data set of size $n$ and $k$ features and a randomly selected label from $\{0, 1\}$. 
With only two adaptive rounds, by instantiating $k = 1000$ and $n = 1000$, this algorithm overfitted and produce the generalization error 
$0.0502245$. As shown in Fig.~\ref{fig:generalization_errors}(a) and (c) already,
the DataSplit mechanism reduced the generalization error the most.
The $2^{nd}$ to $4^{th}$ program is evaluated to fitting a two-dimensional data set of size $n$ into a $1$ degree linear relation.
The fitting depth is $k$.
Our tool identifies the adaptivity rounds are $O(k)$ for these programs and chooses the Gaussian mechanism.
By instantiating $k = 1000$ and $n = 1000$, the evaluation results show that our chosen mechanism performs best among other mechanisms in reducing the generalization error.

The next three data analysis programs are implemented from~\cite{Jamieson2015TheAO}.
Program $\kw{repeatedQueryRoutine}$ is the
repeated query subroutine algorithm with $n\times 2^n$ adaptivity rounds and $n\times 2^n$ query requests in total;
$\kw{lil-UCB}$ is the n-dimension pairwise algorithm with $n$ adaptivity rounds and $n$ query requests in total;
and $\kw{nDimensionPairwise}$ is the lil-ucb algorithm with $n$ adaptivity rounds and $n^2$ queries.
These three algorithms are implemented into the \emph{Guess and Check}~\cite{RogersRSSTW20} framework for ranking
a $1$-demensional dataset of size $n$.
When running it, we are using the same mechanisms pre-defined in this framework as evaluating the first three examples.

By adaptivity analysis from our tool, they all have high adaptivity rounds.
We instantiate $n = 10000$ and these programs produce large generalization errors, which matches our intuition.
According to the large adaptivity rounds and the query numbers, our tool chooses the Gaussian mechanism for these programs, which reduces the rmse the most
among the other mechanisms.


For the four data analysis programs, 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark,
the first two programs, $\kw{decisionTree}$ and $\kw{logisticRegression}$ are
the
implementations of the decision tree, logistic regression classifiers
with $O(n)$ fitting depth,
and the grid search hyperparameter selects an algorithm for each classifier with 
hyperparameter space of constant size $c$.

The next two programs $\kw{decisionTreeOVR}$ and $\kw{logisticRegressionOVR}$ are
the implementations of the decision tree, logistic regression, equipped with one v.s. rest model with $n$ fitting depth for classifying the dataset with $m$ classes.
They are evaluated to classify a uniformly generated data set with $m$ randomly selected classes label.

For these programs, we translate them manually into our syntax and evaluate their adaptivity.
By adaptivity analysis, our tool identified that the fitting depth is indeed the adaptivity rounds of these programs.
The program $\kw{decisionTree}$ with only $5$ fitting depth has adaptivity rounds $5$,
the Thresholdout mechanism performs best in terms of reducing the generalization error.

Programs $\kw{decisionTreeOVR}$ and $\kw{logisticRegressionOVR}$ 
have $n*m$ adaptivity rounds, by instantiating $n = 1000$ and $m = 500$,
the Gaussian mechanism performs best when reducing the generalization error.








The 7 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}
benchmarks include:
3 fully-adaptive progress data analysis
strategy~\cite{ZrnicH19},
specifically the implementation of the recurrent neural network, long-short-term models;
and 4
fully-adaptive conservative data analysis
strategy~\cite{ZrnicH19},
specifically the implementation of the convolutional neural network, transfer learning model,
and multiple-dense layer neural networks equipped with gradient decedent optimizer.

We evaluate them for classifying a uniformly generated data set of size $n$ and dimension $100$ with fitting steps
$k$ and training epoch $m$.
The computation in one fitting step is translated into a simple linear query in our language.
In this sense, {\THESYSTEM} is only able to figure the dependency relation between each fitting steps and
training epochs, which is $k \times m$.
We instantiate the $n = 10000$, $k = 10$, and $m = 1$,
and {\THESYSTEM} produces adaptivity estimation as $10$ for these programs.
It chooses the Thresholdout mechanism, which produces a smaller generalization error than the result without any mechanism.
However, in the evaluation results from this benchmark, the Thresholdout mechanism is not always the best one among others.
This is because each fitting step is not exactly a simple linear query in the neural network,
{\THESYSTEM} fails to recognize the relation between layers of the neural networks in one fitting step.

