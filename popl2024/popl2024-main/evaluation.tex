% \subsection{The Implementation Evaluation Results }
% \label{sec:adapt-impleval}

\paragraph{Set Up}
We implemented $\THESYSTEM$ as a tool that takes a labeled command as input 
and outputs two upper bounds on the program adaptivity and the number of query requests respectively.
This implementation consists of an 
abstract control flow graph generation,
edge estimation (as presented in Section~\ref{sec:alg_edgegen}), and weight estimation (as presented in Section~\ref{sec:alg_weightgen}) in Ocaml, 
and the adaptivity computation algorithm shown in Section~\ref{sec:alg_adaptcompute} in Python.
The OCaml program takes the labeled command as input and outputs the program-based dependency graph and
the abstract transition graph,
feeds into the python program and the python program provides the adaptivity upper bound and the query number as the final output.


\paragraph{Effectiveness}
\input{eval-table}
We evaluated this implementation on $23$ example programs with the evaluation results shown in Table~\ref{tb:adapt-imp}.

In this table,
the first column is the name of each program.
For each program $c$, the second column is its intuitive adaptivity rounds,
and the third column is the output of the $\THESYSTEM$ implementation, which consists of two expressions.
The first one is the upper bound for adaptivity and the second one is the 
upper bound for the total number of query requests in the program. And the last column is the performance evaluation w.r.t. the program size.

The last column is the performance evaluation.
The time contains three parts. The first part is the running time of the Ocaml code, which parses the program and generates the $\progG(c)$.
The second and third parts are the running times of the reachability bound analysis algorithm
and the adaptivity computation algorithm, $\pathsearch(c)$.

The first six programs are adapted from real-world data analysis algorithms.
The first two programs $\kw{twoRounds(k)}$, $ \kw{multiRounds(k)}$ are the same as Figure~\ref{fig:overview-example}(a) and Figure~\ref{fig:multipleRounds}(a).
$\THESYSTEM$ computes tight adaptivity bound for the first three examples.
For the fourth program $\kw{multiRoundsO(k)}$, $\THESYSTEM$ outputs an over-approximated upper bound $1 + 2*k$ 
% for the $A(c)$, which is consistent with our expectation 
as discussed in Example~\ref{ex:multiRoundsO}. 
The fifth program is the evaluation results for the example in Example~\ref{ex:multiRoundsS}, where $\THESYSTEM$ outputs the tightly bound for $A(c)$ but $A(c)$ is a loose definition of the program's actual adaptivity rounds.
%

The next eleven programs from Table~\ref{tb:adapt-imp} rows 7 to 18 are handcrafted programs based on the code pieces extracted from the C library. They all have small sizes but complex structures in order to test the programs under different situations including
data, control dependency,
the multiple paths nested loop with related counters, etc.
The names of these programs obey the convention that,
$\kw{if}$ means there is if control in the program;
$\kw{loop}$ means there is while loop and $\kw{loop2}$ represents two levels nested loop in the program;
$\kw{C}$ denotes Control;
$\kw{D}$ for Dependency; $\kw{V}$ for Variable;
$\kw{M}$ for Multiple; $\kw{P}$ for Path and $\kw{R}$ for Related.


The algorithm computes the tight bound for examples from line six, $\kw{ifCD()}$ to line fourteen, $\kw{loop2MPRV(k)}$
and over-approximate the \emph{adaptivity} for the $15^{th}$ and $16^{th}$ examples in the table due to path-insensitivity.

The last six programs are synthesized programs composed of the previous programs in order to test the performance limitation when the input program is large. 
From the evaluation results, the performance bottleneck is the reachability bound analysis algorithm.


By implementing the bound analysis algorithm in Section~\ref{sec:alg_weightgen} (adapted from \cite{SinnZV17}), we are unable to evaluate the $\kw{Jumbo}$ in a reasonable time period.

\paragraph{Alternative Implementations}
\input{eval-table-alternatives}
Alternatively, we implement three different versions of {\THESYSTEM} and show our alternative implement evaluation result in Table~\ref{tb:adapt-imp-alternatives}.
 
1. In column {\THESYSTEM}-I, the implementation replaces the reachability bound analysis algorithm in Section~\ref{sec:alg_weightgen} with a light reachability bound analysis algorithm and compute the \emph{adaptivity} for
$\kw{jumboS}, \kw{jumbo}$ and $\kw{big}$ effectively.
The results show that the alternative implementation computes the tight bound for all the examples from line:1 to line:14
and over-approximate the \emph{adaptivity} for $15^{th}$ and $16^{th}$ due to path-insensitivity similar to the
$\THESYSTEM$.
For the $17^{th}$ example ($\kw{loop2RC}$), {\THESYSTEM} gives a tight bound while the alternative implementation gives a loose bound, so we keep both implementations.

2. In {\THESYSTEM}-II, we remove the reaching definition analysis from Section~\ref{sec:alg_edgegen}.
The Definition~\ref{def:feasible_flowsto} is replaced by a simple data flow analysis without looking into the variable liveness.
As a result, the estimated data dependency result contains the dependency relation between variables that is ``dead''. In the other words, it over-approximates the variable may-dependency relation.

3. In {\THESYSTEM}-III, we remove the control flow analysis from Definition~\ref{def:feasible_flowsto}.
The estimated data dependency only consider the data flow.
However, the adaptivity should consider both the data and control flow, results produced from this version are unsound.

Overall, {\THESYSTEM} gives the accurate estimated
adaptivity upper bound and our adaptivity formalization also gives precise adaptivity definition w.r.t. the intuitive \emph{adaptivity rounds}.
% and analysis framework $\THESYSTEM$.


\paragraph{Effectiveness Evaluation}
\input{eval-table-generalization}
We evaluate our framework also on the real-world programs from
three benchmark sets, including thensorflow, sklearn and pytorch and research paper~\cite{Jamieson2015TheAO}.

These programs include  
4 data analysis programs 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark,
7 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}~\cite{TensorflowBenchmark}
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}~\cite{PytorchBenchmark}
benchmarks,
{three programs implementing the strategies from~\cite{Jamieson2015TheAO}}
and four programs from our examples in Table~\ref{tb:adapt-imp}.
% \hyperlink{https://homes.cs.washington.edu/~jamieson/resources/kevinJamieson\_Dissertation.pdf}{thesis}.

% The 15 data analysis programs 
% from sklearn benchmark includes 12 programs with $O(n)$-adaptivity rounds
% and 3 programs with $O(n*m)$-adapvitiy rounds.
% % for the database classification.
% These $O(n)$-rounds adaptivity programs are
% the
% implementations of the decision tree, logistic regression, naive Gaussian inference classifiers
% with $O(n)$ fitting depth,
% and the grid search hyperparameter selecting algorithm for each classifier with 
% hyperparameter space of constant size $c$.
% The $O(n*m)$-rounds adaptivity programs are
% the implementations of the  decision tree, logistic regression, naive Gaussian inference models equipped with one v.s. rest model with $O(n)$ fitting depth for classifying the dataset with $O(m)$ classes.
% These programs are evaluated for classifying a demography dataset from 
% the USA census database~\cite{CensusDatabase}.

For each program, we show in Table~\ref{tb:adapt-generalization} the generalization errors when run it without mechanism.
The average generalization error of each program is measured by root-mean-square error over uniformly generated training data.
The root-mean-square error without any mechanism is shown in column ``rmse without mechanism''.
Then re-running all of them by equipping these program with different mechanisms, we present the result
in the columns ``rmse with mechanisms''.
After analyzing result by {\THESYSTEM}, we highlight the rmse produced by the mechanism chosen by our tool.

In general, all of these programs behave overfitting by running over random data set. 
Training a data analysis program usually takes long time, to obtain good data analysis result with low generalization error,
one needs to run programs multiple times with different mechanisms.
However, by using our tool in the first step, analyst can directly equip the program with chosen mechanism to get good
analysis result.


We first evaluate four example programs from Table~\ref{tb:adapt-imp} as the baseline evaluation to show that our tool can choose the mechanism that performs best in reducing the generalization error as we claimed.
$\kw{twoRound}$ is trained to classifying a uniformly generated data set with $n$ features and a randomly selected label from $\{0, 1\}$. 
With only two adaptive rounds, by instantiating $n = 1000$, this algorithm over-fitted and produce generalization error 
$0.0502245$. As we showed in Figure~\ref{fig:generalization_errors}(a) and (c),
DataSplit mechanism reduced the generalization error the most.
The $2^{nd}$ to $4^{th}$ program are evaluated to fitting a random data 


The next three data analysis programs are implemented from~\cite{Jamieson2015TheAO}.
Program $\kw{repeatedQueryRoutine}$ is the
repeated query subroutine algorithm with $n\times 2^n$ adaptivity rounds and $n\times 2^n$ query requests in total;
$\kw{lil-UCB}$ is the n-dimension pairwise algorithm with $n$ adaptivity rounds and $n$ query requests in total;
and $\kw{nDimensionPairwise}$ is the lil-ucb algorithm with $n$ adaptivity rounds and $n^2$ queries.
These three algorithms are implemented into the GnC~\cite{BassilyNSSSU16} framework for ranking
a $1$-demensional dataset of size $n$.
When running it, we are using the same mechanisms pre-defined in this framework.
This framework is also used to evaluate the running example presented in overview section.

By adaptivity analysis from our tool, they all have high adaptivity rounds.
We instantiate $n = 10000$ and these programs produce large generalization error, which matches our intuition.
According to the high generalization error, we choose Gaussian mechanism for these programs, which reduces the rmse the most
among the other mechanisms.


For the four data analysis programs, 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark,
the first two programs, $\kw{decisionTree}$ and $\kw{logisticRegression}$ are
the
implementations of the decision tree, logistic regression classifiers
with $O(n)$ fitting depth,
and the grid search hyperparameter selecting algorithm for each classifier with 
hyperparameter space of constant size $c$.

The next two programs $\kw{decisionTreeOVR}$ and $\kw{logisticRegressionOVR}$ are
the implementations of the  decision tree, logistic regression, equipped with one v.s. rest model with $O(n)$ fitting depth for classifying the dataset with $O(m)$ classes.
They are evaluated to classifying a uniformly generated data set with $m$ randomly selected classes label.

For these program, we translate them manually into our syntax and evaluated their adaptivity.
By adaptivity analysis, our tool identified that the fitting depth is indeed the adaptivity rounds for these programs.
The program $\kw{decisionTree}$ with only $5$ fitting depth has adaptivity rounds $5$,
the Thresholdout mechanism performs best in terms of reducing the generalization error.

Programs $\kw{decisionTreeOVR}$ and $\kw{logisticRegressionOVR}$ 
have $n*m$ adaptivity rounds, by instantiating $n = 1000$ and $m = 500$,
Gaussian mechanism performs best when reducing the generalization error.








The 7 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}
benchmarks include:
\\
3 fully-adaptive progress data analysis
strategy~\cite{ZrnicH19}.
Specifically they are the implementation of the recurrent neural network, long short term models for 
weather forecasting.
\\
9
fully-adaptive conservative data analysis
strategy~\cite{ZrnicH19}.
Specifically they are the implementation of the convolutional neural network, transfer learning model,
and multiple-dense layer neural networks equipped with gradient decedent optimizer for
weather forecasting and image classification.
%  over MNIST dataset
% through fitting the  CNN, RNN, LSTM models for image classification and 
% weather forecasting as well.
\\
9 programs with two adaptivity rounds for text classification,
article classification, action recognition over real world databases.
