% \subsection{The Implementation Evaluation Results }
% \label{sec:adapt-impleval}

\paragraph{Set Up}
We implemented $\THESYSTEM$ as a tool that takes a labeled command as input 
and outputs two upper bounds on the program adaptivity and the number of query requests respectively.
This implementation consists of an 
abstract control flow graph generation,
edge estimation (as presented in Section~\ref{sec:alg_edgegen}), and weight estimation (as presented in Section~\ref{sec:alg_weightgen}) in Ocaml, 
and the adaptivity computation algorithm shown in Section~\ref{sec:alg_adaptcompute} in Python.
The OCaml program takes the labeled command as input and outputs the program-based dependency graph and
the abstract transition graph, which we
feed into the Python program and the Python program provides the adaptivity upper bound and the query number as the final output.


\paragraph{Accuracy and Performance}
\input{eval-table}
We first evaluate our implementation in terms of its accuracy and performance in estimating the adaptivity. To do this we consider $23$ example programs with the evaluation results shown in Table~\ref{tb:adapt-imp}.

In this table,
the first column is the name of each program, the second column is the program adaptivity,
the third column is the output of the $\THESYSTEM$ implementation, which consists of two expressions:
the first one is the upper bound for adaptivity and the second one is the 
upper bound for the total number of query requests in the program. And the last column is the performance evaluation w.r.t. the program size in terms of lines of code.
The performance evaluation contains three parts. The first part is the running time of the Ocaml code, which parses the program and generates the $\progG(c)$.
The second and third parts are the running times of the reachability bound analysis algorithm
and the adaptivity computation algorithm, $\pathsearch(c)$.

The examples show a wide range of values for their adaptivity. 
The first two programs $\kw{twoRounds(k)}$, $ \kw{mR(k)}$ are the same as Fig.~\ref{fig:overview-example}(a) and Fig.~\ref{fig:multipleRounds}(a).
$\THESYSTEM$ computes tight adaptivity bound for the first three examples.
The third example is a logistic regression algorithm with gradient decent.
For the fourth program $\kw{mROdd(k)}$, $\THESYSTEM$ outputs an over-approximated upper bound $2 + \max(1, 2k)$ 
% for the $A(c)$, which is consistent with our expectation 
because of the path insensitive nature of the weight estimation algorithm, as discussed in Section~\ref{sec:examples}. 
The $5^{th}$ program, $\kw{mRSingle(k)}$ is the example program in Fig.~\ref{ex:multiRoundsS}.
$\THESYSTEM$ outputs $1 + \max(1, k) $, which is tight with respect to our adaptvitiy definition in Def.~\ref{def:trace_adapt}.
However, because Def.~\ref{def:trace_adapt} is a loose definition of $\kw{mRSingle(k)}$'s actual adaptivity rounds,
%the result is still an over-approximation.
%

The next thirteen programs in Table~\ref{tb:adapt-imp} from $6^{th}$ to $19^{th}$ row are handcrafted programs based on benchmarks from the work by Gulwani et al.~\cite{GulwaniJK09}. They all have small sizes but complex structures in order to test the programs under different situations including
data, control dependency,
the multiple paths nested loop with related counters, etc.
The names of these programs obey the convention that,
$\kw{if}$ means there is an if control in the program;
$\kw{loop}$ means there is while loop and $\kw{loop2}$ represents two levels nested loop in the program;
$\kw{C}$ denotes Control;
$\kw{D}$ for Dependency; $\kw{V}$ for Variable;
$\kw{M}$ for Multiple; $\kw{P}$ for Path and $\kw{R}$ for Related.


Our algorithm computes tight bounds for the adaptivity of programs from row six, $\kw{seqRV()}$ to row fourteen, $\kw{loop2MPRV(k)}$
and over-approximate the \emph{adaptivity} for the  examples in row fiften and sixteen because of the path-insensitive nature of the algorithm computing the weights.

The last six programs are synthesized programs composed of the previous programs in order to test the performance limitation when the input program is large. 
From the evaluation results, the performance bottleneck is the reachability bound analysis algorithm used to compute the weights.

The reachability-bound analysis algorithm presented in Section~\ref{sec:alg_weightgen} time out after $5$ minutes when evaluating the examples $\kw{tRCom(k)}$,  $\kw{jumbo}$ and ${\kw{big(k)}}$.

\paragraph{Alternative Implementations}
\input{eval-table-alternatives}
With our second evaluation we want to show the need of the different
components of {\THESYSTEM}. To do this we implement three alternative
versions of {\THESYSTEM} where the different components are omitted or
replaced with components with weaker guarantees and we compare them with {\THESYSTEM}. We show the
results  of the comparison in
Table~\ref{tb:adapt-imp-alternatives}. We use the same set of example programs as in
Table~\ref{tb:adapt-imp} and present their estimated adaptivity
rounds, query numbers and the running time.  For each alternative
implementation, we marked the result in red if it is imprecise
w.r.t. the true value.

1. In column {\THESYSTEM}-I, the implementation replaces the reachability bound analysis algorithm in Section~\ref{sec:alg_weightgen} with a light reachability bound analysis algorithm and computes the \emph{adaptivity} for
$\kw{jumboS}, \kw{jumbo}$ and $\kw{big}$ effectively.
The results show that the alternative implementation computes the tight bound for all the examples from line:1 to line:14
and over-approximate the \emph{adaptivity} for $15^{th}$ and $16^{th}$ due to path-insensitivity similar to the
$\THESYSTEM$.
For the $17^{th}$ example ($\kw{loop2R}$), {\THESYSTEM} gives a tight upper bound while the alternative implementation gives a loose bound, so we keep both implementations.

2. In {\THESYSTEM}-II, we remove the control flow analysis from Definition~\ref{def:feasible_flowsto}.
The estimated data dependency only considers the data flow.
However, the adaptivity should consider both the data and control flow, and so results produced from this version are unsound.
For example, program $\kw{ifCD}$, $ \kw{loopMPVCD(k)} $ and $\kw{loopVCD(k)} $ all have control dependency.
However, the alternative implementation II in Table~\ref{tb:adapt-imp-alternatives} estimates the adapvitiy rounds of $2$ because it fails to identify the dependency through control flow.

% 3. In {\THESYSTEM}-III, we remove the reaching definition analysis from Section~\ref{sec:alg_edgegen}.
% The Definition~\ref{def:feasible_flowsto} is replaced by a simple data flow analysis without looking into the variable liveness.
% As a result, the estimated data dependency result contains the dependency relation between variables that is ``dead''. In other words, it over-approximates the variable may-dependency relation.

3. In {\THESYSTEM}-III, we remove the reachability bound estimation from Section~\ref{sec:alg_weightgen}.
We assign the weight $1$ to every labeled variable on the estimated dependency graph.
The adaptivity estimation results are all integers and does not depend on any program inputs.
When instantiating the input variable with varied values, this estimated result is either greater or smaller than the true adaptivity.

In the last column, we present only the performance of the {\THESYSTEM}-I to compare the
light reachability-bound analysis implementation with the original reachability-bound analysis.
The performance of {\THESYSTEM}-II is similar to {\THESYSTEM} because they are based on the same
reachability-bound analysis implementation. For the same reason, the {\THESYSTEM}-III has slightly better
running time than {\THESYSTEM}-I.
Overall, {\THESYSTEM} gives accurate estimated
adaptivity upper bounds.
% and analysis framework $\THESYSTEM$.


\paragraph{Effectiveness Evaluation}
\input{eval-table-generalization}
With our last evaluation we want to show the effectiveness of {\THESYSTEM} in terms of reducing the generalization error of real-world data analysis programs.
%  from Sklearn machine learn benchmark sets and research paper~\cite{Jamieson2015TheAO}.

To do this we consider a set of benchmarks including 
nine real-world data analyses from our examples in Table~\ref{tb:adapt-imp} instantiated with different parameters,
four programs implementing the algorithms from~\cite{Jamieson2015TheAO}
and four data analysis programs 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark.
% 7 machine learning programs
% from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}~\cite{TensorflowBenchmark}
% and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}~\cite{PytorchBenchmark}
% benchmarks.
% The 15 data analysis programs 
% from sklearn benchmark includes 12 programs with $O(n)$-adaptivity rounds
% and 3 programs with $O(n*m)$-adapvitiy rounds.
% % for the database classification.
% These $O(n)$-rounds adaptivity programs are
% the
% implementations of the decision tree, logistic regression, naive Gaussian inference classifiers
% with $O(n)$ fitting depth,
% and the grid search hyperparameter selecting algorithm for each classifier with 
% hyperparameter space of constant size $c$.
% The $O(n*m)$-rounds adaptivity programs are
% the implementations of the decision tree, logistic regression, naive Gaussian inference models equipped with one v.s. rest model with $O(n)$ fitting depth for classifying the dataset with $O(m)$ classes.
% These programs are evaluated for classifying a demography dataset from 
% the USA census database~\cite{CensusDatabase}.

{We use acronyms for these programs to have a better presentation of our evaluation in a single table. We use
mRsgl for mRSingle, 3(4)DeLRGD for 3(4)DegreeLRGD, RQ for repeatedQuery, nDPair for nDimPairwise,
DT, DTOVR for decisionTree abd decisionTreeOVR, LR, LROVR for logisticRegression and logisticRegressionOVR. Similarly, we shorten the mechanisms: GS for Gaussian, DS for Data Split,
TS for ThresholdOut.  }

For each program, we show in Table~\ref{tb:adapt-generalization} the
generalization errors when running without a mechanism and running
with different mechanisms over uniformly generated training data.  The
average generalization error of each program is measured by
root-mean-square error.  The root-mean-square error without any
mechanism is shown in column ``None''.  we mark in bold the rmse
produced by the mechanism chosen by {\THESYSTEM} using the following
heuristics.  According to Theorem~\ref{thm:gaussiannoise},
Theorem~\ref{thm:gaussiannoise2} and Theorem~\ref{thm:nonadapt-adapt}.
we compare the value of $\sqrt{\query \#}$ and
$\progA\sqrt{\log(\query \#)}$ and choose DataSplit if
$\sqrt{\query \#} \gg \progA\sqrt{\log(\query \#)}$ and Gaussian
mechanism if $\sqrt{\query \#} \ll \progA\sqrt{\log(\query \#)}$ and
Thresholdout mechanism if these two quantities are close.  It is worth
stressing that these are rough heuristics since we are omitting the
constants hidden in these theorems. We leave the problem to identify
the best heuristic to future work.


%% In short, all these data analysis programs behave overfitting when running over randomly generated data sets. 
%% To obtain good data analysis result with low generalization error,
%% one needs to run programs multiple times with different mechanisms.
%% This takes much longer time than just training a data analysis program with one run.
%% However, by using our tool in the first step, an analyst can directly equip the program with a chosen mechanism to get good
%% analysis results.


We first evaluate example programs from Table~\ref{tb:adapt-imp} as the baseline evaluation to show that our tool can choose the mechanism that performs best in reducing the generalization error as we claimed.
$\kw{twoRound}$ is trained to classify a uniformly generated data set of size $n$ and $k$ features and a randomly selected label from $\{0, 1\}$. 
With only two adaptive rounds, by instantiating $k = 1000$ and $n = 1000$, this algorithm overfitted and produced the generalization error 
$0.0502245$ (In the table, we approximate it to $0.050$).
Since $2* \sqrt{\log(1000)} \ll \sqrt{1000}$, {\THESYSTEM} chooses the DataSplit mechanism.
Evaluation result(the bold $0.028$ in the first row of the table) shows that we choose the best.
Then by instantiating $k = 10$, we see that $10 * \sqrt{\log(10)}$ is close to $\sqrt{10}$ and our tool chooses ThresholdOut mechanism, while Gaussian mechanism and ThresholdOut both perform well.

The $2^{nd}$ to $5^{th}$ programs represent different algorithms for fitting a two-dimensional data set of size $n$ into a linear relation.
The number of iterations is $k$ and our tool identifies this as the  adaptivity  for these programs.
%  and chooses the Gaussian mechanism.
By instantiating $k = 1000$ and $n = 1000$, we have
$1000* \sqrt{\log(1000)} \gg \sqrt{1000}$ which means that our tool chooses the Gaussian mechanism. When $k=10$ and $n=10$ instead, our tool chooses the ThresholdOut mechanism. 
The evaluation results in Table~\ref{tb:adapt-generalization} shows again that the chosen mechanism performs best among other mechanisms in reducing the generalization error.

$  \kw{ 3DimLRGD }$ and $  \kw{ 4DimLRGD }$ are the generalizations of the logistic regression with gradient decedent
algorithm into three-dimensional and four-dimensional data set of size $n$.
%$  \kw{ 3DegreeLRGD }$ and $  \kw{ 4DegreeLRGD }$ are as well extending the logistic regression with gradient decedent
%algorithm into the fitting tasks of $3$ and $4$ degree polynomial relation on three-dimensional and four-dimensional data sets of size $n$ respectively.
The number of iterations (i.e., the fitting depth) is still $k$, and {\THESYSTEM} estimates correctly the adaptivity rounds are still $k$ for these two programs.
Instantiating $k$ and $n$ with $1000$ and $1000$,
%  the heuristics in Theorem~\ref{thm:gaussiannoise} and Theorem~\ref{thm:gaussiannoise2},
our tool chooses the Gaussian mechanism which still reduces the generalization error the most. When we instantiate $k$ and $n$ with $10$ and $10$, the ThresholdOut mechanism recommended by our tool also performs very well as shown in Table~\ref{tb:adapt-generalization}. 

We have four data analyses which are the implementation of algorithms from~\cite{Jamieson2015TheAO} within the \emph{Guess and Check}~\cite{RogersRSSTW20} framework.
% is the
% repeated query subroutine algorithm with $n\times 2^n$ adaptivity rounds and $n\times 2^n$ query requests in total;
Program $\kw{lilUCB}$ and $\kw{bestArm}$ are the algorithms to solve the best arm problem in the stochastic multi-armed bandit (MAB) setting
with $m$ total number of arms in order to identify the longest arm through sampling at most $n$ times.
Our tool identifies the number of sampling times $n$ is the adaptivity rounds.
%  with $n$ adaptivity rounds and $n$ query requests in total;
Program $\kw{repeatedQuery}$ and $\kw{nDimPairwise}$ are the algorithms for ranking
a $1$-demensional dataset of size $n$ via querying the data $m$ times.
Our tool identifies the adaptvitiy w.r.t. the sampling times for these two programs as $m$ and $m \times 2^m$.

% These three algorithms are implemented into the \emph{Guess and Check}~\cite{RogersRSSTW20} framework 
% When running it, we are using the same mechanisms pre-defined in this framework as evaluating the first three examples.

By instantiating $n = 1000$ and $m = 1000$, these programs produce large generalization errors.
In this situation, $1000* \sqrt{\log(1000^2)}$ and $\sqrt{1000^2}$ are close to each other,
% According to the large adaptivity rounds and the query numbers, 
our tool chooses the Thresholdout mechanism.
Evaluation results in Table~\ref{tb:adapt-generalization} shows that both Thresholdout and Gaussian mechanism can reduce the rmse the most.
When we look at a smaller $n$ and $m$ ($n,m=10$), because the query number is $m \times n$ and our $A_{est}$ is either $m$ or $n$ in these examples,
our heuristic still chooses ThresholdOut mechanism in three of the four examples in the \emph{Guess and Check} framework, $\kw{lilUCB}$, $\kw{nDimPairwise}$ and $\kw{bestArm}$ and our recommendation performs well. The only
exception is the $\kw{repeatedQuery}$ example, whose $A_{est}$ and query number are both quite big as $m \times 2^m$, and out tool
then chooses Gaussian mechanism, which does not perform well($6.01$).


For the four data analysis programs, 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark,
the first two programs, $\kw{decisionTree}$ and $\kw{logisticRegression}$ are
the
implementations of the decision tree, logistic regression classifiers
with $O(n)$ fitting depth,
and the grid search hyperparameter selects an algorithm for each classifier with 
hyperparameter space of constant size $c$.
The next two programs $\kw{decisionTreeOVR}$ and $\kw{logisticRegressionOVR}$ are
the implementations of the decision tree, logistic regression, equipped with one v.s. rest model with $k$ fitting depth for classifying the dataset with $m$ classes.
They are evaluated to classify a uniformly generated data set with $m$ randomly selected classes label. For these programs, we translate them manually into our syntax and evaluate their adaptivity.
By adaptivity analysis, our tool identified that the fitting depth is indeed the adaptivity rounds of these programs.
% The program $\kw{decisionTree}$ with only $5$ fitting depth has adaptivity rounds $5$,
% the Thresholdout mechanism performs best in terms of reducing the generalization error.
Programs $\kw{decisionTreeOVR}$ and $\kw{logisticRegressionOVR}$ 
have $k*m$ adaptivity rounds, by instantiating $k = 1000$ and $m = 1000$, {\THESYSTEM} chooses Gaussian mechanism, although it is not optimal for the decision tree example. While when we have a smaller $k$ and $m$ ($k,m=10$), the choice of our tool is reliable.


In summary, our experimental result shows that most of the time the knowledge of the adaptivity and the number of queries of a data analysis allow us to choose the best mechanism to  reduce the generalization error. We expect that this situation can be further improved by using more precise heuristic than the ones we use here. 



