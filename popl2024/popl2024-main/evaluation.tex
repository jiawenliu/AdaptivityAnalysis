
We implemented $\THESYSTEM$ as a tool which takes a labeled program as input
and outputs the upper bound on the program adaptivity and the total number of queries that the program runs.
This implementation consists of a module written in OCaml for the generation of the estimated graph $\progG$, and a module written in Python for the weight estimation algorithm (Section~\ref{sec:alg_weightgen}) and the algorithm $\pathsearch$ (Section~\ref{sec:alg_adaptcompute}).
The OCaml program takes the labeled program as input and outputs a version of the graph $\progG$ (without weights) and the abstract transition graph $\absG$ for the program. These two objects are then
fed into the python program which computes the weights, and outputs the adaptivity bound and the query number.
%
We evaluated this implementation on $25$ examples with performances summarized in Tab.~\ref{tb:adapt-imp}.
The $1^{st}$ column is the example name.
For each example $c$, the $2^{nd}$ column is its adaptivity rounds,
$\THESYSTEM$ outputs are in the the $3^{rd}$ and $4^{th}$ columns. They are
% two expressions.
% The first one is 
the adaptivity upper bound and
% the second one is the 
% upper bound for
$c$'s total query requests \#. 
The last $4$ columns are {\THESYSTEM}'s performance w.r.t. the program lines.
We track the running time of the OCaml code for parsing the program and generating the $\progG(c)$,
and the running times of the weight analysis
and the $\pathsearch(c)$ in Python.
We implemented two weight estimation methods. The first one (referred as I in Tab.\ref{tb:adapt-imp}) is the one we presented formally in Section~\ref{sec:alg_weightedgegen}. Unfortunately, this method is accurate but slow, it doesn't performs well with big program. The second one (referred as II) is a relaxation of the first one. It is more  efficient but it over-approximate complicated loops. Based on the two implementations, our $\THESYSTEM$ produces two bounds on the adaptivity, corresponding to the left and right side (I | II) in the $3^{rd}$, $4^{th}$ and $6^{th}$ columns\footnote{When the method II produces the same results as I, we omit them and use the symbol $-$.}.
%
The first $5$ programs are adapted from classical data analysis algorithms.
$\THESYSTEM$ computes tight adaptivity bound for the first 3.
For the forth program $\kw{multiRoundsO(k)}$, $\THESYSTEM$ over-approximates the adaptivity as $1 + 2*k$ because of its path-insensitivity.
The fifth program is the one in Example~\ref{ex:multiRoundsS}, where $\THESYSTEM$ outputs the tight bound but we give a loose definition for its actual adaptivity.
%
% The first two examples $\kw{twoRounds(k)}$, $ \kw{multiRounds(k)}$ are the same as Figure~\ref{fig:overview-example}(a) and Figure~\ref{fig:multipleRounds}(a).
The programs from Tab.~\ref{tb:adapt-imp} line:6-17 all have small size but complex structures, to test the programs under different situations including
data, control dependency,
the multiple paths nested loop with related counters, etc.
Both implementations compute tight bounds for examples in line:6-14
and over-approximate the adaptivities for $15^{th}$ and $16^{th}$ due to path-insensitivity.
For the $17^{th}$ one, implementation I gives tight bound bound while II gives loose bound, so we keep both implementations.
The last six programs are big but simple,
to test the performance limitation. 
From the evaluation results, the performance bottleneck is the weight estimation algorithm.
The implementation I
% By implementing the bound analysis algorithm in Section~\ref{sec:alg_weightgen} (adapted from \cite{sinn2017complexity}), we are 
is unable to evaluate them in a reasonable time period, denoted by $*$ on the left side.
While the implementation II computes the \emph{adaptivity} for
them effectively on the right side. 
% Overall for these examples, our system gives both the accurate adaptivity definition and estimated
% adaptivity upper bound through our formalization and analysis framework $\THESYSTEM$.
% The complete programs can be found in the Appendix.

\input{table}
\input{table-alternatives}

We evaluate our framework also on the real-world programs from
three benchmark sets, including thensorflow, sklearn and pytorch and research paper~\cite{Jamieson2015TheAO}.

These programs include  
15 data analysis programs 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark,
21 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}~\cite{TensorflowBenchmark} 
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}~\cite{PytorchBenchmark}
benchmarks,
and {one program implementing the strategies from~\cite{Jamieson2015TheAO}}.
% \hyperlink{https://homes.cs.washington.edu/~jamieson/resources/kevinJamieson\_Dissertation.pdf}{thesis}.

The 15 data analysis programs 
from sklearn benchmark includes 12 programs with $O(n)$-adaptivity rounds
and 3 programs with $O(n*m)$-adapvitiy rounds.
% for the database classification.
These $O(n)$-rounds adaptivity programs are
the
implementations of the decision tree, logistic regression, naive Gaussian inference classifiers
with $O(n)$ fitting depth,
and the grid search hyperparameter selecting algorithm for each classifier with 
hyperparameter space of constant size $c$.
The $O(n*m)$-rounds adaptivity programs are
the implementations of the  decision tree, logistic regression, naive Gaussian inference models equipped with one v.s. rest model with $O(n)$ fitting depth for classifying the dataset with $O(m)$ classes.
These programs are evaluated for classifying a demography dataset from 
the USA census database~\cite{CensusDatabase}.


The 21 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}
benchmarks include:
\\
3 fully-adaptive progress data analysis
strategy~\cite{ZrnicH19}.
Specifically they are the implementation of the recurrent neural network, long short term models for 
weather forecasting.
\\
9
fully-adaptive conservative data analysis
strategy~\cite{ZrnicH19}.
Specifically they are the implementation of the convolutional neural network, transfer learning model,
and multiple-dense layer neural networks equipped with gradient decedent optimizer for
weather forecasting and image classification.
%  over MNIST dataset
% through fitting the  CNN, RNN, LSTM models for image classification and 
% weather forecasting as well.
\\
9 programs with two adaptivity rounds for text classification,
article classification, action recognition over real world databases.

The program implemented from~\cite{Jamieson2015TheAO} is the repeated query subroutine algorithm for ranking
an $n$-demensional dataset, which
has $O(2^n)$ adaptivity.

We translate these programs into our syntax.

For the programs of the same adaptivity complexity, we evaluate their average generalization error measured by root-mean-square error over fresh testing data.
Then with our tool, we choose different mechanism and evaluate each over the same data.
We plot the evaluation result in Figure~\ref{fig:implementation_generalization_errors}.
The result shows that for the same data analysis program, the generalization error is reduced
by equipping with the proper mechanism chosen by our model.

In Figure~\ref{fig:implementation_generalization_errors}(a), we plot the evaluation result over the
programs from the sklearn benchmark with $O(c)$ adaptivity.
We test them for classifying a randomly generated dataset with $n$ raw and $k$ features, and each raw is
assigned a random label from $\{-1, 1\}$.
We run these programs with constant number of adaptivity rounds and increasing the number of
maximum query numbers.
Then we evaluate the generalization error using underlying distribution that generate these data.
% from an underlying distribution

In Figure~\ref{fig:implementation_generalization_errors}(b), we plot the 
evaluation result over the fully adaptive
programs from the tensorflow benchmark with $O(n)$ adaptivity.
We evaluate these programs in the same way as the evaluation for the $O(c)$ adaptivity program.
% for classifying a randomly generated dataset with $n$ raw and constant dimension.
% Then we evaluate the generalization error using underlying distribution that generate these data.

For these data analysis programs,
our tool is able to identify the $O(c)$ and $O(n)$ adaptivity level.
Then by choosing different mechanisms according to our tool,
the evaluation results show that the generalization error is reduced better than 
using the mechanisms without our tool.

In Figure~\ref{fig:implementation_generalization_errors}(c), we plot the 
evaluation result on
%  the over the
program implemented from~\cite{Jamieson2015TheAO} with $O(2^n)$ adaptivity.
We test this algorithm for ranking a random dataset. 
We run this algorithm over a randomly generated sample dataset with $n$ raw and constant dimension,
then we evaluate the generalization error for ranking
a fresh data generated under the same distribution.
We measure its root-mean-square error by counting the incorrectly ranked items.
For the data analysis with exponential adaptivity, our tool is able to identify the high adaptivity level.
Then by using the appropriate mechanism according to the adaptivity level,
the generalization error is reduced more than using other mechanisms.


{\small
\begin{figure}
\centering
\begin{subfigure}{.32\textwidth}
\begin{centering}
\includegraphics[width=1.0\textwidth]{c_adaptivity.png}
\caption{}
\end{centering}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
\begin{centering}
\includegraphics[width=1.0\textwidth]{n_adaptivity.png}
\caption{}
\end{centering}
\end{subfigure}
\quad
\begin{subfigure}{.3\textwidth}
\begin{centering}
\includegraphics[width=1.0\textwidth]{exp_adaptivity.png}
\caption{}
\end{centering}
\end{subfigure}
\vspace{-0.2cm}
 \caption{The adaptive data analysis programs with
 (a) $O(1)$ adaptivity, 
 (b) $O(n)$ adaptivity,
 (c) and $O(2^n)$ adaptivity.
}
\label{fig:implementation_generalization_errors}
\vspace{-0.6cm}
\end{figure}
}
