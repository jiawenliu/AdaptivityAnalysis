% \subsection{The Implementation Evaluation Results }
% \label{sec:adapt-impleval}

We implemented $\THESYSTEM$ as a tool that takes a labeled command as input 
and outputs two upper bounds on the program adaptivity and the number of query requests respectively.
This implementation consists of an 
abstract control flow graph generation,
edge estimation (as presented in Section~\ref{sec:alg_edgegen}), and weight estimation (as presented in Section~\ref{sec:alg_weightgen}) in Ocaml, 
and the adaptivity computation algorithm shown in Section~\ref{sec:alg_adaptcompute} in Python.
The OCaml program takes the labeled command as input and outputs the program-based dependency graph and
the abstract transition graph,
feeds into the python program and the python program provides the adaptivity upper bound and the query number as the final output.

We evaluated this implementation on $23$ example programs with the evaluation results shown in Table~\ref{tb:adapt-imp}.
\input{eval-table}

In this table,
the first column is the name of each program.
For each program $c$, the second column is its intuitive adaptivity rounds,
and the third column is the output of the $\THESYSTEM$ implementation, which consists of two expressions.
The first one is the upper bound for adaptivity and the second one is the 
upper bound for the total number of query requests in the program. And the last column is the performance evaluation w.r.t. the program size.

The last column is the performance evaluation.
The time contains three parts. The first part is the running time of the Ocaml code, which parses the program and generates the $\progG(c)$.
The second and third parts are the running times of the reachability bound analysis algorithm
and the adaptivity computation algorithm, $\pathsearch(c)$.

The first six programs are adapted from real-world data analysis algorithms.
The first two programs $\kw{twoRounds(k)}$, $ \kw{multiRounds(k)}$ are the same as Figure~\ref{fig:overview-example}(a) and Figure~\ref{fig:multipleRounds}(a).
$\THESYSTEM$ computes tight adaptivity bound for the first three examples.
For the fourth program $\kw{multiRoundsO(k)}$, $\THESYSTEM$ outputs an over-approximated upper bound $1 + 2*k$ 
% for the $A(c)$, which is consistent with our expectation 
as discussed in Example~\ref{ex:multiRoundsO}. 
The fifth program is the evaluation results for the example in Example~\ref{ex:multiRoundsS}, where $\THESYSTEM$ outputs the tightly bound for $A(c)$ but $A(c)$ is a loose definition of the program's actual adaptivity rounds.
%

The next eleven programs from Table~\ref{tb:adapt-imp} rows 7 to 18 are handcrafted programs based on the code pieces extracted from the C library. They all have small sizes but complex structures in order to test the programs under different situations including
data, control dependency,
the multiple paths nested loop with related counters, etc.
The names of these programs obey the convention that,
$\kw{if}$ means there is if control in the program;
$\kw{loop}$ means there is while loop and $\kw{loop2}$ represents two levels nested loop in the program;
$\kw{C}$ denotes Control;
$\kw{D}$ for Dependency; $\kw{V}$ for Variable;
$\kw{M}$ for Multiple; $\kw{P}$ for Path and $\kw{R}$ for Related.


The algorithm computes the tight bound for examples from line six, $\kw{ifCD()}$ to line fourteen, $\kw{loop2MPRV(k)}$
and over-approximate the \emph{adaptivity} for the $15^{th}$ and $16^{th}$ examples in the table due to path-insensitivity.

The last six programs are synthesized programs composed of the previous programs in order to test the performance limitation when the input program is large. 
From the evaluation results, the performance bottleneck is the reachability bound analysis algorithm.


By implementing the bound analysis algorithm in Section~\ref{sec:alg_weightgen} (adapted from \cite{SinnZV17}), we are unable to evaluate the $\kw{Jumbo}$ in a reasonable time period.
Alternatively, we implement another light reachability bound analysis algorithm and compute the \emph{adaptivity} for
$\kw{jumboS}, \kw{jumbo}$ and $\kw{big}$ effectively.
We also show our alternative implement evaluation result in Table~\ref{tb:adapt-imp-alternatives}.
The alternative implementation computes the tight bound for all the examples from line:1 to line:14
and over-approximate the \emph{adaptivity} for $15^{th}$ and $16^{th}$ due to path-insensitivity similar to the
$\THESYSTEM$.
For the $17^{th}$ example ($\kw{loop2RC}$), {\THESYSTEM} gives a tight bound while the alternative implementation gives a loose bound, so we keep both implementations.

Overall, {\THESYSTEM} gives the accurate estimated
adaptivity upper bound and our adaptivity formalization also gives precise adaptivity definition w.r.t. the intuitive \emph{adaptivity rounds}.
% and analysis framework $\THESYSTEM$.
\input{eval-table-alternatives}
The programs we evaluated in the Appendix~\ref{apdx:evaluated_examples}.


We evaluate our framework also on the real-world programs from
three benchmark sets, including thensorflow, sklearn and pytorch and research paper~\cite{Jamieson2015TheAO}.
\input{eval-table-generalization}

These programs include  
15 data analysis programs 
from \hyperlink{https://github.com/scikit-learn/scikit-learn/tree/main/examples}{sklearn}~\cite{SklearnBenchmark} benchmark,
21 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}~\cite{TensorflowBenchmark} 
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}~\cite{PytorchBenchmark}
benchmarks,
and {one program implementing the strategies from~\cite{Jamieson2015TheAO}}.
% \hyperlink{https://homes.cs.washington.edu/~jamieson/resources/kevinJamieson\_Dissertation.pdf}{thesis}.

The 15 data analysis programs 
from sklearn benchmark includes 12 programs with $O(n)$-adaptivity rounds
and 3 programs with $O(n*m)$-adapvitiy rounds.
% for the database classification.
These $O(n)$-rounds adaptivity programs are
the
implementations of the decision tree, logistic regression, naive Gaussian inference classifiers
with $O(n)$ fitting depth,
and the grid search hyperparameter selecting algorithm for each classifier with 
hyperparameter space of constant size $c$.
The $O(n*m)$-rounds adaptivity programs are
the implementations of the  decision tree, logistic regression, naive Gaussian inference models equipped with one v.s. rest model with $O(n)$ fitting depth for classifying the dataset with $O(m)$ classes.
These programs are evaluated for classifying a demography dataset from 
the USA census database~\cite{CensusDatabase}.


The 21 machine learning programs
from \hyperlink{https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples}{tensorflow}
and \hyperlink{https://github.com/pytorch/pytorch}{pytorch}
benchmarks include:
\\
3 fully-adaptive progress data analysis
strategy~\cite{ZrnicH19}.
Specifically they are the implementation of the recurrent neural network, long short term models for 
weather forecasting.
\\
9
fully-adaptive conservative data analysis
strategy~\cite{ZrnicH19}.
Specifically they are the implementation of the convolutional neural network, transfer learning model,
and multiple-dense layer neural networks equipped with gradient decedent optimizer for
weather forecasting and image classification.
%  over MNIST dataset
% through fitting the  CNN, RNN, LSTM models for image classification and 
% weather forecasting as well.
\\
9 programs with two adaptivity rounds for text classification,
article classification, action recognition over real world databases.

We implement three data analysis algorithm from~\cite{Jamieson2015TheAO},
which are:
repeated query subroutine algorithm for ranking
an $n$-demensional dataset with
has $n\times 2^n$ adaptivity rounds;
the n-dimension pairwise algorithm with $n$ adaptivity rounds and $n$ query requests in total;
and the lil-ucb algorithm with $n$ adaptivity rounds and $n^2$ query number.
These three algorithms are implemented into the GnC framework.
When running it, we are using the mechanisms pre-defined in this framework.
This framework is also used to evaluate the running example presented in overview section.

We translate these programs into our syntax.

We evaluate the average generalization error of each program measured by root-mean-square error over uniformly generated training data.
For each program, we show in the table the generalization errors when run it without mechanism and with different mechanisms.
Then we highlight the result of running it with the mechanism chosen by {\THESYSTEM}.


% For the programs of the same adaptivity complexity, 
% Then with our tool, we choose different mechanism and evaluate each over the same data.
% We plot the evaluation result in Figure~\ref{fig:implementation_generalization_errors}.
% The result shows that for the same data analysis program, the generalization error is reduced
% by equipping with the proper mechanism chosen by our model.


% \input{evaluation-plots}