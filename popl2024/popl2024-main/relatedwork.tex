
% \dg{Please cite adaptive Fuzz, and explain how its adaptivity analysis differs from ours.}

%In terms of techniques, our work relies on ideas from both static analysis and dynamic analysis. We discuss closely related work in both areas.


\paragraph{Dependency Definitions and Analysis} 
There is a vast literature on dependency definitions and dependency analysis. 
We consider a semantics definition of dependencies which consider (intraprocedural) data and control dependency~\cite{bilardi1996framework,cytron1991efficiently,pollock1989incremental}.    
Our definition is inspired by classical works on traditional dependency analysis~\cite{DenningD77} and noninterference~\cite{GoguenM82a}.
Formally, our definition is similar to the one by \citet{Cousot19a}, which also identifies dependencies by considering differences in two execution traces. 
However, Cousot excludes some forms of implicit dependencies, e.g. the ones generated by empty observations,  which instead we consider. 
%
Common tools to study dependencies are dependency graphs~\cite{ferrante1987program}. We use here a semantics-based approach to dependency graph similar, for example, to works by \citet{austin1992dynamic}, \citet{hammer2006dynamic} and \cite{mastroeni2008data}.
%propose ways of constructing different kinds of program slices, by choose different program 
%DDGs have been used in many other domains. \citet{nagar2018automated} use DDGs to find serializability violations. dependency. 
% For example, in either syntactic or semantics sense.
% This abstract dependency is based on properties rather than exact data.
% Aims to give finer and smaller program slice. 
%They actually use a combination of  
%static and dynamic dependency graphs but in a manner that is different from how we use the two. Their slicing uses both static and dynamic dependency graphs, while we use the dynamic dependency graph as the basis of a definition, which is then soundly approximated by an analysis based on the static dependency graph.}
%\paragraph{Static program analysis} 
%Our algorithm in Section~\ref{sec:algorithm} is influenced by previous works in static analysis related to effect systems, control-flow analysis, and data-flow analysis. 
%The idea of statically estimating a sound upper bound for the adaptivity from the semantics is indirectly inspired from prior work on cost analysis via effect systems~\cite{cciccek2017relational,radivcek2017monadic,qu2019relational}. The idea of defining adaptivity using data flow is inspired by the work of graded Hoare logic~\cite{gaboardi2021graded}, which reasons about data flows as a resource. 
%
Our approach shares some similarities with the use of dependency graphs in works analyzing dependencies between events, e.g. in event programming. \citet{memon2007event} uses an event-flow graph, representing all the possible event interactions, where vertices are GUI event edges represent pairs of events that can be performed immediately one after the other. In a similar way, we use edges to track the may-dependence between variables looking at all the possible interactions. 
% of one variable with respect to another variable. The main difference is in the way the graph is constructed. {\THESYSTEM} relies on the structure of the target program, while the event-flow model only considers the event type.
\citet{arlt2012lightweight} use a weighted edges indicating a dependency between two events, e.g. one event possibly reads data written by the other event, with the weight showing the intensity of the dependency (the quantity of data involved). We also use weights but on vertices and with different meaning, they are functions describing the number of times the vertices can be visited given an initial state.
% WCET on systems: \cite{} 
% [GustafssonEL05]Towards a Flow Analysis for Embedded System C Programs
% --> abstract interpretation.
% --> on embedded system of c program
% [AlbertAGP08] Automatic Inference of Upper Bounds for Recurrence Relations in Cost Analysis
% --> invariant generation through ranking functions
%
% General While langue:
% [BrockschmidtEFFG16]
% Analyzing Runtime and Size Complexity of Integer Programs
% --> invariant generation through ranking functions
% [AliasDFG10] Multi-dimensional Rankings, Program Termination, and Complexity Bounds of Flowchart Programs
% --> invariant generation through ranking functions
% [Flores-MontoyaH14]Resource Analysis of Complex Programs with Cost Equations
% --> invariant generation through cost equations or ranking functions
%
% [GulwaniJK09]Control-flow Refinement and Progress Invariants for Bound Analysis
% --> program abstraction and invariant inference
% []Bound Analysis using Backward Symbolic Execution
% --> program abstraction and invariant inference
%
% [CicekBG0H17]relational Cost Analysis 0
% Monadic refinements for relational cost analysis
% [RajaniG0021]A unifying type-theory for higher-order (amortized) cost analysis
% --> type-system
Differently from all these previous works, we use a dependency graph with quantitative information needed to identify the length of chain of dependencies. Our weight estimation is inspired by  works in complexity analysis and WCET. 
Specifically, it is inspired by works on  reachability-bound analysis using program abstraction and invariant inference~\cite{GulwaniZ10, SinnZV17,GulwaniJK09} and work on invariant inference through cost equations and ranking functions~\cite{BrockschmidtEFFG16,AlbertAGP08,AliasDFG10,Flores-MontoyaH14}.
% The techniques are based on
% type system~\cite{CicekBG0H17, RajaniG0021}, Hoare logic~\cite{CarbonneauxHS15}, abstract interpretation~\cite{GustafssonEL05, HumenbergerJK18},
% i
% or a combination of
% In general, these techniques give the approximated upper bound of the program's total running time or resource cost.
% However, they failed to consider the case where the cost -- the adaptivity-- could decrease when there isn't a dependency relation between variables.


\paragraph{Generalization in Adaptive Data Analysis}
Starting from the works by \citet{DworkFHPRR15} and \citet{HardtU14}, several works have designed methods that ensure generalization for adaptive data analyses~\cite{dwork2015reusable,dwork2015generalization,BassilyNSSSU16,UllmanSNSS18,FeldmanS17,jung2019new,SteinkeZ20,RogersRSSTW20}.
Several of these works drew inspiration from differential privacy, a notion of formal data privacy. By limiting the influence that an individual can have on the result of a data analysis, even in adaptive settings, differential privacy can also be used to limit the influence that a specific data sample can have on the statistical validity of a data analysis. This connection is actually in two directions, as discussed for example by \citet{YeomGFJ18}.
%
Considering this connection between generalization and privacy, it is not surprising that some of the works on programming language techniques for privacy-preserving data analysis are related to our work. 
Adaptive Fuzz~\cite{Winograd-CortHR17} is a programming framework for differential privacy that is designed around the concept of adaptivity. 
This framework is based on a typed functional language that distinguish between several forms of adaptive and non-adaptive composition theorem with the goal of achieving better upper bounds on the privacy cost. Adaptive Fuzz uses a type system and some partial evaluation to guarantee that the programs respect differential privacy. However, it does not include any technique to bound the number of rounds of adaptivity. 
\citet{lobo2021programming} propose a language for differential privacy where one can reason about the accuracy of programs in terms of confidence intervals on the error that the use of differential privacy can generate. These are akin to bounds on the generalization error. This language is based on a static analysis which however cannot handle adaptivity. 
%
The way we formalize the access to the data mediated by a mechanism is a reminiscence of how the interaction with an oracle is modeled in the verification of security properties. As an example, the recent works by \citet{BarbosaBGKS21} and \citet{AguirreBGGKS21} use different techniques to track the number of accesses to an oracle. However, reasoning about the number of accesses is easier than estimating the adaptivity of these calls, as we do instead here.

%\cite{SatoABGGH19}

% together with guarantees on their accuracy? 
% {The first important application of the differential privacy concept started from the work by \cite{DworkFHPRR15}}, which applied this concept into guaranteeing the generalization error of adaptive data analysis. 

% Previous works on reducing the risk of spurious scientific discoveries are under the assumption that a fixed collection of learning algorithms to be applied are selected non-adaptively before seeing the data. In contrast with them, they developed this work under the adaptive data analysis settings. They formalized the generalization error for adaptive data analysis and then presented their validation guarantees.
% Concretely, they proposed the famous transfer theorem.
% And based on this theorem, they proved high probabilistic bounds on the generalization error for $\epsilon$- and $(\epsilon,\delta)$-differentially private adaptive data analysis as well as adaptive analysis with statistic and non-statistic queries.
% These works connected the theory with the practice of data analysis, which in my perspective, is the most significant and interesting contribution of this paper. 
% %
% % At the end, they presented the application of applying concrete differentially private techniques into adaptive data analysis.

% {This extension from differential privacy to adaptive data analysis made significant progress in reducing the overfitting risks in practical works (i.e. the data analysis in adaptive setting). Further works on improving these probabilistic bounds, guaranteeing the generalization error such as \cite{dwork2015generalization}, \cite{BassilyNSSSU16}, \cite{dwork2015reusable}, \cite{jung2019new} etc. are all influenced by this work.}

% {Following all previous works on preserving the statistical validity in adaptive data analysis, \cite{smith2017information} made a survey.}
% This survey started from giving formal and clear introduction to the concept of adaptive data analysis.
%  % by giving formal definitions of the concepts and clear representations and notations. 
% Then, it summarized the probability bounds on the generalization error w.r.t. the true population when applying different mechanisms in numeric adaptive data analysis.
% The mechanisms includes split data with adaptivity, adding Gaussian noise with specific standard derivation, adopting differentially private algorithms etc.
% Next, he extended the scope onto the non-numeric adaptive data analysis and presented the corresponding probabilistic bounds based on the information measures. 

% {This survey was developed in an easy-to-understand way and included a thorough knowledge on state-of-the-art works on adaptive data analysis, which helped me to sort out the results from existing works and relations between them.}

% {Following the same line of work, \cite{jung2019new} gave a new analysis on the role of differential privacy in adaptive data analysis.}
% They gave a substantially better probability bounds on differential privacy's generalization guarantee based on a new proof technique of the transfer theorem (initially from \cite{dwork2015generalization}). 

% The key point in their proof technique is looking into the posterior distributions, which is an insightful new perspective on proving the generalization error bound. This new perspective also brought a better understanding in the specific reason of analysis overfitting and the role of differential privacy in adaptive data analysis. This new technique I think will be fruitful in future work.
% %  based on my personal interests on the posterior distribution analysis
% % Another very meaningful structural insight inspired by this paper is the role of differential privacy and sample accuracy. This is pointed to the end of the paper that the sample accuracy serves to guarantee that the reported answers are close to their posterior means and differential privacy serves to guarantee that the posterior means are close to their true answers.

% % There are also some interesting works on further improving the accuracy bound unresolved in this paper, such as replace the Markov-like dependence with a Chernoff-like dependence. I'm deeply interested in making contributions on it.

% % {Based on all the excellent theory works on guaranteeing the statistical validity of adaptive data analysis, I started to think from the perspective of the programming language.}
% {Existing works on adaptive data analysis are trying to improve the probabilistic bounds for generalization error in terms of the adaptive and non-adaptive queries numbers and size of the data sample on pen-and-paper proofs.
% However, we still cannot guarantee implementations of the corresponding algorithms adhere to this generalization error bounds.
% Given an arbitrary data analysis program, we are unable to tell its generalization error. So in my consideration, verifying the programs' generalization error would be a possible interesting research direction. Furthermore, since the size of the data sample can be determined by the input or the users, then the most interesting and challenging point would be figuring out the program's adaptive query numbers. 
% This motivated us to look into the verification of algorithms' adaptivity, in order to formally verifiy their generalization error.}

% \paragraph{Data analysis} There is a significant amount of work on programming for data analysis. Many popular platforms are based on the R language\cite{ihaka1996r, marcon2021orchestrating}. Jaql is a declarative scripting language for large-scale data analysis\cite{beyer2011jaql}. 


% Program Analysis in terms of dependency graph:


% \subsection{Dynamic Program Graph Analysis}

% \cite{sinha2001interprocedural}: 
% Support Interprocedural Control dependence analyzing, semantically.
% \\
% They identified the dependence information between the interactions of among procedures, specifically the control dependence between procedures.
% Their analysis support the relationship of control and data dependence to semantics dependence.

% % \cite{austin1992dynamic}: Dynamic Program Dependency Graph.
% % \\
% % They gave the dynamic analysis for the program's dependency, by producing 3 different kinds of graph, 
% % including the data flow graph, storage dependence and control dependence graph from program's execution traces. 
% % \\
% % Then, they constructing dynamic execution graphs by adopting the 3 graph, aims to expose the parallization of the programs

% \cite{hammer2006dynamic}: dynamic path conditions in dependence graphs.
% They adopting the dynamic information from program trace to the path condition in dependency graph. Then based on these information, 
% they present new approach combining dynamic slicing, which could reveal both dependences holding during program execution as well as why these dependences are holding. 
% Aims to have a finer and preciser analysis of the program.

% % \subsection{Utilization of the Dynamic Program Dependency}

% % \cite{nagar2018automated}: Utilize dependency graph for finding serializability violation. 
% % \\
% % Combine with the dependency graph of serialization and abstract execution, to statically finding bounded serializability violation. 
% % Then reduce the problem of serializability to satisfiability of a formula in FOL.
% % Also reason about unbounded executions.

% \subsection{Static Program Dependency}

% \cite{mastroeni2008data}: They propose ways of constructing different kinds of program slices, by choose different program dependency. For example, in either syntactic or semantics sense.
% This abstract dependency is based on properties rather than exact data.
% Aims to give finer and smaller program slice. 

% \subsection{Utilization Static Program Flow Graph}

% \cite{arlt2012lightweight}: Lightweight Static Analysis for GUI Testing. They give the relevant event graph based on black and white Box.
% To construct finer Event Sequence Graph, 
% they propose new approach to select relevant event sequences among the event sequences generated by black box.
% This new approach based on static analysis on bytecode of the applications, 
% giving a precisely defined dependency between a fixed number of events in event sequence.
% Then, they inferred a finer Event Dependency graph, aims to give a better lightweight static analysis on applications.


