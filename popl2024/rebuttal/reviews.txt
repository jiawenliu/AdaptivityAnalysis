POPL 2024 Paper #11 Reviews and Comments
===========================================================================
Paper #11 Program Analysis for Adaptive Data Analysis


Review #11A
===========================================================================

Overall merit
-------------
B. Weak accept: I lean towards acceptance.

Reviewer expertise
------------------
X. Expert: I am an expert on the topic of the paper (or at least key
   aspects of it).

Summary of the paper
--------------------
The paper considers a while-loop language for writing adaptive data analyses. In such analyses, the goal is to identify a property of the population from which the data are drawn (generalizing beyond a specific data sample). In an adaptive data analysis, there are multiple queries on the data, and the choice/order of queries is path-dependent---i.e., can rely on the results of previous queries.

The core issue is to understancd/control the generalization error of such programs. The paper develops a program analysis to characterize adaptivity in the program. The goal is to apply results that to control generalization, interpose a mechanism between an adaptive data analysis---in this case an adaptive-data-analysis program---and the data itself. (Methods include (i) the addition of noise to the result of a query, (ii) data splitting, and (iii) using sampling methods.)

A key issue for controlling generalization error is the number of ``rounds of adaptivity,'' meaning the length of a chain of dependent queries. The IR is a weighted dependence graph (essentially the subgraph of a program dependence graph consisting of just the flow-dependence edges) with weights that upper bound the number of times a variable can be reached.

Assessment of the paper
-----------------------
*Strengths*
- As far as I know, the questions explored are new w.r.t. the field of program analysis, showing how a variety of relatively standard techniques to build/use an IR can support analyses that incorporate research from statistics and machine learning.
- The language is designed so that a chosen mechanism can be interposed merely by reinterpreting the meaning of the ``query($\cdot$)'' construct


*Weaknesses*

- The explanation is confusing at several key points (749, Alg. 1, explanation of Alg. 1, 899)
- There are several places that would be helped by examples
- There are several places where the presentation would be helped by providing the explanatory text/example *before* the definition/theorem.  One example: Definition 5.

Detailed comments for authors
-----------------------------
Note: the following section-by-section notes were written as I read the paper.  In a few cases, a point that I found troublesome was clarified later.  Please use my mischaracterizations as an indication of where the paper needs some clarifying material.

1. Introduction
- Context: adaptive data analysis to be performed by a program (in a while-loop language)
- Gap: Stats/ML literature have results about how to upper bound generalization error for (hand-created) non-looping (?) sequences of queries
- Innovation: Devise a program-analysis method to upper bound generalization error for while-loop programs

2. Overview
- Shouldn't Theorems 2.1-2.3 all contain the clause ``with high probability'' (as do the informal statements of theorems in [18])?
- line 268: would be good to continue the sentence with ``... so that the program can be transformed to take advantage of Theorems 2.2 and 2.3
  (although performing such transformations automatically is beyond the scope of this paper).''  (But maybe there is no transformation, as hinted at in lines 288-289.)
- I'm confused by lines 284-288: you use $\Chi$ to ``abstract a possible row'' (where row is used in the singular), but then say that query($\Chi[j], \Chi[k]$) computes an empirical mean.  I think the problem is the abuse of notation on line 193.  There, ``query(P)'' returns an expectation.  However, in your programming language, the similar-looking program statement ``query(P)'' computes an empirical mean (and involves an implicit loop that you never talk about or represent in your IR).
- lines 289-290: ``compute an approximation of the product of the empirical mean of the first $k$ attributes.''  I'm even more mystified: in Fig. 3(a), statement 5 accumulates $a$ as a running sum.  You've added together emperical means of products of the $j$-th attribute with the $k$-th attribute.
- line 368: ``estimated dependency graph'': Are these graphs just a program dependence graph (PDG) p20] with weights on nodes?  Or perhaps they are just the subgraph of the PDG consisting of the flow-dependence edges.  PDGs are a well-known program representation, so say what the difference is with PDGs. 

3. Language
- line 467 says ``expected value.''  Should it be ``empirical mean''?

4. Adaptivity

5. Algorithm
- lines 749-762: unclear.  Please add an example.
- lines 864-866, breakdown of the steps of Algorithm 1: please refer to specific line numbers (or ranges of line numbers)
- line 864: ``first collects all the paths in $SCC_i$''.  First, you probably mean ``all simple cycles.''  Second, I don't see any code in Algorithm 1 that creates a data structure that collects any set of paths.

6. Examples

- A poor explanation in lines 899-917: What is the goal of the ``monitor argument''?  You haven't said enough, and suddenly introduce a ``hidden database D'' in lin 903.

7. Implementation
- Change the section title to ``Evaluation.'' The section is not much about the implementation per se
- Table 1: In the $A_{est}$ column, it would help to put in bold font the entries that match the ground truth adaptivity in column 2.
- line 1017-19: How about a much longer time out for the three examples, like 120 minutes?
- line 1020: Change paragraph title to ``Ablation studies''
- Table 3: in line 1105 to the end of the section, you give a lot of text discussing the circumstances in which the heuristic in your tool would choose one mechanism or another.  You should indicate the choice in the table, then then the reader would easily see how the choices (based on your analysis) agreed with the best generalization error, shown in bold.  You've only given us half the story in Table 1, and make the reader hunt for insight in the text from line 1105 to the end of the section!

Minor:
- line 157, 162, 1053, and probably elsewhere: I suggest replacing ``control flow analysis'' by ``control dependence analysis''
- line 281: ``on rows'': what rows are being referred to?  You mention ``database'' in line 285, so you've not described the setting to the reader properly
- lines 282 & 285: you overload $\Chi$ to mean a domain and a sampled row (or sampled set of rows?).  either change the notation or warn of the overloading.
- line 302 don't you mean ``or a control dependency''
- line 385: $a^5$ should be $k$ with 0, not $k with 1
- line 855: change * to $\times$ to match line 852
- lines 860-881 should be rewritten; for this reader, the explanation was totally unclear
- line 1051: ``so we keep both.''  I don't understand what point you are trying to make

Questions to be addressed by author response
--------------------------------------------
- Please clarify Figs. 2(a) and 2(c).  The ``last adaptive query'' is the 401st in Fig. 2(a), which for Gaussian noise has RMSE 0.37.
  But in the x=400 position of Fig. 2(c), RMSE for Gaussian noise appears to be 0.028, which is wildly different.  Clearly, I'm not
  understanding what you meant to convey.
- Shouldn't Theorems 2.1-2.3 all contain the clause ``with high probability'' (as do the informal statements of theorems in [18])?
- Please give an example to illustrate lines 749-762

I like the paper, but what will convert me into a stronger supporter is seeing revised paragraphs of explanation for the places that I've indicated (in Weaknesses + Detailed Comments) as not being clear.



Review #11B
===========================================================================

Overall merit
-------------
A. Strong accept: I will argue for acceptance.

Reviewer expertise
------------------
X. Expert: I am an expert on the topic of the paper (or at least key
   aspects of it).

Summary of the paper
--------------------
Data analyses can overfit and introduce generalization error if queries are adaptive. That is the output of one query is input to another. There are multiple approaches to reduce generalization error with such queries: (1) to split data and make sure that each query samples from different split subsets of the data, (2) add Gaussian noise (like in Differential privacy), and (3) a technique called "threshold out". The paper proposes an automated program analysis which takes the description of the adaptive query as an input and outputs which of the above techniques produces least generalization error and an estimate of the generalization error.

Assessment of the paper
-----------------------
The paper is well executed and well written. 
The algorithm is as follows:
(1) construct a dependency graph of the query
(2) use the 'resource bound' analysis of Gulwani and Zulegar (reference [25]) to find out how many times each query node in the dependency graph is executed, as its "weight"
(3) calculate longest weighted paths in the dependency graph. This is called "adaptivity"
Then use the longest weighted path to calculate a bound on the generalization error for each technique (this is done using known error bound formulas from each technique)

The empirical results presented in Section 7 show the utility of the approach. In most cases, the adaptivity estimate calculated by the tool is close the actual adaptivity. Ablations are done to show that each component of the implementation is useful. 

On the whole, I think the paper proposes a sensible approach, and provides evidence that the approach produces useful results in practice.

Detailed comments for authors
-----------------------------
(1) A high-level question I had is as follows: is it possible to describe the approach in the simple steps I describe above? is there any more nuance than the above description? If so, you could plainly state it that way, and your paper will still be a useful contribution. I think you have a lot of notation to capture control and data dependence etc which are well-known concepts and there is no need to describe these in the level of detail you have done.

(2) In Table 3, can you add an extra column for the actual generalization error? How would you estimate the actual generalization error?

(3) In Table 2 it will be good to also add the actual adaptivity (as in Table 1) - - it will make it easier to compare, rather than flip back and forth with Table 1.

(4) Is there value in combining different mechanisms for different queries. That is, use split data for some queries, gaussian noise for some queries etc.

Questions to be addressed by author response
--------------------------------------------
Please address the questions I ask above.  Questions in (1),  (2) and (4) above are the ones I am curious about.



Review #11C
===========================================================================

Overall merit
-------------
C. Weak reject: I lean towards rejection.

Reviewer expertise
------------------
X. Expert: I am an expert on the topic of the paper (or at least key
   aspects of it).

Summary of the paper
--------------------
The paper identifies and addresses the problem of inferring adaptivity bounds of adaptive data analyses. In high level, a data analysis performs a sequence of queries to a probability distribution in order to arrive at an estimate, such as the population mean of a function over this distribution. Generally, future queries can depend on past queries. The adaptivity of a data analysis is the longest depth of such dependencies it can exhibit, and it is important to know as it can affect the generalization error of the analysis. 

The paper presents an automated inference approach for the adaptivity of algorithms constructed out of a simple while language. It first presents formal semantics that allow the notion of adaptivity to be made precise for this class of programs. Then, it exhibits a sequence of overapproximations of adaptivity by using a set of existing static analyses, such as identifying dataflow and controlflow dependencies between program variables and estimating execution frequencies, which yield a weighted-graph abstraction over the program, and then running an overapproximate algorithm on this abstraction to estimate the final adaptivity bounds.

The paper concludes with an experimental evaluation on a number of adaptive data analysis programs, most of which consist of a few lines of code. The results show that the proposed technique is able to infer fairly tight adaptivity bounds that can be even parametric on inputs of the analysis algorithms. It is also shown that, knowing these bounds in advance leads to a smaller generalization error by using the right mechanism to combine data from the obtained queries.

Assessment of the paper
-----------------------
Strengths

- The formal, semantic development of adaptivity is a novel concept.
- The static program analysis for adaptivity brings together a range of more basic static analyses, which have been developed independently. Combining all of them in a sound framework is a challenge, and requires wide domain knowledge.
- A good experimental evaluation.

Weaknesses

- It is unclear (to me) how important the problem is. I would expect a typical adaptive analysis to know its level of adaptivity by design, rather than relying on a technique for inferring it. This is also hinted by the (true) adaptivity bounds of the benchmark set, which are all simple expressions that seem be chosen by design.

- Though gluing together all the required concepts (dependencies, adaptivity, and analysis approximations) is a challenge, the technical contributions of the paper appear not fundamentally deep or novel. For most of its development, the paper relies on standard-ish existing techniques. This is generally ok, but shifts the impact of the paper more on the application side, which, as stated in the previous item, is not entirely clear to me.

- The writing quality could be improved. I found myself in several places wondering what the precise meaning is. A few examples follow later. Reading Section 5 was cumbersome, which lead me going through the appendix for "formal proofs" that would disambiguate my lack of understanding (e.g., the correctness of Algorithm 1 and Theorem 5.3). Unfortunately, a significant part of the "formal proofs" in the appendix is close to unreadable, with an abundance of sentences that are not event syntactically meaningful (e.g., Sections E, F). Putting all other aspects aside, I find that a significant overhaul in writing is needed before publication.

Detailed comments for authors
-----------------------------
Here are some suggestions of places in the paper that I found myself wondering about their meaning and may be improved. I do not list parts of the appendix, which I think require more substantial work.

- The exposition in Section 2.1 presents the problem of coming up with an estimate $a$ of the population mean $query(P)$. Then the text writes "...test depends on the prefix $query_1, a_1,...$", which creates some confusion around these $a_j$, which seem to appear out of nowhere. Are we instead in an online setting where a new estimate is given after each query?

- In Theorems 2.1, 2.2, 2.3, what are the $a_j$? The bounds should depend on the way the $a_j$ are computed, yet this is not specified. I suppose these are empirical means, like $a$ earlier (lines 199-200)?

- line 300, "However, capturing this concept formally is surprisingly difficult". I don't see why this is more difficult than standard dependency analysis. You write "the difficulty comes from the fact that a query can depend on the result of another query in multiple ways, by means of data dependency or control flow dependency". These are standard challenges in dependency analyses, why are they labeled as new here?

- line 353, "we consider the walk that visits...". Why is such a walk uniquely defined? I suspect it is not.

- line 355, "the number of query nodes visited", are these distinct nodes or not? Consider disambiguating here. The paper also mixes the terms "node" and "vertex", consider sticking to one.

- line 357, why don't you take the path $l^6 \to x^3$ as the one defining adaptivity? Why go through $a^5$?

- line 412, a small discussion on query values would help here.

- Defintion 1, I could not see a definition of $\mathcal{E}$ (perhaps I missed it). I suppose it denotes the events of the program. Similarly with $\mathcal{C}$ in Definition 2.

- line 543, "whether the change in $\epsilon_1$ affects the appearance in the computation..". Which change? The appearance of what?

- line 644 (also other places). Perhaps "upper bound" is a better term than "estimate", since you are stating soundness.

- line 659, "if the command with label $l'$ can execute right after". Though I likely understand what you mean here (simple control-flow), this statement is technically ambiguous ("can execute" refers to your formal semantics, and cannot be computed).

- line 840, "The algorithm uses another algorithm AdaptBD_{SCC} recursively..". I don't see the recursion.

- line 864, "first collects all the paths". Which data structure collects all the paths? What do you mean by "all the paths"? There are infinitely many of those. Perhaps all simple paths/simple cycles?

- line 867, "by the property of SCC", which property?

- line 872, a comment on how you compute the maximum over symbolic expressions (or what types of symbolic expressions you can handle) would be helpful.



- (Minor): there are several typos throughout the paper, that could be caught by a few carefull passes, or some grammar-checking software. Eg,

-- line 364, "formally in 5"

-- line 413, "Command" -> "commands"

-- line 419, "assigned to the result", maybe "assigned the result"?

-- line 530, "$\epsilon_1$" -> "$\epsilon_1'$"?

-- line 819, "notice that different from"

-- line 820, "this because"

-- line 834, "these approach"

-- line 841, "for a strong connected"

-- line 853, (algo line 9), "visited[v]=1" -> "visited[v]=True"?

-- line 866, "maximal adaptivity", maximality is not defined. Perhaps you mean maximum?

-- line 1098, capitalization

-- line 1110, "Evaluation result.. shows that we choose the best".

Questions to be addressed by author response
--------------------------------------------
- How common is it that an adaptive data analysis algorithm does not have its adaptivity set by design?
- What is the complexity of Algorithm 1?
