We would like to sincerely thank all the reviewers for their precious time to carefully reading the paper and giving their constructive and detailed comments.

- Overview


We agree some of our sentences or presentation will cause confusion, we plan to eliminate these confusing sentences by carefully rewriting.  Our evaluation can be improved as the reviewers suggests, we will add the 
necessary of the evaluation. The key proof in the appendix will also be checked and we will make it easier to read. 


- Change List
 1. We will add some more examples to make the presentation easy to follow up, we plan to add one example(running example twoRounds) to show how we calculate the estimated weight using reachability bound algorithm TB(e,c) for an edge and a command c, one example (the running example) to show how AdaptBD_scc works, add the algorithm adpatBD and shows how it uses adaptBD_scc, more details of example of "monitor argument", moving examples clarification before the definition 5, 6 when we introduce walk and query length concepts.

 2. Make our evaluation clearer by adding the actual generation error, describing how to calculating it, and highlight and stress the mechanism our heuristic chooses, add more statements and evaluation on comparing our choices with the best mechanism to show the power of our tool.

 3. Carefully rewrite the sentences which causes confusions, such as the abuse of query(P) at line 193, the query answer a in a = query(P), the discussion of query expression and values and add concrete examples to explain the empirical means the query(chi[i].chi[k]). At line 864 in Alg.1, The sentences "first collects all the paths in $SCC_i$." is confusing and not precise, will change the presentation.  Be consistent with nodes and vertices, make sure the definition shows up when it is used such as event set $Epsilon$ with respect to Def 1. 
  


- Detailed Response

Review A  

First of all, thanks for the detailed comments, section by section and your constructive suggestion on using more examples to clarify some key concepts. We will make the changes to make our paper easier to follow up!  

Question to be address by this response:
------------------------------------------
> Please clarify Figs. 2(a) and 2(c).  The ``last adaptive query'' is the 401st in Fig. 2(a), which for Gaussian noise has RMSE 0.37.
  But in the x=400 position of Fig. 2(c), RMSE for Gaussian noise appears to be 0.028, which is wildly different.  Clearly, I'm not
  understanding what you meant to convey.
  @Marco @Jiawen Does 2(c) has the same setting of 2(a)?


> Shouldn't Theorems 2.1-2.3 all contain the clause ``with high probability'' (as do the informal statements of theorems in [18])?
Yes.

> Please give an example to illustrate lines 749-762
line 749-762 describes how our reachability bound TB(e,c) returns an symbolic upper bound for an edge e in the set of edges absE(c). The local bound of this edge e is computed using difference constraint described in last step between line 738-748. TB(e,c) calculated local bound of e, if e is an symbolic constant(short for SC, either a nature number such as 3, or infinity, or input variable such as k in the running example, or a symbol Q_m representing a query request), TB(e,c) just return this symbolic constant. Let us look at Fig.6(b), the edge e1 (0, top, 1) has local bound 1 so TB(e1,c) returns 1 as shown in the node 0 and 1 in Fig.6(c) with bound 1.  
Otherwise, local bound of e is not SC, it will be a variable. For instance, still Fig.6(b), the edge e4 (4, j' <= j -1 ,5) is in an SCC appearing in the while loop, the local bound is variable j, not a symbolic constant. 
Then, to get TB(e4,c), it calculates two quantities: the first one is the sum of the reachability bound TB(e,c) of all the edges that may increment j, such as of the form (l, j' <= j + v ,l') plus the increment v, v is a SC.  For the edge e4, Fig.6(b) does not have any edge that increment j, so the first quantity is 0. 
The second one is the sum of the reachability bound of edge which reset j, such as of the form (l, j <  v ,l') multiplied with maximal value of this symbolic expression v. For the edge e4, Fig.6(b), the only edge that resets j is e2(1,j' <= k ,2), TB(e2,c) = 1 and maximal value of k is k. So the second quantity is k.
In this case, TB(e4,c) = k. It shows that the edges e4 will be executed at most k times.

@Jiawen, can you verity what the example I illustrate?    


Response to comments
----------------------------------------------------------
> The explanation is confusing at several key points (749, Alg. 1, explanation of Alg. 1, 899)
> There are several places that would be helped by examples.
> 5. Algorithm, lines 749-762: unclear.  Please add an example.
We will add an example, for instance, the running example in section Overview to show how we can calculate the symbolic weight by using the reachability bound analysis.
 
The explanation of Alg 1 is not clear. We use the algorithm AdaptBD to estimate the adaptivity, and AdaptBD_scc to estimate the Strong connected component of the graph. AdaptBD uses AdaptBD_scc. We will add the algorithm of AdaptBD as All 2, and add an example to show how Alg1(AdaptBD_scc) calculates the result. 

- There are several places where the presentation would be helped by providing the explanatory text/example *before* the definition/theorem.  One example: Definition 5.
Definition 5, the walk can be better understood if we discuss the an example, the one in Fig.3(b), before the definition. We will arrange to move the explaining examples before Definition 5.
Same for Definition 6, the query length, we will move the explanation of Fig.3(b) before the definition 6. 


5. Algorithm

> lines 864-866, breakdown of the steps of Algorithm 1: please refer to specific line numbers (or ranges of line numbers)
It is nice to refer to specific line numbers between lines 864 - 866, thanks.

> line 864: ``first collects all the paths in $SCC_i$''.  First, you probably mean ``all simple cycles.''  Second, I don't see any code in Algorithm 1 that creates a data structure that collects any set of paths.
"first collects all the paths in $SCC_i$",  from line 13 to line 15, every vertex v represents a path because we say "paths collected in step 1 are all simple cycles
with same starting and ending vertex", so at line 13, every v representing a path from v to v. Sorry for the confusion, to be precise, the algorithm handles paths on the fly, one by one. "collect all the paths" is implicit and not precise. We will change the sentence.   
  

 2. Overview
> Shouldn't Theorems 2.1-2.3 all contain the clause ``with high probability'' (as do the informal statements of theorems in [18])?
  Yes, thanks for pointing it out.

> I'm confused by lines 284-288: you use $\Chi$ to ``abstract a possible row'' (where row is used in the singular), but then say that query($\Chi[j], \Chi[k]$) computes an empirical mean.  I think the problem is the abuse of notation on line 193.  There, ``query(P)'' returns an expectation.  However, in your programming language, the similar-looking program statement ``query(P)'' computes an empirical mean (and involves an implicit loop that you never talk about or represent in your IR).
> lines 289-290: ``compute an approximation of the product of the empirical mean of the first $k$ attributes.''  I'm even more mystified: in Fig. 3(a), statement 5 accumulates $a$ as a running sum.  You've added together emperical means of products of the $j$-th attribute with the $k$-th attribute.

The abuse of notation of query does cause some confusion, we will modify it. For the example in Fig. 3(a), statement 5 accumulates $a$ as a running sum, statement 3, x<- query(chi[j].chi[k]), returns the empirical means of products of the $j$-th attribute with the $k$-th attribute. Since j starting from k to 0, a will finally store sum of the means of row k times row k, row (k-1) times row k, until row 0 times row k. The sum a will be used to construct another query at line 6. This is how two rounds algorithm is designed. We realize the sentences may cause confusion, will rewrite it with concrete details.  


6. Examples

> A poor explanation in lines 899-917: What is the goal of the ``monitor argument''?  You haven't said enough, and suddenly introduce a ``hidden database D'' in lin 903.
  More details about "monitor argument" is worth to showing up and then a hidden database D will be accepted in a natural way. We will make the change to make the example easier to follow up.


7. Implementation
> Change the section title to ``Evaluation.'' The section is not much about the implementation per se
> Table 1: In the $A_{est}$ column, it would help to put in bold font the entries that match the ground truth adaptivity in column 2.
> line 1020: Change paragraph title to ``Ablation studies''
Agree, thanks for the advice.

> line 1017-19: How about a much longer time out for the three examples, like 120 minutes?
   jumbo takes around 40 mins. the long time is for weight, AdaptBD is fast. For the other 2, more than 120 mins, still caused by getting the weight.


> Table 3: in line 1105 to the end of the section, you give a lot of text discussing the circumstances in which the heuristic in your tool would choose one mechanism or another.  You should indicate the choice in the table, then then the reader would easily see how the choices (based on your analysis) agreed with the best generalization error, shown in bold.  You've only given us half the story in Table 1, and make the reader hunt for insight in the text from line 1105 to the end of the section!

Thanks for the feedback. In table 3, the bold results are the one chosen by our heuristic. Some of them is not the best, for instance, RQ, nDPair, our choice(in bold) in Table 3 is not the best among the 3 mechanisms but still acceptable. We should clarify that our choice is highlighted in Table 3 to make it clear. 


minor:
Thanks for pointing out the minor, we will fix them.



Review B

Thanks for your detailed suggestion.



Detailed comments for authors
-----------------------------
(1) A high-level question I had is as follows: is it possible to describe the approach in the simple steps I describe above? is there any more nuance than the above description? If so, you could plainly state it that way, and your paper will still be a useful contribution. I think you have a lot of notation to capture control and data dependence etc which are well-known concepts and there is no need to describe these in the level of detail you have done.
Yes, it is. agree that we can save more space by omitting some well-know control and data dependence to add more examples for better presentation. 


(2) In Table 3, can you add an extra column for the actual generalization error? How would you estimate the actual generalization error?
Yes, we will.   @Jiawen, can you describe how we estimate the generation error?


(4) Is there value in combining different mechanisms for different queries. That is, use split data for some queries, gaussian noise for some queries etc.
Not sure if it is possible.  @Marco


(3) In Table 2 it will be good to also add the actual adaptivity (as in Table 1) - - it will make it easier to compare, rather than flip back and forth with Table 1.
Agree. Thanks for the suggestion.


Review C


Questions to be addressed by author response
--------------------------------------------
- How common is it that an adaptive data analysis algorithm does not have its adaptivity set by design?


- What is the complexity of Algorithm 1?


------------------------------

Detailed comments

> The exposition in Section 2.1 presents the problem of coming up with an estimate $a$ of the population mean $query(P)$. Then the text writes "...test depends on the prefix $query_1, a_1,...$", which creates some confusion around these $a_j$, which seem to appear out of nowhere. Are we instead in an online setting where a new estimate is given after each query?
> In Theorems 2.1, 2.2, 2.3, what are the $a_j$? The bounds should depend on the way the $a_j$ are computed, yet this is not specified. I suppose these are empirical means, like $a$ earlier (lines 199-200)?

Sorry for the confusion, $a_j$ stands for the result of the jth query, similar as a = query(P) in the previous sentence of this text, we will make it explicit.

> line 300, "However, capturing this concept formally is surprisingly difficult". I don't see why this is more difficult than standard dependency analysis. You write "the difficulty comes from the fact that a query can depend on the result of another query in multiple ways, by means of data dependency or control flow dependency". These are standard challenges in dependency analyses, why are they labeled as new here?
The challenge also covers the formal definition of adaptivity, not just dependency analysis.


- line 353, "we consider the walk that visits...". Why is such a walk uniquely defined? I suspect it is not.
It is not uniquely defined. To get the definition of adaptivity, we only care about one walk with most number of queries. 

- line 355, "the number of query nodes visited", are these distinct nodes or not? Consider disambiguating here. The paper also mixes the terms "node" and "vertex", consider sticking to one.
The nodes will be distinct. Thanks for the suggestion, we will be consistent.


- line 357, why don't you take the path $l^6 \to x^3$ as the one defining adaptivity? Why go through $a^5$?
Good point! Based on our definition, either going through $a^5$ or not will not affect the definition, we will explain that both will be fine to get the adaptivity.


- line 412, a small discussion on query values would help here.
Indeed, we plan to add a concrete example to make it clear.


- Defintion 1, I could not see a definition of $\mathcal{E}$ (perhaps I missed it). I suppose it denotes the events of the program. Similarly with $\mathcal{C}$ in Definition 2.
At line 431, we define the set of events. Sorry for the confusion, we should clarify the definition near the definition and we will.


> line 543, "whether the change in $\epsilon_1$ affects the appearance in the computation..". Which change? The appearance of what?
It is not clear. To understand it, the change of value in the event $\epsilon_1$ (maybe the assignment assigns a different value to some variable) affects the appearance of the event $\epsilon_2$.

> line 644 (also other places). Perhaps "upper bound" is a better term than "estimate", since you are stating soundness.
Agree.


- line 659, "if the command with label $l'$ can execute right after". Though I likely understand what you mean here (simple control-flow), this statement is technically ambiguous ("can execute" refers to your formal semantics, and cannot be computed).
Good point. We will rewrite this sentence to make it unambiguous!


- line 840, "The algorithm uses another algorithm AdaptBD_{SCC} recursively..". I don't see the recursion.
The algorithm AdaptBD is not shown in the paper. We will add it. @Jiawen, not recursively, right?

- line 864, "first collects all the paths". Which data structure collects all the paths? What do you mean by "all the paths"? There are infinitely many of those. Perhaps all simple paths/simple cycles?
"first collects all the paths in $SCC_i$",  from line 13 to line 15, every vertex v represents a path because we say "paths collected in step 1 are all simple cycles
with same starting and ending vertex", so at line 13, every v representing a path from v to v. Sorry for the confusion, to be precise, the algorithm handles paths on the fly, one by one. "collect all the paths" is implicit and not precise. We will change the sentence.   


- line 867, "by the property of SCC", which property?

- line 872, a comment on how you compute the maximum over symbolic expressions (or what types of symbolic expressions you can handle) would be helpful.



Monor: 
Thanks for pointing typos out, we will fix them.





======================================================================End of response================



Review #11A
===========================================================================

Overall merit
-------------
B. Weak accept: I lean towards acceptance.

Reviewer expertise
------------------
X. Expert: I am an expert on the topic of the paper (or at least key
   aspects of it).

Summary of the paper
--------------------
The paper considers a while-loop language for writing adaptive data analyses. In such analyses, the goal is to identify a property of the population from which the data are drawn (generalizing beyond a specific data sample). In an adaptive data analysis, there are multiple queries on the data, and the choice/order of queries is path-dependent---i.e., can rely on the results of previous queries.

The core issue is to understancd/control the generalization error of such programs. The paper develops a program analysis to characterize adaptivity in the program. The goal is to apply results that to control generalization, interpose a mechanism between an adaptive data analysis---in this case an adaptive-data-analysis program---and the data itself. (Methods include (i) the addition of noise to the result of a query, (ii) data splitting, and (iii) using sampling methods.)

A key issue for controlling generalization error is the number of ``rounds of adaptivity,'' meaning the length of a chain of dependent queries. The IR is a weighted dependence graph (essentially the subgraph of a program dependence graph consisting of just the flow-dependence edges) with weights that upper bound the number of times a variable can be reached.

Assessment of the paper
-----------------------
*Strengths*
- As far as I know, the questions explored are new w.r.t. the field of program analysis, showing how a variety of relatively standard techniques to build/use an IR can support analyses that incorporate research from statistics and machine learning.
- The language is designed so that a chosen mechanism can be interposed merely by reinterpreting the meaning of the ``query($\cdot$)'' construct


*Weaknesses*

- The explanation is confusing at several key points (749, Alg. 1, explanation of Alg. 1, 899)
- There are several places that would be helped by examples
- There are several places where the presentation would be helped by providing the explanatory text/example *before* the definition/theorem.  One example: Definition 5.

Detailed comments for authors
-----------------------------
Note: the following section-by-section notes were written as I read the paper.  In a few cases, a point that I found troublesome was clarified later.  Please use my mischaracterizations as an indication of where the paper needs some clarifying material.

1. Introduction
- Context: adaptive data analysis to be performed by a program (in a while-loop language)
- Gap: Stats/ML literature have results about how to upper bound generalization error for (hand-created) non-looping (?) sequences of queries
- Innovation: Devise a program-analysis method to upper bound generalization error for while-loop programs

2. Overview
- Shouldn't Theorems 2.1-2.3 all contain the clause ``with high probability'' (as do the informal statements of theorems in [18])?
- line 268: would be good to continue the sentence with ``... so that the program can be transformed to take advantage of Theorems 2.2 and 2.3
  (although performing such transformations automatically is beyond the scope of this paper).''  (But maybe there is no transformation, as hinted at in lines 288-289.)
- I'm confused by lines 284-288: you use $\Chi$ to ``abstract a possible row'' (where row is used in the singular), but then say that query($\Chi[j], \Chi[k]$) computes an empirical mean.  I think the problem is the abuse of notation on line 193.  There, ``query(P)'' returns an expectation.  However, in your programming language, the similar-looking program statement ``query(P)'' computes an empirical mean (and involves an implicit loop that you never talk about or represent in your IR).
- lines 289-290: ``compute an approximation of the product of the empirical mean of the first $k$ attributes.''  I'm even more mystified: in Fig. 3(a), statement 5 accumulates $a$ as a running sum.  You've added together emperical means of products of the $j$-th attribute with the $k$-th attribute.
- line 368: ``estimated dependency graph'': Are these graphs just a program dependence graph (PDG) p20] with weights on nodes?  Or perhaps they are just the subgraph of the PDG consisting of the flow-dependence edges.  PDGs are a well-known program representation, so say what the difference is with PDGs. 

3. Language
- line 467 says ``expected value.''  Should it be ``empirical mean''?

4. Adaptivity

5. Algorithm
- lines 749-762: unclear.  Please add an example.
- lines 864-866, breakdown of the steps of Algorithm 1: please refer to specific line numbers (or ranges of line numbers)
- line 864: ``first collects all the paths in $SCC_i$''.  First, you probably mean ``all simple cycles.''  Second, I don't see any code in Algorithm 1 that creates a data structure that collects any set of paths.

6. Examples

- A poor explanation in lines 899-917: What is the goal of the ``monitor argument''?  You haven't said enough, and suddenly introduce a ``hidden database D'' in lin 903.

7. Implementation
- Change the section title to ``Evaluation.'' The section is not much about the implementation per se
- Table 1: In the $A_{est}$ column, it would help to put in bold font the entries that match the ground truth adaptivity in column 2.
- line 1017-19: How about a much longer time out for the three examples, like 120 minutes?
- line 1020: Change paragraph title to ``Ablation studies''
- Table 3: in line 1105 to the end of the section, you give a lot of text discussing the circumstances in which the heuristic in your tool would choose one mechanism or another.  You should indicate the choice in the table, then then the reader would easily see how the choices (based on your analysis) agreed with the best generalization error, shown in bold.  You've only given us half the story in Table 1, and make the reader hunt for insight in the text from line 1105 to the end of the section!

Minor:
- line 157, 162, 1053, and probably elsewhere: I suggest replacing ``control flow analysis'' by ``control dependence analysis''
- line 281: ``on rows'': what rows are being referred to?  You mention ``database'' in line 285, so you've not described the setting to the reader properly
- lines 282 & 285: you overload $\Chi$ to mean a domain and a sampled row (or sampled set of rows?).  either change the notation or warn of the overloading.
- line 302 don't you mean ``or a control dependency''
- line 385: $a^5$ should be $k$ with 0, not $k with 1
- line 855: change * to $\times$ to match line 852
- lines 860-881 should be rewritten; for this reader, the explanation was totally unclear
- line 1051: ``so we keep both.''  I don't understand what point you are trying to make

Questions to be addressed by author response
--------------------------------------------
- Please clarify Figs. 2(a) and 2(c).  The ``last adaptive query'' is the 401st in Fig. 2(a), which for Gaussian noise has RMSE 0.37.
  But in the x=400 position of Fig. 2(c), RMSE for Gaussian noise appears to be 0.028, which is wildly different.  Clearly, I'm not
  understanding what you meant to convey.
- Shouldn't Theorems 2.1-2.3 all contain the clause ``with high probability'' (as do the informal statements of theorems in [18])?
- Please give an example to illustrate lines 749-762

I like the paper, but what will convert me into a stronger supporter is seeing revised paragraphs of explanation for the places that I've indicated (in Weaknesses + Detailed Comments) as not being clear.



Review #11B
===========================================================================

Overall merit
-------------
A. Strong accept: I will argue for acceptance.

Reviewer expertise
------------------
X. Expert: I am an expert on the topic of the paper (or at least key
   aspects of it).

Summary of the paper
--------------------
Data analyses can overfit and introduce generalization error if queries are adaptive. That is the output of one query is input to another. There are multiple approaches to reduce generalization error with such queries: (1) to split data and make sure that each query samples from different split subsets of the data, (2) add Gaussian noise (like in Differential privacy), and (3) a technique called "threshold out". The paper proposes an automated program analysis which takes the description of the adaptive query as an input and outputs which of the above techniques produces least generalization error and an estimate of the generalization error.

Assessment of the paper
-----------------------
The paper is well executed and well written. 
The algorithm is as follows:
(1) construct a dependency graph of the query
(2) use the 'resource bound' analysis of Gulwani and Zulegar (reference [25]) to find out how many times each query node in the dependency graph is executed, as its "weight"
(3) calculate longest weighted paths in the dependency graph. This is called "adaptivity"
Then use the longest weighted path to calculate a bound on the generalization error for each technique (this is done using known error bound formulas from each technique)

The empirical results presented in Section 7 show the utility of the approach. In most cases, the adaptivity estimate calculated by the tool is close the actual adaptivity. Ablations are done to show that each component of the implementation is useful. 

On the whole, I think the paper proposes a sensible approach, and provides evidence that the approach produces useful results in practice.

Detailed comments for authors
-----------------------------
(1) A high-level question I had is as follows: is it possible to describe the approach in the simple steps I describe above? is there any more nuance than the above description? If so, you could plainly state it that way, and your paper will still be a useful contribution. I think you have a lot of notation to capture control and data dependence etc which are well-known concepts and there is no need to describe these in the level of detail you have done.

(2) In Table 3, can you add an extra column for the actual generalization error? How would you estimate the actual generalization error?

(3) In Table 2 it will be good to also add the actual adaptivity (as in Table 1) - - it will make it easier to compare, rather than flip back and forth with Table 1.

(4) Is there value in combining different mechanisms for different queries. That is, use split data for some queries, gaussian noise for some queries etc.

Questions to be addressed by author response
--------------------------------------------
Please address the questions I ask above.  Questions in (1),  (2) and (4) above are the ones I am curious about.



Review #11C
===========================================================================

Overall merit
-------------
C. Weak reject: I lean towards rejection.

Reviewer expertise
------------------
X. Expert: I am an expert on the topic of the paper (or at least key
   aspects of it).

Summary of the paper
--------------------
The paper identifies and addresses the problem of inferring adaptivity bounds of adaptive data analyses. In high level, a data analysis performs a sequence of queries to a probability distribution in order to arrive at an estimate, such as the population mean of a function over this distribution. Generally, future queries can depend on past queries. The adaptivity of a data analysis is the longest depth of such dependencies it can exhibit, and it is important to know as it can affect the generalization error of the analysis. 

The paper presents an automated inference approach for the adaptivity of algorithms constructed out of a simple while language. It first presents formal semantics that allow the notion of adaptivity to be made precise for this class of programs. Then, it exhibits a sequence of overapproximations of adaptivity by using a set of existing static analyses, such as identifying dataflow and controlflow dependencies between program variables and estimating execution frequencies, which yield a weighted-graph abstraction over the program, and then running an overapproximate algorithm on this abstraction to estimate the final adaptivity bounds.

The paper concludes with an experimental evaluation on a number of adaptive data analysis programs, most of which consist of a few lines of code. The results show that the proposed technique is able to infer fairly tight adaptivity bounds that can be even parametric on inputs of the analysis algorithms. It is also shown that, knowing these bounds in advance leads to a smaller generalization error by using the right mechanism to combine data from the obtained queries.

Assessment of the paper
-----------------------
Strengths

- The formal, semantic development of adaptivity is a novel concept.
- The static program analysis for adaptivity brings together a range of more basic static analyses, which have been developed independently. Combining all of them in a sound framework is a challenge, and requires wide domain knowledge.
- A good experimental evaluation.

Weaknesses

- It is unclear (to me) how important the problem is. I would expect a typical adaptive analysis to know its level of adaptivity by design, rather than relying on a technique for inferring it. This is also hinted by the (true) adaptivity bounds of the benchmark set, which are all simple expressions that seem be chosen by design.

- Though gluing together all the required concepts (dependencies, adaptivity, and analysis approximations) is a challenge, the technical contributions of the paper appear not fundamentally deep or novel. For most of its development, the paper relies on standard-ish existing techniques. This is generally ok, but shifts the impact of the paper more on the application side, which, as stated in the previous item, is not entirely clear to me.

- The writing quality could be improved. I found myself in several places wondering what the precise meaning is. A few examples follow later. Reading Section 5 was cumbersome, which lead me going through the appendix for "formal proofs" that would disambiguate my lack of understanding (e.g., the correctness of Algorithm 1 and Theorem 5.3). Unfortunately, a significant part of the "formal proofs" in the appendix is close to unreadable, with an abundance of sentences that are not event syntactically meaningful (e.g., Sections E, F). Putting all other aspects aside, I find that a significant overhaul in writing is needed before publication.

Detailed comments for authors
-----------------------------
Here are some suggestions of places in the paper that I found myself wondering about their meaning and may be improved. I do not list parts of the appendix, which I think require more substantial work.

- The exposition in Section 2.1 presents the problem of coming up with an estimate $a$ of the population mean $query(P)$. Then the text writes "...test depends on the prefix $query_1, a_1,...$", which creates some confusion around these $a_j$, which seem to appear out of nowhere. Are we instead in an online setting where a new estimate is given after each query?

- In Theorems 2.1, 2.2, 2.3, what are the $a_j$? The bounds should depend on the way the $a_j$ are computed, yet this is not specified. I suppose these are empirical means, like $a$ earlier (lines 199-200)?

- line 300, "However, capturing this concept formally is surprisingly difficult". I don't see why this is more difficult than standard dependency analysis. You write "the difficulty comes from the fact that a query can depend on the result of another query in multiple ways, by means of data dependency or control flow dependency". These are standard challenges in dependency analyses, why are they labeled as new here?

- line 353, "we consider the walk that visits...". Why is such a walk uniquely defined? I suspect it is not.

- line 355, "the number of query nodes visited", are these distinct nodes or not? Consider disambiguating here. The paper also mixes the terms "node" and "vertex", consider sticking to one.

- line 357, why don't you take the path $l^6 \to x^3$ as the one defining adaptivity? Why go through $a^5$?

- line 412, a small discussion on query values would help here.

- Defintion 1, I could not see a definition of $\mathcal{E}$ (perhaps I missed it). I suppose it denotes the events of the program. Similarly with $\mathcal{C}$ in Definition 2.

- line 543, "whether the change in $\epsilon_1$ affects the appearance in the computation..". Which change? The appearance of what?

- line 644 (also other places). Perhaps "upper bound" is a better term than "estimate", since you are stating soundness.

- line 659, "if the command with label $l'$ can execute right after". Though I likely understand what you mean here (simple control-flow), this statement is technically ambiguous ("can execute" refers to your formal semantics, and cannot be computed).

- line 840, "The algorithm uses another algorithm AdaptBD_{SCC} recursively..". I don't see the recursion.

- line 864, "first collects all the paths". Which data structure collects all the paths? What do you mean by "all the paths"? There are infinitely many of those. Perhaps all simple paths/simple cycles?

- line 867, "by the property of SCC", which property?

- line 872, a comment on how you compute the maximum over symbolic expressions (or what types of symbolic expressions you can handle) would be helpful.



- (Minor): there are several typos throughout the paper, that could be caught by a few carefull passes, or some grammar-checking software. Eg,

-- line 364, "formally in 5"

-- line 413, "Command" -> "commands"

-- line 419, "assigned to the result", maybe "assigned the result"?

-- line 530, "$\epsilon_1$" -> "$\epsilon_1'$"?

-- line 819, "notice that different from"

-- line 820, "this because"

-- line 834, "these approach"

-- line 841, "for a strong connected"

-- line 853, (algo line 9), "visited[v]=1" -> "visited[v]=True"?

-- line 866, "maximal adaptivity", maximality is not defined. Perhaps you mean maximum?

-- line 1098, capitalization

-- line 1110, "Evaluation result.. shows that we choose the best".

Questions to be addressed by author response
--------------------------------------------
- How common is it that an adaptive data analysis algorithm does not have its adaptivity set by design?
- What is the complexity of Algorithm 1?
