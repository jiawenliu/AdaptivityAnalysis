{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Basic preprocessing\n",
    "def preprocess(df):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Discard identifiers, style information, timestamps\n",
    "    df_new = df_new[df_new.columns.difference(['image', 'style', 'reviewTime', \n",
    "                                               'reviewerID', 'asin', 'reviewerName', 'unixReviewTime'])]\n",
    "\n",
    "    # Turn category into binary features\n",
    "    for cat in df_new.category.unique():\n",
    "        df_new[cat] = df_new['category'] == cat\n",
    "\n",
    "    # Drop category column\n",
    "    df_new.drop(columns=['category'], inplace=True)\n",
    "\n",
    "    # NaN vote is 0 users found helpful\n",
    "    df_new.vote.fillna(0, inplace=True)\n",
    "    \n",
    "    # Turn vote into binary feature\n",
    "    df_new.vote = df_new.vote > 0\n",
    "    # df_new.vote.clip(0, 10)\n",
    "    # df_new.vote = df_new.vote / 10\n",
    "\n",
    "    # NaN summary is empty summary\n",
    "    df_new.summary.fillna('', inplace=True)\n",
    "\n",
    "    # Turn Booleans into binary variables\n",
    "    df_new.replace({False: 0, True: 1}, inplace=True)\n",
    "    \n",
    "    return df_new\n",
    "# Remove 'overall' column and add cutoff column applying cutoff\n",
    "def apply_cutoff(df, cutoff):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Apply cutoff\n",
    "    cut = df['overall'] > cutoff\n",
    "    df_new['cutoff'] = cut\n",
    "\n",
    "    # Drop overall and category\n",
    "    df_new.drop(columns=['overall'], inplace=True)\n",
    "    \n",
    "    # Turn Booleans into binary variables\n",
    "    df_new.replace({False: 0, True: 1}, inplace=True)\n",
    "    \n",
    "    return df_new\n",
    "def apply_tfidf(df, review_vectorizer, summary_vectorizer):\n",
    "    review_matrix = pd.DataFrame(data=review_vectorizer.transform(df.reviewText).toarray(), columns='R_' + review_vectorizer.get_feature_names_out())\n",
    "    summary_matrix = pd.DataFrame(data=summary_vectorizer.transform(df.summary).toarray(), columns='S_' + summary_vectorizer.get_feature_names_out())\n",
    "    df_new = pd.concat([df, review_matrix, summary_matrix], axis=1)\n",
    "    df_new.drop(columns=['summary', 'reviewText'], inplace=True)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# Processing the data - I\n",
    "# Preprocessing of training data\n",
    "def load_and_process_data():\n",
    "    training_df = pd.read_csv('../data/Training.csv')\n",
    "    test_df = pd.read_csv('../data/Test.csv')\n",
    "\n",
    "    proc_training_df = apply_cutoff(preprocess(training_df), 1)\n",
    "\n",
    "    # Set cutoff to be the label; define data_x and y accordingly\n",
    "    data_x = proc_training_df.drop('cutoff', axis=1)\n",
    "    data_y = proc_training_df['cutoff']\n",
    "\n",
    "    # Fit TF-IDF vectorizer for 'reviewText' and 'summary' features, creating max. 11500 features.\n",
    "    r_vectorizer = TfidfVectorizer(max_features=11500, stop_words='english', ngram_range=(1, 3))\n",
    "    s_vectorizer = TfidfVectorizer(max_features=11500, stop_words='english', ngram_range=(1, 3))\n",
    "    r_vectorizer.fit(data_x.reviewText)\n",
    "    s_vectorizer.fit(data_x.summary)\n",
    "\n",
    "    # Apply TF-IDF vectorization \n",
    "    data_x = apply_tfidf(data_x, r_vectorizer, s_vectorizer)\n",
    "\n",
    "    # Apply robust scaling\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    data_x = pd.DataFrame(scaler.fit_transform(data_x), columns=data_x.columns, index=data_x.index)\n",
    "\n",
    "    # Let us reduce the number of features by eliminating the statistically least correlated ones.\n",
    "    relcols = data_x.columns[abs(data_x.corrwith(data_y)) > 0.01]\n",
    "\n",
    "\n",
    "    # We will go with these columns.\n",
    "    data_x = data_x[relcols]\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "def create_splits(data_x, data_y, n_splits):\n",
    "    # 5-fold cross validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    for train_idx, val_idx in kf.split(data_x, data_y):\n",
    "        # Apply split\n",
    "        x_train, x_val = data_x.iloc[train_idx], data_x.iloc[val_idx]\n",
    "        y_train, y_val = data_y.iloc[train_idx], data_y.iloc[val_idx]\n",
    "        \n",
    "        # Reset indices\n",
    "        x_train.reset_index(drop=True, inplace=True)\n",
    "        y_train.reset_index(drop=True, inplace=True)\n",
    "        x_val.reset_index(drop=True, inplace=True)\n",
    "        y_val.reset_index(drop=True, inplace=True)\n",
    "        splits.append((x_train, x_val, y_train, y_val))\n",
    "    return splits\n",
    "\n",
    "## Mechanized Decision Tree Classifier\n",
    "# Hyperparameter selection on one split\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mechanism.mechanized_models import Mechanism\n",
    "from mechanism.mechanized_models import MechanizedGaussianNB, MechanizedLogisticRegression, MechanizedOneVSRest, MechanizedDecisionTree\n",
    "from mechanism.mechanized_models import MechanizedGridSearchCV\n",
    "\n",
    "\n",
    "def hyper_parameter(splits):\n",
    "    x_train, x_val, y_train, y_val = splits[0]\n",
    "    estimator = MechanizedLogisticRegression(max_iter=1500)\n",
    "    estimator.choose_mechanism(Mechanism.GAUSSIAN)\n",
    "    gs_cls = MechanizedOneVSRest(estimator = estimator)\n",
    "    gs_cls.choose_mechanism(Mechanism.GAUSSIAN)\n",
    "\n",
    "    params_LR = {'estimator__C': np.logspace(-0.2, 0.7, num = 10)}\n",
    "    gs_LR = GridSearchCV(estimator=gs_cls, param_grid=params_LR, cv = 2, verbose=2, scoring='f1_macro')\n",
    "    gs_LR.fit(x_train, y_train)\n",
    "\n",
    "BEST_C = 1.5848931924611134\n",
    "'''\n",
    "C=1.5848931924611134, max_iter=1500\n",
    "'''\n",
    "\n",
    "X_DATA, Y_DATA = load_and_process_data()\n",
    "\n",
    "# print(len(X_DATA))\n",
    "\n",
    "def eval(estimator, mechanism, splits):\n",
    "    # f1_scores, acc_scores, models = [], [], [], [], []\n",
    "    x_train, x_val, y_train, y_val = splits[0]\n",
    "    estimator = estimator\n",
    "    \n",
    "    model = MechanizedOneVSRest(estimator, mechanism = mechanism)\n",
    "    # model.choose_mechanism(mechanism)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(x_val)\n",
    "\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "\n",
    "def eval_multiple_rounds(stepped_rounds, mechanism, non_adaptive_num):\n",
    "    splits = create_splits(X_DATA, Y_DATA, non_adaptive_num)\n",
    "    generalization_error_list = []\n",
    "    for r in stepped_rounds:\n",
    "        estimator = MechanizedLogisticRegression(C = BEST_C, max_iter = r, mechanism = mechanism)\n",
    "        generalization_error_list.append(eval(estimator, mechanism, splits))\n",
    "    return generalization_error_list\n",
    "\n",
    "def eval_const_rounds(round, mechanism, stepped_non_adaptive_num):\n",
    "    generalization_error_list = []\n",
    "    for n_splits in stepped_non_adaptive_num:\n",
    "        splits = create_splits(X_DATA, Y_DATA, n_splits)\n",
    "        estimator = MechanizedLogisticRegression(C = BEST_C, max_iter = round, mechanism = mechanism)\n",
    "        generalization_error_list.append(eval(estimator, mechanism, splits))\n",
    "    return generalization_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_error(rounds, generalization_error, mechanism):\n",
    "    plt.plot(rounds, generalization_error, label = mechanism)\n",
    "    plt.xlabel(\"Queries\")\n",
    "    plt.ylabel(\"RMSE (Generalization Error) for adaptive queries\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "stepped_non_adaptive_num = range(2, 4, 1)\n",
    "round = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_generalization_error_list = eval_const_rounds(round, Mechanism(mechanism_type = Mechanism.MechanismType.NONE), stepped_non_adaptive_num)\n",
    "# baseline_generalization_error_list = [0.47190818773552584, 0.473107228502912, 0.473792394655704, 0.47070914696813976, 0.4659129838985954, 0.4720794792737239, 0.47447756080849607, 0.473107228502912, 0.4775608084960603, 0.47619047619047616, 0.4712230215827338, 0.47447756080849607, 0.47567660157588215, 0.473792394655704, 0.47533401849948614]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_generalization_error_list = eval_const_rounds(round, Mechanism(mechanism_type = Mechanism.MechanismType.GAUSSIAN, sigma = 0.1), stepped_non_adaptive_num)\n",
    "# gaussian_generalization_error_list = [0.3984241178485783, 0.4035628639945187, 0.39482699554642003, 0.3977389516957862, 0.3977389516957862, 0.3917437478588558, 0.3975676601575882, 0.39585474477560806, 0.38866050017129156, 0.3970537855429942, 0.39568345323741005, 0.3857485440219253, 0.38506337786913325, 0.39157245632065774, 0.3871188763275094]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x_range = [len(X_DATA)/ step for step in stepped_non_adaptive_num]\n",
    "plot_error(x_range, baseline_generalization_error_list, \"Baseline\")\n",
    "plot_error(x_range, gaussian_generalization_error_list, \"Gaussian\")\n",
    "# plot_error(stepped_rounds, threshold_generalization_error_list, \"Threshold\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
