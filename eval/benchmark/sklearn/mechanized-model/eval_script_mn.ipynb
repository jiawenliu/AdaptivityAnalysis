{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_with_str_seed(init_str):\n",
    "    \"\"\"\n",
    "    Initializes random number generator with seed corresponding to given input string init_str.\n",
    "    :param init_str: Initialization string according to which seed will be computed. Seed is the sum of the ASCII\n",
    "                     values of each character in init_str.\n",
    "    \"\"\"\n",
    "    rnd_val = 0\n",
    "    if init_str:\n",
    "        for c in init_str:\n",
    "            rnd_val += ord(c)\n",
    "    np.random.seed(rnd_val)\n",
    "\n",
    "def gen_data(n, d, seed = None):\n",
    "    if seed:\n",
    "        initialize_with_str_seed(seed)\n",
    "    p = (1.0 + np.sqrt(max(2 * Q_MEAN - 1, 1 - 2 * Q_MEAN))) / 2 \n",
    "    data = np.random.choice([-1, 1], (n, d), p=[1 -p, p])\n",
    "    data_y = np.random.choice([0, 1], n, p=[1 -p, p])\n",
    "    return data, data_y\n",
    "\n",
    "def gen_valid(n, d, seed = None):\n",
    "    if seed:\n",
    "        initialize_with_str_seed(seed)\n",
    "    \n",
    "    n = int(n/10)\n",
    "    \n",
    "    p = (1.0 + np.sqrt(max(2 * Q_MEAN - 1, 1 - 2 * Q_MEAN))) / 2 \n",
    "    data = np.random.choice([-1, 1], (n, d), p=[1 -p, p])\n",
    "    data_y = np.random.choice([0, 1], n, p=[1 -p, p])\n",
    "    return data, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_population, y_population = gen_data(POPULATION_SIZE, TRAIN_DIM)\n",
    "x_valid, y_valid = gen_data(int(POPULATION_SIZE/500), TRAIN_DIM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, mean_squared_error, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Basic preprocessing\n",
    "def preprocess(df):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Discard identifiers, style information, timestamps\n",
    "    df_new = df_new[df_new.columns.difference(['image', 'style', 'reviewTime', \n",
    "                                               'reviewerID', 'asin', 'reviewerName', 'unixReviewTime'])]\n",
    "\n",
    "    # Turn category into binary features\n",
    "    for cat in df_new.category.unique():\n",
    "        df_new[cat] = df_new['category'] == cat\n",
    "\n",
    "    # Drop category column\n",
    "    df_new.drop(columns=['category'], inplace=True)\n",
    "\n",
    "    # NaN vote is 0 users found helpful\n",
    "    df_new.vote.fillna(0, inplace=True)\n",
    "    \n",
    "    # Turn vote into binary feature\n",
    "    df_new.vote = df_new.vote > 0\n",
    "    # df_new.vote.clip(0, 10)\n",
    "    # df_new.vote = df_new.vote / 10\n",
    "\n",
    "    # NaN summary is empty summary\n",
    "    df_new.summary.fillna('', inplace=True)\n",
    "\n",
    "    # Turn Booleans into binary variables\n",
    "    df_new.replace({False: 0, True: 1}, inplace=True)\n",
    "    \n",
    "    return df_new\n",
    "# Remove 'overall' column and add cutoff column applying cutoff\n",
    "def apply_cutoff(df, cutoff):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Apply cutoff\n",
    "    cut = df['overall'] > cutoff\n",
    "    df_new['cutoff'] = cut\n",
    "\n",
    "    # Drop overall and category\n",
    "    df_new.drop(columns=['overall'], inplace=True)\n",
    "    \n",
    "    # Turn Booleans into binary variables\n",
    "    df_new.replace({False: 0, True: 1}, inplace=True)\n",
    "    \n",
    "    return df_new\n",
    "def apply_tfidf(df, review_vectorizer, summary_vectorizer):\n",
    "    review_matrix = pd.DataFrame(data=review_vectorizer.transform(df.reviewText).toarray(), columns='R_' + review_vectorizer.get_feature_names_out())\n",
    "    summary_matrix = pd.DataFrame(data=summary_vectorizer.transform(df.summary).toarray(), columns='S_' + summary_vectorizer.get_feature_names_out())\n",
    "    df_new = pd.concat([df, review_matrix, summary_matrix], axis=1)\n",
    "    df_new.drop(columns=['summary', 'reviewText'], inplace=True)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# Processing the data - I\n",
    "# Preprocessing of training data\n",
    "def load_and_process_data():\n",
    "    training_df = pd.read_csv('../data/Training.csv')\n",
    "    test_df = pd.read_csv('../data/Test.csv')\n",
    "\n",
    "    proc_training_df = apply_cutoff(preprocess(training_df), 1)\n",
    "\n",
    "    # Set cutoff to be the label; define data_x and y accordingly\n",
    "    data_x = proc_training_df.drop('cutoff', axis=1)\n",
    "    data_y = proc_training_df['cutoff']\n",
    "\n",
    "    # Fit TF-IDF vectorizer for 'reviewText' and 'summary' features, creating max. 11500 features.\n",
    "    r_vectorizer = TfidfVectorizer(max_features=11500, stop_words='english', ngram_range=(1, 3))\n",
    "    s_vectorizer = TfidfVectorizer(max_features=11500, stop_words='english', ngram_range=(1, 3))\n",
    "    r_vectorizer.fit(data_x.reviewText)\n",
    "    s_vectorizer.fit(data_x.summary)\n",
    "\n",
    "    # Apply TF-IDF vectorization \n",
    "    data_x = apply_tfidf(data_x, r_vectorizer, s_vectorizer)\n",
    "\n",
    "    # Apply robust scaling\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    data_x = pd.DataFrame(scaler.fit_transform(data_x), columns=data_x.columns, index=data_x.index)\n",
    "\n",
    "    # Let us reduce the number of features by eliminating the statistically least correlated ones.\n",
    "    relcols = data_x.columns[abs(data_x.corrwith(data_y)) > 0.01]\n",
    "\n",
    "\n",
    "    # We will go with these columns.\n",
    "    data_x = data_x[relcols]\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "def create_splits(data_x, data_y, n_splits):\n",
    "    # 5-fold cross validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    splits = []\n",
    "    for train_idx, val_idx in kf.split(data_x, data_y):\n",
    "        # Apply split\n",
    "        x_train, x_val = data_x.iloc[train_idx], data_x.iloc[val_idx]\n",
    "        y_train, y_val = data_y.iloc[train_idx], data_y.iloc[val_idx]\n",
    "        \n",
    "        # Reset indices\n",
    "        x_train.reset_index(drop=True, inplace=True)\n",
    "        y_train.reset_index(drop=True, inplace=True)\n",
    "        x_val.reset_index(drop=True, inplace=True)\n",
    "        y_val.reset_index(drop=True, inplace=True)\n",
    "        splits.append((x_train, x_val, y_train, y_val))\n",
    "    return splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and create the population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_MEAN = 0.5\n",
    "EPOCH = 2\n",
    "POPULATION_SIZE = 500000\n",
    "TRAIN_DIM = 100\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_with_str_seed(init_str):\n",
    "    \"\"\"\n",
    "    Initializes random number generator with seed corresponding to given input string init_str.\n",
    "    :param init_str: Initialization string according to which seed will be computed. Seed is the sum of the ASCII\n",
    "                     values of each character in init_str.\n",
    "    \"\"\"\n",
    "    rnd_val = 0\n",
    "    if init_str:\n",
    "        for c in init_str:\n",
    "            rnd_val += ord(c)\n",
    "    np.random.seed(rnd_val)\n",
    "\n",
    "def gen_data(n, d, seed = None):\n",
    "    if seed:\n",
    "        initialize_with_str_seed(seed)\n",
    "    p = (1.0 + np.sqrt(max(2 * Q_MEAN - 1, 1 - 2 * Q_MEAN))) / 2 \n",
    "    data = np.random.choice([-1, 1], (n, d), p=[1 -p, p])\n",
    "    data_y = np.random.choice([0, 1], n, p=[1 -p, p])\n",
    "    return data, data_y\n",
    "\n",
    "def gen_valid(n, d, seed = None):\n",
    "    if seed:\n",
    "        initialize_with_str_seed(seed)\n",
    "    \n",
    "    n = int(n/10)\n",
    "    \n",
    "    p = (1.0 + np.sqrt(max(2 * Q_MEAN - 1, 1 - 2 * Q_MEAN))) / 2 \n",
    "    data = np.random.choice([-1, 1], (n, d), p=[1 -p, p])\n",
    "    data_y = np.random.choice([0, 1], n, p=[1 -p, p])\n",
    "    return data, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_population, y_population = gen_data(POPULATION_SIZE, TRAIN_DIM)\n",
    "x_valid, y_valid = gen_data(int(POPULATION_SIZE/500), TRAIN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mechanism.mechanized_models import Mechanism\n",
    "from mechanism.mechanized_models import MechanizedGaussianNB, MechanizedLogisticRegression, MechanizedOneVSRest, MechanizedDecisionTree\n",
    "from mechanism.mechanized_models import MechanizedGridSearchCV\n",
    "\n",
    "def hyper_parameter(splits):\n",
    "    x_train, x_val, y_train, y_val = splits[0]\n",
    "    estimator = MechanizedLogisticRegression(max_iter=1500)\n",
    "    estimator.choose_mechanism(Mechanism.GAUSSIAN)\n",
    "    gs_cls = MechanizedOneVSRest(estimator = estimator)\n",
    "    gs_cls.choose_mechanism(Mechanism.GAUSSIAN)\n",
    "\n",
    "    params_LR = {'estimator__C': np.logspace(-0.2, 0.7, num = 10)}\n",
    "    gs_LR = GridSearchCV(estimator=gs_cls, param_grid=params_LR, cv = 2, verbose=2, scoring='f1_macro')\n",
    "    gs_LR.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "BEST_C = 1.5848931924611134\n",
    "'''\n",
    "C=1.5848931924611134, max_iter=1500\n",
    "'''\n",
    "\n",
    "def generalization_error(true_val, pred_val):\n",
    "    return np.sqrt(abs(mean_squared_error(true_val, pred_val) - Q_MEAN))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_error(rounds, generalization_error, mechanism, color = None):\n",
    "    plt.plot(rounds, generalization_error, color, label = mechanism)\n",
    "    plt.xlabel(\"Queries\")\n",
    "    plt.ylabel(\"RMSE (Generalization Error) for adaptive queries\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the O(n*m) Adaptivity Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nm(round, train_size, mechanism):\n",
    "    # f1_scores, acc_scores, models = [], [], [], [], []\n",
    "    x_train, y_train = x_population[:train_size], y_population[:train_size]\n",
    "    \n",
    "    estimator = MechanizedLogisticRegression(C = BEST_C, max_iter = round, mechanism = mechanism, solver = 'lbfgs', random_state = np.random.randint(1000000))\n",
    "    model = MechanizedOneVSRest(estimator, mechanism = mechanism)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(x_valid)\n",
    "\n",
    "    return generalization_error(y_valid, y_pred)\n",
    "\n",
    "\n",
    "def eval_nm_rounds(stepped_rounds, mechanism, non_adaptive_num):\n",
    "    generalization_error_list = []\n",
    "    for r in stepped_rounds:\n",
    "        # estimator = MechanizedLogisticRegression(C = BEST_C, max_iter = r, mechanism = mechanism, solver = 'sag')\n",
    "        generalization_error_list.append(eval_nm(r, non_adaptive_num, mechanism))\n",
    "    return generalization_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepped_rounds = range(100, 1100, 10)\n",
    "non_adaptive_num = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_generalization_error_list = eval_nm_rounds(stepped_rounds, Mechanism(mechanism_type = Mechanism.MechanismType.NONE), non_adaptive_num)\n",
    "print((baseline_generalization_error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gaussian_generalization_error_list = eval_nm_rounds(stepped_rounds, Mechanism(mechanism_type = Mechanism.MechanismType.GAUSSIAN, sigma = 0.08), non_adaptive_num)\n",
    "print(gaussian_generalization_error_list) \n",
    "# = [0.3984241178485783, 0.4035628639945187, 0.39482699554642003, 0.3977389516957862, 0.3977389516957862, 0.3917437478588558, 0.3975676601575882, 0.39585474477560806, 0.38866050017129156, 0.3970537855429942, 0.39568345323741005, 0.3857485440219253, 0.38506337786913325, 0.39157245632065774, 0.3871188763275094]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_generalization_error_list = eval_nm_rounds(stepped_rounds, Mechanism(mechanism_type = Mechanism.MechanismType.THRESHOLD, sigma = 0.08, hold_frac = 0.7, threshold = 0.9), non_adaptive_num)\n",
    "# gaussian_generalization_error_list = [0.3984241178485783, 0.4035628639945187, 0.39482699554642003, 0.3977389516957862, 0.3977389516957862, 0.3917437478588558, 0.3975676601575882, 0.39585474477560806, 0.38866050017129156, 0.3970537855429942, 0.39568345323741005, 0.3857485440219253, 0.38506337786913325, 0.39157245632065774, 0.3871188763275094]\n",
    "\n",
    "print(threshold_generalization_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "plt.figure()\n",
    "x_range = stepped_rounds\n",
    "plot_error(x_range, baseline_generalization_error_list, \"Emripircal\", 'g')\n",
    "plot_error(x_range, gaussian_generalization_error_list, \"Gaussian\", 'r')\n",
    "plot_error(x_range, threshold_generalization_error_list, \"Threshold - Adaptfun\", \"y\")\n",
    "plt.savefig(\"../plots/nm_adaptivity.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
