\documentclass[a4paper,11pt]{article}
\usepackage[table]{xcolor}



\input{ldefs}
\input{prelude}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\newcommand{\THESYSTEM}{\textsf{AdaptFun}}

% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]

\begin{document}
\title{Program Analysis for Adaptivity Analysis}

\author{}

\date{}

\maketitle


\tableofcontents


% \section{Introduction}
\section{System Overview}


\section{Labeled {\tt While} Language}
\label{sec:while_language}
%
\subsection{Syntax and Semantics}
%
\paragraph{Language}
\begin{figure}[h]
	$$
	\begin{array}{rcl}
		\text{Types} & \quad & \tau ::= b \sep \tau \multimap \tau' \sep !_n \tau \sep
		\tau \times \tau \sep \tforallN{i}{\tau} \sep \query \\[2mm]
   
		\text{Term} & \quad & t ::= c \sep \fix{t} \sep \app{t}{t} \sep !t \sep (t_1,t_2) \sep  \letx{!x}{t_1}{t_2} \sep \Lambda.t \sep t[] \sep \abs{x}{t} \sep  M(t) \sep x \sep q \sep\\
		&& \quad   \tcaseof{t}\ \{c_i \Rightarrow t_i\}_{c_i \in b}  \sep \letx{(x_1,x_2)}{t_1}{t_2} \\[2mm]
		 
		\text{Normal Form} &\quad & v ::=  c \sep \fix{t} \sep !t \sep (v_1, v_2) \sep \Lambda. t \sep \abs{x}{t}  \sep x \sep q \sep \tcaseof{v}\ \{c_i \Rightarrow v_i\}_{c_i \in b_i} \sep\\
		&& \enil \sep \econs(v_1,v_2)  \\[2mm]
   
		\text{Mechanisms} &\quad & M ::=  {\tt gauss} \sep {\tt thdt} \\[2mm]
   
   
	   \text{Tree} &\quad& T_b :: = c \sep M(T_{query}) \sep \tcaseof{T_b}\ \{ c_i \Rightarrow T_{b_i}\}_{c_i \in b} \\
   
	   \text{} &\quad& T_{query} :: = q \sep \tcaseof{T_b}\ \{ c_i \Rightarrow T_{query_i}\}_{c_i \in b} \\[2mm]
		\text{Depth} &\quad&   \depth(c) = 0 \\
		  &\quad& \depth(!t) = \depth(t) \\
			  &\quad&      \depth( \app{t_1}{t_2} ) = \max(\depth(t_1), \depth(t_2)) \\
			   &\quad&  \depth(M(t)) = 1 + \depth(t) \\
				&\quad&  \depth(\abs{x}{t}) = \depth(t) \\
				 &\quad& \depth(x) = 0 \\
				 & \quad & \depth(q) = 0 \\
				 & \quad & \depth((t_1, t_2)) = \max(\depth(t_1), \depth(t_2))\\
				 & \quad & \depth(\letx{(x_1, x_2)}{t}{t'}) = \max(\depth(t), \depth(t'))\\
				 & \quad & \depth(\letx{!x}{t}{t'}) = \max(\depth(t), \depth(t'))\\
				 & \quad & \depth(\tcaseof{t}\ \{c_i \Rightarrow t_i\}_{c_i \in b}) = \max(\depth(t), \depth(t_i))\\
			   & \quad & \depth(\Lambda.t) = \depth(t)\\
				 & \quad & \depth(t\, []) = \depth(t)\\
   \end{array}
   $$
   \caption{syntax}
   \end{figure}
   

\subsection{ Trace-based Adaptivity}
%
We define adaptivity through a query-based dependency graph. In our model, an \emph{analyst} asks a sequence of queries to the mechanism, and the analyst receives the answers to these queries from the mechanism. A query is adaptively chosen by the analyst when the choice of this query is affected by answers from previous queries. In this model, the adaptivity we are interested in is the length of the longest sequence of such adaptively chosen queries, among all the queries the data analyst asks to the mechanism.  Also, when the analyst asks a query, the only information the analyst will have will be the answers to previous queries and the state of the program. It means that when we want to know if this query is adaptively chosen, we only need to check whether the choice of this query will be affected by changes of answers to previous queries. There are two possible situations that can  affect the choice of a query,  
either the query argument directly uses the results of previous queries (data dependency), or the control flow of the program with respect to a query (whether to ask this query or not) depends on the results of previous queries (control flow dependency).

{
As a first step, we give a definition of when one query may depend on a previous query, which is supposed to consider both control dependency and data dependency. We first look at two possible candidates:
\begin{enumerate}
    \item One query may depend on a previous query if and only if a change of the answer to the previous query may also change the result of the query.
    \item One query may depend on a previous query if and only if a change of the answer to the previous query may also change the appearance of the query.
\end{enumerate}
}

{
   The first candidate works well by witnessing the result of one query according to the change of the answer of another query. We can easily find that the two queries have nothing to do with each other in a simple example   
%
    $ c = \assign{x}{\query(\chi(1))} ; \assign{y}{\query(\chi(2))}$. This candidate definition works well with respect to data dependency. 
    However, if fails to handle control dependency since it just monitors the changes to the answer of a query when the answer of previous queries returned change. 
    The key point is that this query may also not be asked because of an analyst decision which depend on the answers of previous queries. 
    An example of this situation is shown in program $c_1$ as follows.
    \[
      c_1 = \assign{x}{\query(\chi(1))} ; \eif( x > 2 ,\assign{y}{\query(\chi(2))}, \eskip )
   	\]
	%   
   	We choose the second candidate, which performs well by witnessing the appearance of one query $\query(\chi(2))$ upon the change of the result of one previous query $\query(\chi(1))$ in $c_1$. 
   	It considers the control dependency, and at the same, does not miss the data dependency.
   	In particular, the arguments of a query characterizes it.
   	In this sense, if the data used in the arguments changes due to a different answer to a certain previous query, the appearance of the query may change as well.
   	This situation is also captured by our definition. 
   	Let us look at another variant of program $c$, $p_2$, in which the queries equipped with functions using previously assigned variables storing answer of its previous query.
    \[
      c_2 = \assign{x}{\query(\chi(2))} ; \assign{y}{\query(x+\chi(3))}
   	\]
    As a reminder, in the {\tt While} language, the query request is composed by two components: a symbol $\query$ representing a linear query type and the argument $\expr$, which represents the function specifying what the query asks. 
    So we do think $\query(\chi(1))$ is different from $\query(\chi(2))$.
    Informally, we think $\query(x+\chi(3))$ may depend on the query $\query(\chi(2))$, because equipped function of the former $x+\chi(3)$ depend on the data assigned with $\query(\chi(2))$.
    We can see the appearance definition catches data dependency in such a way, 
    since $\query(x+\chi(2))$ will not be the same query if the value of $x$ is changed.    
}

   We give a formal definition of variable may dependency based on the trace-based operational semantics as follows.
%
% 
%
% \begin{defn}
% \todo{[Remove ? Query May Dependency]}.
% \label{def:query_dep}
% \\
% \jl{
% One annotated query $\av_2 = ({\qval}_2,l_2, w_2)$ may depend on another query 
% $\av_1 = ({\qval}_1, l_1, w_1)$ in a program $c$,
% with a starting memory $m$ and a hidden database $D$, denoted as 
% %
% $\qdep(\av_1, \av_2, c, m, D)$ is defined below. 
% %
% \[
% \exists m_1,m_3,t_1,t_3,c_2,v_1.
% \\
% \left (
%   \begin{array}{l}   
% 	\config{m, c, [], []} \rightarrow^{*} 
% 	\config{m_1, [\assign{x}{\query({\qval}_1)}]^{l_1} ; c_2,  t_1, w_1} 
% 	\rightarrow^{\textbf{query-v}} 
% 	\\ 
% 	\config{m_1[v_1/x], c_2,
% 	t_1++[\av_1], w_1} \rightarrow^{*} \config{m_3, \eskip,
% 	t_3,w_3}
% 	 \\ 
% 	 \bigwedge
% 	 \left( 
% 	 \begin{array}{l}
% 		\av_2 \avin (t_3 - (t_1 ++ [\av_1])) 
% 		\\
% 		\implies 
% 		\exists v \in \qdom, v \neq v_1, m_3', t_3', w_3'.
% 		\config{m_1[v/x], {c_2}, t_1 ++ [\av_1], w_1} 
% 		\\ 
% 		\quad \quad 
% 		\rightarrow^{*}
% 		(\config{m_3', \eskip, t_3', w_3'} 
% 		\\ 
% 		\quad \quad 
% 		\land 
% 		\av_2 \not \avin (t_3'-(t_1 ++ [\av_1])))
% 	\end{array} 
% 	\right)
% 	\\
% 	\bigwedge
% 	\left( 
%     \begin{array}{l}
% 		\av_2 \not\avin (t_3 - (t_1 ++ [\av_1]))
% 	  	\\
% 	  	\implies 
% 		\exists v \in \qdom, v \neq v_1, m_3', t_3', w_3'. 
% 		\config{m_1[v/x], {c_2}, t_1 ++ [\av_1], w_1}
% 		\\ 
% 		\quad \quad 
% 		\rightarrow^{*} 
% 		(\config{m_3', \eskip, t_3', w_3'} 
% 		\\ 
% 		\quad \quad 
% 		\land 
% 		\av_2  \avin (t_3' - (t_1 ++ [\av_1])))
% 	\end{array} 
% 	\right)
% \end{array}
% \right )
% \]
% }
% \end{defn}
% %
% \begin{defn}
% [remove :?: Query Variable May Dependency].
% \label{def:qvar_dep}
% \\
% {
% One annotated ssa variable $\av_2 = (\ssa{x}_2,l_2, w_2)$ may depend on another one 
% $\av_1 = (\ssa{x}_1, l_1, w_1)$ in a program $c$,
% with a starting memory $m$ and a hidden database $D$, denoted as 
% %
% $\vardep^{ssa}(\av_1, \av_2, c, m, D)$ is defined below. 
% %
% \[
% \exists \qval_1, \qval_2. ~
% \aq_1 = (\qval_1, l_1, w_1)
% \land
% \aq_2 = (\qval_2, l_2, w_2)
% \land 
% \qdep^{ssa}(\aq_1, \aq_2, c, m, D)
% \]
% }
% \end{defn}
%
\jl{
\begin{defn}
[Annotated Variables May Dependency]
\label{def:avar_dep}.
\\
One annotated variable $\av_2 = (x_2, v_2, l_2, n_2)$ may depend on another one 
$\av_1 = (x_1, v_1, l_1, n_1)$ in a program $c$,
with a starting memory $m$ and  hidden database $D$, denoted as 
%
$\avdep(\av_1, \av_2, c, m, D)$ is defined below. 
%
%
\[
\begin{array}{l}
\exists \ssa{m}_1, \ssa{m}_3, \vtrace_1, \vtrace_3, \ssa{c}_2, v_1, ({\qval}_1 \lor \sexpr_1).\\
  \left (\begin{array}{l}   
\config{\ssa{m}, \ssa{c}, [], []} \rightarrow^{*} 
\config{\ssa{m}_1, [\assign{\ssa{x}_1}{\query({\qval}_1) (/ \sexpr_1)}]^{l_1} ; \ssa{c}_1, \qtrace_1,  \vtrace_1, w_1} 
\rightarrow^{\textbf{ssa-query-v (/ assn-v)}} 
\\ 
\config{\ssa{m}_1[v_1/\ssa{x}], c_2, \qtrace_1', \vtrace_1 ++ [\av_1], w_1} 
\rightarrow^{*} \config{\ssa{m}_3, \eskip, \qtrace_3, \vtrace_3, w_3}
  % 
 \\ \bigwedge
  \left( 
  \begin{array}{l}
  \av_2 \in (\vtrace_3'-(\vtrace_1 ++ [\av_1])) 
  % 
  \\
  \implies 
  \exists v \in \qdom, v \neq v_1, \ssa{m}_3', \qtrace_3', \vtrace_3', w_3'.  
  \config{\ssa{m}_1[v/\ssa{x}], {\ssa{c}_2}, \qtrace_1', \vtrace_1 ++ [\av_1], w_1} 
  \\ 
  \quad \quad 
  \rightarrow^{*}
  (\config{\ssa{m}_3', \eskip, \qtrace_3', \vtrace_3', w_3'} 
		\\ 
		\quad \quad 
  \land 
  \av_2 \not\in (\vtrace_3'-(\vtrace_1 ++ [\av_1])))
\end{array} \right )
\\\bigwedge
\left( 
  \begin{array}{l}
  	\av_2 \notin (\vtrace_3 - (\vtrace_1 ++ [\av_1]))
  	% 
  	\\
  	\implies 
	\exists v \in \qdom, v \neq v_1, \ssa{m}_3', \qtrace_3', \vtrace_3', w_3'. 
	\config{\ssa{m}_1[v /\ssa{x}], {\ssa{c}_2}, \qtrace_1', \vtrace_1 ++ [\av_1], w_1}
	\\ 
	\quad \quad 
	\rightarrow^{*} 
	(\config{\ssa{m}_3', \eskip, \qtrace_3', \vtrace_3', w_3'} 
		\\ 
		\quad \quad 
	\land 
	\av_2  \in (\vtrace_3' - (\vtrace_1 ++ [\av_1])))
\end{array} \right )
\end{array} \right )
\end{array}
\]
%
\end{defn}
%
\begin{defn}
[Annotated Variables May Dependency -- Version 2]
\label{def:avar_dep2}.
\\
One annotated variable $\av_2 = (x_2, v_2, l_2, n_2)$ may depend on another one  $\av_1 = (x_1, v_1, l_1, n_1)$in a program $\ssa{c}$,
with a starting memory $\ssa{m}$ and hidden database $D$, denoted as 
%
$\avdep(\av_1, \av_2, c, m, D)$ is defined below. 
%
\[
\begin{array}{l}
\exists \ssa{m}, \ssa{m}_1, \ssa{m}_2, \ssa{m}_3, \ssa{m}_2', \ssa{m}_3', 
\vtrace_1, \vtrace_2, \vtrace_2', t_1, t_2, t_2', \ssa{c}_1, \ssa{c}_2, v_1'.
\\
  \left(
  \begin{array}{l}   
\config{\ssa{m}, \ssa{c}, []} \rightarrow^{*} 
\config{\ssa{m}_1, [\assign{\ssa{x}_1}{\query({\qval}_1) (/ \sexpr_1)}]^{l_1} ; \ssa{c}_1, \vtrace_1, t_1} 
\\ 
 \bigwedge
 \config{\ssa{m}_1[v_1/\ssa{x}_1], c_1, \vtrace_1 ++ [\av_1], t_1[\ssa{x}_1]++} 
\rightarrow^{*} 
\config{\ssa{m}_2, [\assign{\ssa/{x}_2}{\query({\qval}_2) (/ \sexpr_2)}]^{l_2} ; \ssa{c}_2, \vtrace_2, t_2} 
\\
\qquad \rightarrow^{\textbf{{ssa-query-v} (/ assn-v)}} 
\config{\ssa{m}_3, \ssa{c}_2,  \vtrace_2 ++ [\av_2], t_2[\ssa{x}_2]++} 
  % 
 \\ 
 \bigwedge
 \config{\ssa{m}_1[v_1'/\ssa{x}_1], \ssa{c}_1, \vtrace_1, t_1} 
\rightarrow^{*} 
\config{\ssa{m}_2', \ssa{c}_2,  \vtrace_2', t_2'}
\\
\bigwedge
\av_2 \notin \vtrace_2'
\end{array}
\right)
\end{array}
 \]
%
\end{defn}
%
\begin{defn}[Variable May Dependency].
\label{def:var_dep}
\\
Given a program $\ssa{c}$ with its assigned variables $\avar_{\ssa{c}}$, 
one variable $\ssa{x}_2 \in \avar_{\ssa{c}}$ may depend on another variable 
$\ssa{x}_1 \in \avar_{\ssa{c}}$ in $\ssa{c}$ denoted as 
%
$\vardep(\ssa{x}_1, \ssa{x}_2, \ssa{c})$ is defined below.
%
\[
\exists v_1, v_2, n_1, n_2, m, D. ~
\av_1 = (x_1, v_1, l_1, n_1)
\land
\av_2 = (x_2, v_2, l_2, n_2)
\land 
\avdep(\av_1, \av_2, c, m, D)
\] 
%
%
\end{defn}
%
%
\begin{defn}[Execution Based Dependency Graph].
\\
Given a program $c$, a database $D$, a starting memory $m$ with its assigned variables $\avar_c$ and initial variable counter $\vcounter^0_{c}$ with its corresponding execution:
$\config{m, c, [], \vcounter^0_{c}} 
\to^{*}
\config{\ssa{m'}, \eskip, \vtrace, \vcounter}$,
the dependency graph $\traceG(c, m, D) = (\vertxs, \edges, \weights, \qflag)$ is defined as:
%
\[
\begin{array}{rlcl}
	\text{Vertices} &
	\vertxs & := & \left\{ 
	x \in \mathcal{VAR}
	~ \middle\vert ~
	x = \avar_{c}(i); i = 0, \ldots, |\avar_{c}| 
	\right\}
	\\
	\text{Directed Edges} &
	\edges & := & 
	\left\{ 
	(x, x') \in \mathcal{VAR} \times \mathcal{VAR}
	~ \middle\vert ~
	\vardep(x, x', c); 
	x = \avar_{c}(i); x' = \avar_{c}(j); i,j = 0, \ldots, |\avar_{c}| 
	\right\}
	\\
	\text{Weights} &
	\weights & := & 
	\left\{ 
	(x, n) \in \mathcal{VAR} \times \mathbb{N}
	~ \middle\vert ~
	n = \vcounter(x); x = \avar_{c}(i); i = 0, \ldots, |\avar_{c}|
	\right\}
	\\
	\text{Query Flags} &
	\qflag & := & 
	\left\{(x, n)  \in \vertxs \times \{0, 1\} 
	~ \middle\vert ~
	\left\{
	\begin{array}{ll}
	n = 1 & x \in \qvar_{c} \\ 
	n = 0 & o.w.
	\end{array}
	\right\};
	x = \avar_{c}(i); i = 0, \ldots, |\avar_{c}|
	\right\}
\end{array}
\]
\end{defn}
%
%
\begin{defn}[Finite Walk ($k$)].
\label{def:finitewalk}
\\
Given a labeled weighted graph $G = (\vertxs, \edges, \weights, \qflag)$, a \emph{finite walk} $k$ in $G$ is a sequence of edges $(e_1 \ldots e_{n - 1})$ 
for which there is a sequence of vertices $(v_1, \ldots, v_{n})$ such that:
\begin{itemize}
    \item $e_i = (v_{i},v_{i + 1})$ for every $1 \leq i < n$.
    \item every vertex $v \in \vertxs$ appears in this vertices sequence $(v_1, \ldots, v_{n})$ of $k$ at most $W(v)$ times.  
\end{itemize}
$(v_1, \ldots, v_{n})$ is the vertex sequence of this walk.
\\
%
Length of this finite walk $k$ is the number of vertices in its vertex sequence, i.e., $\len(k) = n$.
\end{defn}
%
Given a labeled weighted graph $G = (\vertxs, \edges, \weights, \qflag)$, 
we use $\walks(G)$ to denote a set containing all finite walks $k$ in $G$;
and $k_{v_1 \to v_2} \in \walks(G)$where $v_1, v_2 \in \vertxs$ denotes the walk from vertex $v_1$ to $v_2$ .
%
%
\begin{defn}[Length of Finite Walk w.r.t. Query ($\qlen$)].
\label{def:qlen}
\\
Given a labeled weighted graph $G = (\vertxs, \edges, \weights, \qflag)$ and a \emph{finite walk} $k$ in $G$ with its vertex sequence $(v_1, \ldots, v_{n})$, the length of $k$ w.r.t query is defined as:
\[
	\qlen(k) = \len\big(
	v \mid v \in (v_1, \ldots, v_{n}) \land \flag(v) = 1 \big)
\]
, where $\big(v \mid v \in (v_1, \ldots, v_{n}) \land \flag(v) = 1 \big)$ is a subsequence of $k$'s vertex sequence.
\end{defn}
%
Given a program $c$ with a starting memory $m$ and database $D$, we generate its program-based graph 
$\traceG(c, m, D) = (\vertxs, \edges, \weights, \qflag)$.
%
Then the adaptivity bound based on program analysis for $\ssa{c}$ is the number of query vertices on a finite walk in $\progG(\ssa{c})$. This finite walk satisfies:
%
\begin{itemize}
\item the number of query vertices on this walk is maximum
\item the visiting times of each vertex $v$ on this walk is bound by its weight $\weights(v)$.
\end{itemize}
%
It is formally defined in \ref{def:trace_adapt}.
%
\begin{defn}
[Adaptivity of A Program].
\label{def:trace_adapt}
\\
Given a program $\ssa{c}$ in SSA language, 
its adaptivity is defined for all possible starting SSA memory $\ssa{m}$ and database $D$ as follows:
%
$$
A(c) = \max \big 
\{ \qlen(k) \mid \ssa{m} \in \mathcal{SM},D \in \dbdom , k \in \walks(\traceG(c, m, D) \big \} 
$$
\end{defn}
}
\\
%
%
We proved some useful properties for our language.
\\
%
\todo{
\begin{defn}[Well-formed Trace]
\label{def:wf_trace}
A trace $t$ is well formed if and only if it preserves the following two properties:
\begin{itemize}
	\item{\emph{(Uniqueness)}} $\forall \av_1, \av_2 \avin t. ~ (\av_1 \avneq \av_2)$
	%
	\item{\emph{(Ordering)}} $\forall \av_1, \av_2 \avin t. ~ 
	(\av_1 \avlt \av_2) \Longleftrightarrow
	\exists t_1, t_2, t_3, \av_1', \av_2'. ~ s.t.,~ 
	(\av_1 \aveq \av_1') \land (\av_2 \aveq \av_2') \land t_1 ++ [\av_1'] ++ t_2 ++ [\av_2'] ++ t_3 = t$
\end{itemize}
\end{defn}
}
\\
\todo{
\begin{thm}[Variable Trace Generated from Operational Semantics is Well-formed].
\label{thm:os_wf_trace}
\\
Given a program $c$, 
with arbitrary starting memory $m$, trace $\vtrace$ and variable counter $\vcounter$
if $\config{m, c, \vtrace, \vcounter} \to^{*} 
\config{m', \eskip, \vtrace', \vcounter'}$, then $(\vtrace' - \vtrace)$ is a well formed trace with respect to program $c$, $m$ and $w$, denoted as $m, c \vDash \vtrace' - \vtrace$.
% \wq{ we call a trace $t$ satisfies the program $c$ in the memory $m$, denoted as $m, c \vDash t$, if
% there exists the evaluation 
% $\config{m, c, [], []} \to^{*} \config{m', \eskip, t, w}$, and
% $t$ is well-formed. }
\end{thm}
\begin{proof}
Proof in File: {\tt ``thm\_os\_wf\_trace.tex''}.
% \input{thm_os_wf_trace}
\end{proof}
}
%
% \\
%
%
\todo{
\begin{lem}[While Map Remains Unchanged (Invariant)]
\label{lem:wunchange}
Given a program $c$ with a starting memory $m$, trace $t$ and while map $w$, s.t.,
$\config{m, c, t, w} \to^{*} \config{m', \eskip, t', w'}$ and $Labels(c) \cap Keys(w) = \emptyset$, then 
\[
	w = w'
\]
\end{lem}
\begin{subproof}[Proof of Lemma~\ref{lem:wunchange}]
%
Proof in File: {\tt ``lem\_wunchange.tex''}
% \input{lem_wunchange}
%
\end{subproof}
}
%
\todo{
\begin{lem}[Trace is Written Only]
\label{lem:twriteonly}
Given a program $c$ with starting trace $t_1$ and $t_2$,
for arbitrary starting memory $m$ and while map $w$,
if there exist evaluations
$$\config{m, c, t_1, w} \to^{*} \config{m_1', \eskip, t_1', w_1'}$$
% 
$$\config{m, c, t_2, w} \to^{*} \config{m_2', \eskip, t_2', w_2'}$$
%
then:
%
\[
	m_1' = m_2' \land w_1' = w_2'
\]
\end{lem}
%
\begin{subproof}[Proof of Lemma~\ref{lem:twriteonly}]
%
Proof in File: {\tt ``lem\_twriteonly.tex''}
% \input{lem_twriteonly}
\end{subproof}
}
%
\todo{
\begin{lem}[Trace Uniqueness]
\label{lem:tunique}
Given a program $c$ with a starting memory $m$, \wq{a while map w,}
for any starting trace $t_1$ and $t_2$, if there exist evaluations
$$\config{m, c, t_1, w} \to^{*} \config{m_1', \eskip, t_1', w_1'}$$
% 
$$\config{m, c, t_2, w} \to^{*} \config{m_2', \eskip, t_2', w_2'}$$
%
then:
%
\[
	t_1' - t_1 = t_2' - t_2
\]
\end{lem}
%
\begin{subproof}[Proof of Lemma~\ref{lem:tunique}]
%
Proof in File: {\tt ``lem\_tunique.tex''}
% \input{lem_tunique}
\end{subproof}
}
%
\todo{
\begin{coro}
\label{coro:aqintrace}
\[
\av \avin t \implies \exists t_1, t_2, \av'. ~ s.t., ~ (\av \aveq \av') \land t_1 ++ [\av'] ++ t_2 = t	
\]
\end{coro}
\begin{subproof}
Proof in File: {\tt ``coro\_aqintrace.tex''}
% \input{coro_aqintrace}
%
\end{subproof}
}
%
\begin{lem}
[Trace Non-Decreasing].
\\
\jl{
For any program $c$ with a starting memory $m$, trace $t$ and while map $w$: 
$$
\config{m, c, t, w} 
\rightarrow
\config{m, c', t', w'} \implies \exists ~ t'', ~ s.t., ~ t ++ t'' = t'
$$
}
\end{lem}
%
\begin{proof}
{
Proof is obvious by induction on the operational semantic rules applied in the transition 
.
\\
By induction on the operational semantic rules applied in the transition $\config{m, c, t, w} 
\rightarrow
\config{m, c', t', w'}$, 
we have cases for each rule.
By observation on the rules, 
the trace $t$ remains unchanged in all the rules except the only one \textbf{query-v}.
So, the rule \textbf{query-v} is the only interesting case to be discussed as following.
\begin{itemize}
\caseL{
\[
	\inferrule
	{
	\query(\qval) = v
	}
	{
	\config{m, [\assign{x}{\query(\qval)}]^l, t, w} \xrightarrow{}  
	\config{m, \eskip, t ++ [(\qval, l, w)], w}
	}
	~\textbf{query-v}
\]
}
%
In this case, we have $c' = \eskip$, 
$t' = t ++ [(\qval, l, w)]$, $m' = m[v/x]$ and $w' = w$.
\\
Let $t'' = [(\qval, l, w)]$, we have $t ++ [(\qval, l, w)] = t'$,
i.e., $t ++ t'' = t'$. This case is proved.
\end{itemize}
}
\end{proof}
%
%
\todo{
The following lemma describes a property of the trace-based dependency graph.
For any program $c$ with a database $D$ and a starting memory $m$,
the directed edges in its trace-based dependency graph can only be constructed from nodes representing 
smaller annotated queries to annotated queries of greater order.
There doesn't exist backward edges with direction from greater annotated queries to smaller ones.
}
\begin{lem}
\label{lem:edgeforwarding}
[Edges are Forwarding Only].
\\
%
{
Given a program $c$, a database $D$, a starting memory $m$ and the corresponding trace-based dependency graph $G(c,D,m) = (\vertxs, \edges)$, 
for any directed edge $(\av', \av) \in \edges$, 
this is not the case that:
%
$$\av' \avgeq \av$$
%
}
\end{lem}
%
\begin{proof}
Proof in File: {\tt ``edge\_forward.tex''}.
% \input{edge_forward}
\end{proof}
%
%
%
\begin{lem}
\label{lem:DAG}
[Trace-based Dependency Graph is Directed Acyclic].
\\
%
{
Every trace-based dependency graph is a directed acyclic graph.
}
\end{lem}
%
{
\begin{proof}
Proof is obvious based on the Lemma \ref{lem:edgeforwarding}.
\end{proof}
}
%
\begin{lem}
[Adaptivity is Bounded].
\\
{
Given the program $c$ with a certain database $D$ and starting memory $m$, the $A(c)$ w.r.t. the $D$ and $m$ is bounded, i.e.,:
%
\[
\config{m, c, [], []} 
\rightarrow^{*} 
\config{m', \eskip, t', w'} 
\implies
A_{D, m}(c) \leq |t'|
\]
}
\end{lem}
%
\begin{proof}
{
Proof is obvious based on the Lemma \ref{lem:DAG}.
}
\end{proof}
%
%
\clearpage
%
%
\input{ssa}
%
%
\section{\THESYSTEM}
There are four steps to get the adaptivity of a program $\ssa{c}$ based on analyzing the program. 
\begin{enumerate}
    \item Collecting the variables that are newly assigned in the program (via assignment expressions). These variables are stored in an assigned variable vector $\avar$. 
    We also track extra information of each assigned variable (whether it is assigned by a query result, or showing up in loop, or showing up in $\eif$ expression or o.w.) and store it in a vector $\flag$ of the same size as $\avar$.
    %
    \item Tracking the data flow relations between all these assigned variables. These informations are stored in a matrix $\Mtrix$, whose size is $|\avar| \times |\avar|$. 
    %
    \item Estimating the reachability bound of each variable in $\avar$.
    %
    \item With all these informations from previous steps, generating a program-based dependency graph $\progG$ and compute the adaptivity bound.
\end{enumerate}

In the following subsections, 
we first define the notations and symbols being used in \THESYSTEM  with a simple example for understanding these definitions. 
Then we present the algorithmic analysis rules, which is the core of the \THESYSTEM, with
3 examples illustrating how \THESYSTEM  works.
In the following subsections, we present the adaptivity analysis based on the \THESYSTEM's analyzing results, and the soundness w.r.t. the trace-based analyzing results in previous sections.

\subsection{Notations}
%
\label{subsec:alg_notation}
%
\begin{defn}[Assigned Variables ($\avar$)]
Given a program $\ssa{c}$, its assigned variables $\avar$ is a vector containing all variables newly assigned in the program preserving the order. 
It is defined as follows:
$$
  \avar_{\ssa{c}} \triangleq
  \left\{
  \begin{array}{ll}
   		[\ssa{x}] 									
   		& \ssa{c} = [\ssa{\assign x e}]^{(l, w)} 
   		\\
     	\left[ \ssa{x} \right] 									
     	& \ssa{c} = [\ssa{\assign x \query(\qexpr)}]^{(l, w)} 
     	\\
     	\avar_{\ssa{c_1}} ++ \avar_{\ssa{c_2}} 	
     	& \ssa{c} = \ssa{c_1};\ssa{c_2}
     	\\
     	\avar_{\ssa{c_1}} ++ \avar_{\ssa{c_2}} ++ \ssa{[\bar{x}, \bar{y}, \bar{z}]} 
     	& \ssa{c} =\eif([\sbexpr]^{(l, w)} , \ssa{[\bar{x}, \bar{x_2}, \bar{x_2}], 
     	[\bar{y}, \bar{y_2}, \bar{y_3}], 
     	[\bar{z}, \bar{z_2}, \bar{z_3}], c_1, c_2}) 
     	\\
     	\avar_{\ssa{c}'} ++ [\ssa{\bar{x}}]
     	& \ssa{c} 	= \ewhile ([\sbexpr]^{(l, w)}, [\ssa{\bar{x}, \bar{x_2}, \bar{x_2}}], \ssa{c}')
\end{array}
\right.
$$
\end{defn}
%
\jl{
We are abusing the notations and operators from list here. 
The notation $[]$ represents an empty vector
and $x::A$ represents add an element $x$ to the head of the vector $A$.
The concatenation operation between 2 vectors $A_1$ and $A_2$, i.e., $A_1 ++ A_2$ is mimic the standard list concatenation operations as follows:
%
\begin{equation}
		A_1 ++ A_2  
		\triangleq \left\{
		\begin{array}{ll} 
			A_2 				& A_1 = []\\
			x::(A_1' ++ A_2)	& A_1 = x::A_1'
		\end{array}
		\right.
\end{equation}
%
We use index within parenthesis to denote the access to the element of corresponding location,
$A(i)$ denotes the element at location $i$ in the vector $A$ and 
$M(i, j)$ denotes the element at location $i$-th raw, $i$-th column in the matrix $M$. 
}
%

Consider the program $c$ below in the left hand side as an example, its assigned variables $\avar$ (short for $\avar(\ssa{c})$) is as in the right hand side is shown as follows:
$$
\ssa{c} = 
\begin{array}{l}
\left[\ssa{\assign {x_1} {\query(0)}}		\right]^1;
\\
\left[\ssa{\assign {x_2} {x_1 + 1}}		\right]^2;
\\
\left[\ssa{\assign {x_3} {x_2 + 2}}		\right]^3
\end{array}
~~~~~~~~~~~~
\avar = \left [ 
\begin{matrix}
\ssa{x_1} \\
\ssa{x_2} \\
\ssa{x_3} \\
\end{matrix} \right ]
$$
%
\begin{lem}
For any program $\ssa{c}$, every variable in $\avar(\ssa{c})$ is distinct
\end{lem}
\begin{proof}
 It is due to the SSA nature. We can prove it by induction on $\ssa{c}$.
\end{proof}

\jl{
\begin{defn}[Variable Flags ($\flag$)].
\\
Given a program  $\ssa{c}$ with its assigned variables $\avar$, the $\flag$ is a vector of the same length as $\avar$, s.t. for each variable $\ssa{x}$ showing up as the $i$-th element in $\avar$ (i.e., $\ssa{x} = \avar(i)$), 
$\flag(i) \in \{0, 1, 2\}$ is defined as follows:
%
%
\[
	\flag(i) := 
	\left\{
	\begin{array}{ll}
	2 & 
	\ssa{x} = \avar(i) \land (\exists \ssa{\qexpr}. ~ s.t., ~
	[\assign{\ssa{x}}{\query(\ssa{\qexpr})}]^l \in_{c} \ssa{c})
	\\
	1 &  
	\begin{array}{l}
	\ssa{x} = \avar(i) \bigwedge \\
	\left(
	\begin{array}{l}
	\big(\exists  ~ \ssa{c'}, \ssa{\expr}, \sbexpr, l, l'. ~
		\ewhile [\sbexpr]^l \edo \ssa{c'} \in_{c} \ssa{c}
		\land 
		[\ssa{\assign{x}{\expr}}]^{l'} \in_{c}  \ssa{c'}
	\big) \bigvee
	\\
	\big(\exists ~ \sbexpr, l, l_1, l_2, \ssa{c_1}, \ssa{c_2}, \ssa{\expr}_1, \ssa{\expr}_2. ~
		\eif([\sbexpr]^l, \ssa{c_1}, \ssa{c_2}) \in_{c} \ssa{c} \land
		([\ssa{\assign{x}{\expr_1}}]^{l1} \in_{c} \ssa{c_1} \lor 
		[\ssa{\assign{x}{\expr_2}}]^{l2} \in_{c} \ssa{c_2})
	\big)
	\end{array}
	\right)
	\end{array}
	\\
	0 & \text{o.w.}
	\end{array}
	\right\}. 
\] 
%
\end{defn}
%
Operations on $\flag$ are defined as follows:
\begin{equation}
\begin{array}{llll}
{\flag_1 \uplus \flag_2}(i) & := &
\left\{
\begin{array}{ll}
k & k = \max{\big\{\flag_1(i), \flag_2(i)\big\}} 
	\land |\flag_1| = |\flag_2|\\
0 & o.w.
\end{array}\right.
& i = 1, \cdots, |\flag_1|  
\\
{\flag \uplus n}(i) & := & 
\max\big\{ \flag(i), n \big\} 
& i = 1, \ldots, |\flag|    
\\
\left[ n \right]^k (i) & := &  n
& i = 1, \ldots, k ~ \land ~ |\left[ n \right]^k| = k
\end{array}
\end{equation}
%
\todo{
Given a program  $\ssa{c}$ with its assigned variables $\avar$,
and two variables $\ssa{x}$, $\ssa{y}$ showing up as $i$-th, $j$-th elements in $\avar$ 
(i.e., $\ssa{x} = \avar(i)$ and $\ssa{y} = \avar(j)$),
we say $\ssa{y}$ flows to $\ssa{x}$ in $\ssa{c}$ if and only if $j < i$ and 
the value of $\ssa{y}$ directly or indirectly influence the evaluation of the value of $\ssa{x}$ as follows:
%
\begin{itemize}
	\item (Directly Influence) The program $\ssa{c}$ contains either 
	a command $\assign{\ssa{x}}{\sexpr}$ or $\assign{\ssa{x}}{\query(\ssa{\qexpr})}$,
	such that $\ssa{y}$ shows up as a free variable in $\sexpr$ or $\ssa{\qexpr}$.
	We use $\flowsto(\ssa{x, y, c})$ to denote $\ssa{y}$ flows to $\ssa{x}$ in $\ssa{c}$.
%
	\item (Indirectly Influence) The program $\ssa{c}$ contains either a while loop
	command
	or if condition command, such that $\ssa{y}$ shows up in the guard
	and $\ssa{x}$ shows up in the left hand of an assignment command in the body.
\end{itemize}
%
This is formally defined in \ref{def:flowsto}.
We use $FV(\expr)$, $FV(\sbexpr)$ and $FV(\qexpr)$ denote the set of free variables in 
expression $\expr$, boolean expression $\sbexpr$ and query expression $\qexpr$ respectively.
%
\begin{defn}[Data Flows between Assigned Variables ($\flowsto$)].
\label{def:flowsto}
\\
Given a program  $\ssa{c}$ with its assigned variables $\avar$,
and two variables $\ssa{x}$, $\ssa{y}$ s.t., $\ssa{x} = \avar(i)$ and $\ssa{y} = \avar(j)$,
$\ssa{y}$ flows to $\ssa{x}$ in $\ssa{c}$, i.e., $\flowsto(\ssa{x, y, c})$ is defined as:
%
\[
	\begin{array}{l}
	\flowsto(\ssa{x, y, c}) \triangleq 	(j < i) \land 
	\\
	\left( \bigvee
	\begin{array}{l}
	(\exists \sexpr, l . ~ [\assign{\ssa{x}}{\sexpr}]^l \in_{c} \ssa{c} 
	\land \ssa{y} \in FV(\sexpr))
	\\
	(\exists \ssa{\qexpr}, l. ~ [\assign{\ssa{x}}{\query(\ssa{\qexpr})}]^l \in_{c} \ssa{c} 
	\land \ssa{y} \in FV(\ssa{\qexpr}))
	\\
	\big(\exists  ~ \ssa{c'}, \ssa{\expr}, \sbexpr, l, l'. ~
		\ewhile [\sbexpr]^l \edo \ssa{c'} \in_{c} \ssa{c}
		\land 
		[\ssa{\assign{x}{\expr}}]^{l'} \in_{c}  \ssa{c'}
		\land \ssa{y} \in FV(\sbexpr)
	\big)
	\\
	\big(\exists ~ \sbexpr, l, l_1, l_2, \ssa{c_1}, \ssa{c_2}, \ssa{\expr}_1, \ssa{\expr}_2. ~
		\eif([\sbexpr]^l, \ssa{c_1}, \ssa{c_2}) \in_{c} \ssa{c} \land
		([\ssa{\assign{x}{\expr_1}}]^{l1} \in_{c} \ssa{c_1} \lor 
		[\ssa{\assign{x}{\expr_2}}]^{l2} \in_{c} \ssa{c_2})
		\land \ssa{y} \in FV(\sbexpr)
	\big)
	\end{array}
	\right).
	\end{array}
\]
%
\end{defn}
}
}
%
%
\begin{defn}[Data Flow Matrix ($\Mtrix$)]
Given a program  $\ssa{c}$ with its assigned variables $\avar$ of length $N$,
its data flow matrix $\Mtrix$ is a matrix of size $N \times N$ s.t.
$\forall \ssa{x, y} \in \avar. ~ \ssa{x} = \avar(i), \ssa{y} = \avar(j)$:
%
\[
\Mtrix(i, j) \triangleq
\left\{
\begin{array}{ll}
1	&	\flowsto(\ssa{x, y, c}) \\
0	& o.w.
\end{array}
\right.,
\ssa{x} = \avar(i); \ssa{y} = \avar(j); i, j = 1, \ldots, N .
\]
%
\end{defn}
%
Operations on the data flow matrices are defined as follows:
%
\begin{equation}
\Mtrix_1 ; \Mtrix_2 
:= \Mtrix_2 \cdot \Mtrix_1 + \Mtrix_1 + \Mtrix_2
\end{equation}
%
Consider the same program $c$ as above, its data flow matrix $\Mtrix$ and $\flag$ for the program $c$ is as follows:
$$
\ssa{c} = 
\begin{array}{l}
\left[\ssa{\assign {x_1} {\query(0)}}	\right]^1;
\\
\left[\ssa{\assign {x_2} {x_1 + 1}}		\right]^2;
\\
\left[\ssa{\assign {x_3} {x_2 + 2}}		\right]^3
\end{array}
~~~~~~~~~~~~
\Mtrix
=  \left[ 
\begin{matrix}
 0 & 0 & 0 \\
 1 & 0 & 0 \\
 1 & 1 & 0 \\
\end{matrix} \right] ~ , 
\flag = \left [ \begin{matrix}
1 \\
0 \\
0 \\
\end{matrix} \right ]
$$
%
There are two special matrices used for generating the data flow matrix $\Mtrix$ in the analysis algorithm. They are the left matrix $\lMtrix_i$ and right matrix $\mathsf{R_{(e, i)}}$.

Given a program  $\ssa{c}$ with its assigned variables $\avar$ of length $N$,
the left matrix $\lMtrix_i$ generates a matrix of $1$ column, $N$ rows, 
where the $i$-th row is $1$ and all the other rows are $0$.
%
\begin{defn}[Left Matrix ($\lMtrix_i$)].
\\
Given a program  $\ssa{c}$ with its assigned variables $\avar$ of length $N$, 
the left matrix $\lMtrix_i$ is defined as follows:
\[
	\lMtrix_i(j) : = 
	\left
	\{
	\begin{array}{ll}
	1 & j = i \\
	0 & o.w.
	\end{array}
	\right.,
	j = 1, \ldots, N.
\]
\end{defn}
%
Given a program  $\ssa{c}$ with its assigned variables $\avar$ of length $N$,
the right matrix $\rMtrix_{\expr, i}$ generates a matrix of one row and $N$ columns, 
where the locations of free variables in $\expr$ is marked as $1$. 
%
%
\begin{defn}[Right Matrix ($\rMtrix_{\expr}$)].
\\
Given a program  $\ssa{c}$ with its assigned variables $\avar$ of length $N$, 
the right matrix $\rMtrix_{\expr}$ is defined as follows:
\[
	\rMtrix_{\expr}(j) : = 
	\left\{
	\begin{array}{ll}
	1 & \ssa{x} \in FV(\expr) 
	\\
	0 & o.w.
	\end{array}
	\right.,
	\ssa{x} = \avar(j) ~ , ~ j = 1, \ldots, N.
\]
%
%
\end{defn}
%
Using the same example program $\ssa{c}$ as above with assigned variables $\avar = [ \ssa{x_1 , x_2 , x_3} ] $,
the left and right matrices w.r.t. its $2$-nd command 
$\left[\ssa{\assign {x_2} {x_1 + 1}}\right]^2$  are as follows:
\[
\lMtrix_1 = \left[ \begin{matrix}
 0   \\
 1 	 \\
 0   \\
\end{matrix}   \right ] 
~~~~~~~~~~~~~~
\rMtrix_{\ssa{x}_1 + 1}
= \left[ \begin{matrix} 
   1 & 0 & 0 \\
\end{matrix}  \right]
\]
%
%
%
\subsection{Algorithmic Analysis Rules}
%
\paragraph{Variable Collection Algorithm, $\varCol$}
% The $\varCol$ algorithm shows how the assigned variables $\avar$ are collected 
% (via the command $\ssa{\assign{x}{\expr}}$ or $\ssa{\assign{x}{\query(\qexpr)}}$) from the program $\ssa{c}$ in the first step.
% The algorithmic rules for $\varCol$ algorithm is defined in Figure~\ref{fig:var_col}. 
% It has the form: $\ag{\avar; w; \ssa{c}}{ \avar'; w'} $. 
% The input of $\varCol$ is the assigned variables $\avar$ collected before the program $\ssa{c}$, a while map $w$ consistent with previous estimation, a program $\ssa{c}$. 
% The output of the algorithm is the updated assigned variables $\avar'$, along with the updated while map $w$ for next steps' collecting.   
The $\varCol$ algorithm shows how the assigned variables $\avar$ are collected 
(via the command $\ssa{\assign{x}{\expr}}$ or $\ssa{\assign{x}{\query(\qexpr)}}$) from the program $\ssa{c}$ in the first step, 
along with constructing the flag for each variable, i.e., $\flag$.
The algorithmic rules for $\varCol$ algorithm is defined in Figure~\ref{fig:var_col}. 
It has the form: 
\jl{$\ag{\avar; \flag; \ssa{c}}{ \avar'; \flag'} $}. 
The input of $\varCol$ is a program $\ssa{c}$, 
the assigned variables $\avar$ collected before the program $\ssa{c}$ 
as well as the flags $\flag$ for every corresponding variable .
The output of the algorithm is the updated assigned variables $\avar'$ and flags $\flag'$ thorough the program $\ssa{c}$
%
% We have the algorithmic rules for $\varCol$ algorithm of the form: $\ag{\avar; w; \ssa{c}}{\avar';w'} $ as in Figure \ref{fig:var_col}. 
%
\begin{figure}
\jl{
\begin{mathpar}
\inferrule
{
\empty
}
{ \ag{\avar ; \flag; \ssa{[\assign {x}{\expr}]^{l}}}
 {\avar ++ [\ssa{x}]; \flag++[0]}
}
~\textbf{\varCol-asgn}
\and
\inferrule
{
}
{ \ag{\avar; \flag; [ \assign{\ssa{x}}{\query(\ssa{\qexpr})}]^{l}}
{\avar ++ [\ssa{x}]; \flag ++ [2]} 
}~\textbf{\varCol-query}
%
\and 
%
\inferrule
{
\ag{\avar; [];  \ssa{c_1}}{\avar_1; \flag_1}
\and 
\ag{\avar_1; []; \ssa{c_2}}{ \avar_2; \flag_2}
\and 
\avar' = [\bar{\ssa{x}}]++ \ssa{[\bar{y}]} ++ \ssa{[\bar{z}]}
 \\
k = \len(\avar')
\and
\avar_3 = \avar_2 ++ \avar'
 \and
 \flag_3 = \flag ++ ((\flag_1 ++ \flag_2) \uplus 1) ++ ([1]^k)
 }
{
\ag{\avar; \flag;
[\eif(\ssa{\bexpr},[ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}],
[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}],[ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}], \ssa{ c_1, c_2)}]^{l} }
{\avar_3; \flag_3}
}~\textbf{\varCol-if}
%
%
%
\and 
%
\inferrule
{
\ag{\avar; \flag \ssa{c_1}}{\avar_1; \flag_1}
\and 
\ag{\avar_1; \flag_1 ; \ssa{c_2}}{\avar_2; \flag_2}
}
{
\ag{\avar; \flag;
\ssa{(c_1 ; c_2)}}{\avar_2 ; \flag_2}
}
~\textbf{\varCol-seq}
\and 
%
%
{
\inferrule
{
{ \ag{\avar; [] ; \ssa{c}}
{\avar'; \flag' }  }
\\
\avar'' = \avar'++ \ssa{[\bar{x}]}
\and 
\flag'' = \flag ++ (\flag' \uplus 1) ++ ([1]^{\len(\ssa{[\bar{x}]})})
}
{
\ag{\avar; \flag;  
\ewhile [\ssa{b}]^{l}, \ssa{n}, 
[\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] 
\edo  \ssa{c} }{\avar''; \flag''}
}
~\textbf{\varCol-while}
 }
\end{mathpar}
}
 \caption{The Algorithmic Rules of $\varCol$ }
    \label{fig:var_col}
\end{figure}
%
%
The assignment commands are the source of variables $\varCol$ collecting, 
	in the case $\textbf{\varCol-asgn}$ and $\textbf{\varCol-query}$, 
	the output assigned variables are extended by $\ssa{x}$. 
\\
	When it comes to the $\eif \ldots \ethen \ldots \eelse$ command in the rule $\textbf{\varCol-if}$, variables assigned in the then branch $\ssa{c_1}$, as well as the variables assigned in the else branch $\ssa{c_2}$, and the new generated variables $\bar{\ssa{x}},\bar{\ssa{y}},\bar{\ssa{z}}$ in $ [ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ,[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}],[ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}]$.
\\ 
	The sequence command $\ssa{c_1;c_2}$ is standard by accumulating the predicted variables in the two commands $\ssa{c_1}$ and $\ssa{c_2}$ preserving their order. 
\\
	The while command $\ewhile \ssa{\bexpr}, [\ssa{\bar{x}}] \ldots \edo \ssa{c}$ considers the newly generated variables by SSA transformation $\ssa{\bar{x}}$
	as well and the newly assigned variables in its body $\ssa{c}$.

%
Below we present the definition for a valid index, to have a clear understanding on the variable collecting algorithm:
%
%
\jl{
\begin{defn}[Valid Index (Remove?)]
Given an assigned variable list $\avar$, $\avar; \vDash (\ssa{c},i_1,i_2)$ iff 
$\avar' = \avar[0,\ldots, i_1-1], \avar';\ssa{c} \to \avar'' \land \avar'' = \avar[0, \ldots, i_2-1] $.  
\end{defn}}
%
%
\paragraph{Data Flow Matrix Generating Algorithm}
%
In this data flow matrix generating algorithm, we analyze the data flow information among all assigned variables $\avar$ collected via the the $\varCol$ algorithm of length $N$.
%
We track the data flow relations between all these assigned variables. These informations are stored in a matrix $\Mtrix$, whose size is $N \times N$. 
% We also track whether arbitrary variable is assigned with a query result in a vector $\flag$ with size $|\avar|$. 
%
The algorithm to fill in the matrix is of the form: 
\jl{$\ad{\Gamma ; \ssa{c} ; \avar}{\Mtrix}$}
$\ad{\Gamma ; \ssa{c} ; i_1, i_2}{\Mtrix; \flag}$. 
$\Gamma$ is a vector records the variables the current program $\ssa{c}$ depends on, the index $i_1$ is a pointer which refers to the position of the first new-generated variable in $\ssa{c}$ in the assigned variables $\avar$, and $i_2$ points to the first new variable that is not in $\ssa{c}$ (if exists). 
%
%
\jl{
\begin{defn}[Valid Gamma (Remove?)]
$\Gamma \vDash i_1$ iff $\forall i \geq i_1, \Gamma(i_1)=0 $.  
\end{defn}
}
%%
%
\framebox{$ {\Gamma} \vdash^{i_1, i_2}_{\Mtrix, \flag} ~ c $}
\begin{mathpar}
\inferrule
{\Mtrix = \lMtrix_i * ( \rMtrix_{\ssa{\expr},i} + \Gamma )
}
{
 \ad{\Gamma;[\assign {\ssa{x}}{\ssa{\expr}} ]^{l}; i }{\Mtrix; \flag_{0}; i+1 }
}
~\textbf{\graphGen-asgn}
\and
{
\inferrule
{\Mtrix = \lMtrix_i * ( \rMtrix_{\ssa{\expr},i} + \Gamma )
\\
\flag = \lMtrix_i \and \flag(i) = 1
}
{ 
\ad{\Gamma;[ \assign{\ssa{x}}{\query(\ssa{\expr})} ]^{l} ; i }
{\Mtrix;\flag;i+1}
}~\textbf{\graphGen-query}}
%
\and 
%
{
\inferrule
{
{\ad{\Gamma + \rMtrix_{\ssa{\bexpr}, i_1}; \ssa{c_1} ; i_1 }{ \Mtrix_1;\flag_1;i_2 }}
\and 
{\ad{\Gamma + \rMtrix_{\ssa{\bexpr}, i_1};\ssa{c_2} ; i_2 }{ \Mtrix_2; \flag_2 ;i_3}}
\\
{\ad{\Gamma; [ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]; i_3 }{ M_x; \flag_{\emptyset}; i_3+|\bar{\ssa{x}}| }}
%
\\
%
{\ad{\Gamma; [ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}]; i_3+|\bar{\ssa{x}}| }{ \Mtrix_y; \flag_{\emptyset}; i_3+|\bar{\ssa{x}}|+|\bar{\ssa{y}}| }}
%
\\
%
{\ad{\Gamma; [ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}]; i_3+|\bar{\ssa{x}}|+ |\bar{\ssa{y}}|}{ \Mtrix_y; \flag_{\emptyset}; i_3+|\bar{\ssa{x}}|+|\bar{\ssa{y}}| + |\bar{\ssa{z}}| }}
\\
{\Mtrix = (\Mtrix_1 + \Mtrix_2)+ \Mtrix_x+ \Mtrix_y + \Mtrix_z }
}
{
\ad{\Gamma ; \eif([\ssa{\bexpr}]^{l},[ \bar{\ssa{x}}, \bar{\ssa{x_1}},
\bar{\ssa{x_2}}] ,[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}], 
[ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}],
\ssa{ c_1, c_2)} ; i_1}{ \Mtrix ; \flag_1 \uplus \flag_2 \uplus 2  ; i_3+|\bar{x}|+|\bar{y}|+|\bar{z}| }
}
~\textbf{\graphGen-if}
}
%
%
%
\and 
%
\inferrule
{
{\ad{\Gamma; \ssa{c_1} ; i_1 }{ \Mtrix_1 ; \flag_1; i_2 }  }
\and 
{
\ad{\Gamma;\ssa{c_2}; i_2}{ \Mtrix_2; \flag_2 ;i_3 }}
}
{
\ad{\Gamma ; (\ssa{c_1 ; c_2} ) ; i_1}{( \Mtrix_1 {;} \Mtrix_2) ; \flag_1 \uplus V_2 ; i_3  }
}
~\textbf{\graphGen-seq}
%
\and 
%
\and 
%
{ 
\inferrule
{
B= |\ssa{\bar{x}}| \and {A = |\ssa{c}|}
\\
{\ad{\Gamma;[\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]; i+ (B+A) }{ \Mtrix_{1};V_{1}; i+B+(B+A) }}
\\
{
\ad{\Gamma;\ssa{c} ; i+B+(B+A)  }{ \Mtrix_{2}; \flag_{2}; i+B+A+(B+A) }
}
\\
{
\ad{\Gamma ; [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] ; i+(B+A) }{ \Mtrix; \flag ;i+(B+A)+B}
}
\\
{ \Mtrix' = \Mtrix + ( \Mtrix_{1} + \Mtrix_{2}) }
\and
{
\flag' = \flag \uplus (( \flag_{1} \uplus \flag_{2}) \uplus 2)  }
}
{
\ad{\Gamma;
\ewhile ~ [ b ]^{l} ~ \ssa{n} ~
[\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] 
~ \edo ~  c;
i }{ \Mtrix'; \flag' ;i+(B+A)+B }
}~\textbf{\graphGen-while}
}
\end{mathpar}
\jl{Updated Flow Generation Algorithm}
\jl{
\framebox{$ {\Gamma} 
\vdash_{\Mtrix, \avar} ~ \ssa{c} $}
\begin{mathpar}
\inferrule
{
\ssa{x} = \avar(i)
\and 
\Mtrix = \lMtrix_i * ( \rMtrix_{\ssa{\expr}} + \Gamma )
}
{
\ad{\Gamma; [\assign {\ssa{x}}{\ssa{\expr}} ]^{l}; \avar}
 {\Mtrix}
}
~\textbf{\graphGen-asgn}
\and
{
\inferrule
{
\ssa{x} = \avar(i)
\and 
\Mtrix = \lMtrix_i * ( \rMtrix_{\ssa{\expr}} + \Gamma )
}
{ 
\ad{\Gamma;[ \assign{\ssa{x}}{\query(\ssa{\qexpr})} ]^{l} ; \avar }
{\Mtrix}
}~\textbf{\graphGen-query}}
%
\and 
%
{
\inferrule
{
{\ad{\Gamma + \rMtrix_{\ssa{\bexpr}}; \ssa{c_1} ; \avar }{ \Mtrix_1}}
\and 
{\ad{\Gamma + \rMtrix_{\ssa{\bexpr}}; \ssa{c_2}; \avar }{ \Mtrix_2}}
\\
\ad{\Gamma; [ \bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]; \avar }{ \Mtrix_x}
%
\\
%
\ad{\Gamma; [ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}]; \avar }{ \Mtrix_y}
%
\\
%
\ad{\Gamma; [ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}]; \avar }{ \Mtrix_z}
\\
{\Mtrix = (\Mtrix_1 + \Mtrix_2)+ \Mtrix_x+ \Mtrix_y + \Mtrix_z }
}
{
\ad{\Gamma ; \eif([\ssa{\bexpr}]^{l},[ \bar{\ssa{x}}, \bar{\ssa{x_1}},
\bar{\ssa{x_2}}] ,[ \bar{\ssa{y}}, \bar{\ssa{y_1}}, \bar{\ssa{y_2}}], 
[ \bar{\ssa{z}}, \bar{\ssa{z_1}}, \bar{\ssa{z_2}}],
\ssa{ c_1, c_2)}}
{ \Mtrix }
}
~\textbf{\graphGen-if}
}
%
%
%
\and 
%
\inferrule
{
{\ad{\Gamma; \ssa{c_1}; \avar }{ \Mtrix_1}  }
\and 
{
\ad{\Gamma;\ssa{c_2}; \avar}{ \Mtrix_2}}
}
{
\ad{\Gamma ; (\ssa{c_1 ; c_2} ); \avar}
{( \Mtrix_1 {;} \Mtrix_2) }
}
~\textbf{\graphGen-seq}
%
\and 
%
\and 
%
{ 
\inferrule
{
{
\ad{\Gamma + \rMtrix_{\ssa{\bexpr}};\ssa{c}; \avar  }{ \Mtrix_{1}}
}
\\
{
\ad{\Gamma ; [\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}]; \avar }{\Mtrix_2}
}
% \and
% { \Mtrix' = \Mtrix + ( \Mtrix_{1} + \Mtrix_{2}) }
}
{
\ad{\Gamma;
\ewhile [ \sbexpr ]^{l},\ssa{n},
[\bar{\ssa{x}}, \bar{\ssa{x_1}}, \bar{\ssa{x_2}}] 
\edo  \ssa{c}; \avar }
{ \Mtrix_{1} + \Mtrix_{2}}
}~\textbf{\graphGen-while}
}
\end{mathpar}
}
%
Below we define the valid data flow matrix, to have a clear understanding on the data flow generating algorithm:
\begin{defn}[Valid Matrix]
For a assigned variables $\avar$, $\avar \vDash (\Mtrix,\flag)$ iff the cardinality of $\avar$ equals to the one of $\flag$, $|\avar| = |\flag|$ 
and the matrix $\Mtrix$ is of size $|\flag| \times |\flag|$.
\end{defn}
\jl{
\begin{defn}[Valid Matrix]
Given a program $\ssa{c}$ with its assigned variables $\avar$, 
$\avar \vDash \Mtrix$ iff the cardinality of $\Mtrix$ equals to the product of  $\avar$'s cardinality,
i.e., $|\Mtrix| = |\avar| \times |\avar|$.
\end{defn}
}%
%
%
\paragraph{Reachability Bounds}
Given a program $c$ with its assigned variables $\avar$,
we use the $\rb(\ssa{x}, \ssa{c})$ algorithm, from paper \cite{10.1145/1806596.1806630}, to estimate the reachability bound for each variable $\ssa{x} \in \avar$. 
The input of $\rb$ is a program $\ssa{c}$ in SSA language and a variable $\ssa{x} $ from $\ssa{c}$.
The output of $\rb(\ssa{x}, \ssa{c})$ is an integer representing the reachability bound of $\ssa{x}$ in $\ssa{c}$.
%

%
The following example programs $\ssa{c}2$ and $\ssa{c}3$ with while loop illustrate how the algorithm works.
The collected assigned variables, $\avar_{\ssa{c}2}$ and $\avar_{\ssa{c}3}$,
data flow matrix $\Mtrix_{\ssa{c}2}$ and  $\Mtrix_{\ssa{c}3}$
and variable flags $\flag_{\ssa{c}2}$ and $\flag_{\ssa{c}3}$
for program $\ssa{c}2$ and $\ssa{c}3$
are presented in the right hand side.
%
\[
{\ssa{c}2 \triangleq
\begin{array}{l}
    \left[\ssa{ x_1} \leftarrow \query(1)  \right]^1 ; 
    \\
    \left[\ssa{i_1} \leftarrow 0 \right]^2 ; 
    \\
    \ewhile
    ~ [\ssa{i_1} < 2]^3
  	\\
    ~\ssa{[ x_3,x_1 ,x_2 ], [i_3, i_1, i_2] }
    ~ \edo 
    \\
    ~ \Big( 
    \left[\ssa{y}_1 \leftarrow \query(2) \right]^4;
    \\
    \left[\ssa{x_2 \leftarrow y_1  + x_3 } \right]^5;
    \\
    \left[\ssa{i_2 \leftarrow 1  + i_3 } \right]^6
    \Big) ; 
    \\
    \left[ \ssa{\assign{z_1}{x_3}} + 2  \right]^{7}
\end{array}
,
~~~~
\avar_{\ssa{c}2} = \left [ \begin{matrix}
\ssa{x}_1 \\
\ssa{x}_3 \\
\ssa{y}_1 \\
\ssa{x}_2 \\
\ssa{z}_1 \\
\ssa{i}_1 \\
\ssa{i}_2 \\
\ssa{i}_3 
\end{matrix} \right ]
% \Mtrix =  \left[ \begin{matrix}
%  & (x_1)  & (y_1) & (x_2) & (x_3) &  (z_1) & i_1 & i_2 & i_3\\
% (x_1) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
% (y_1) & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
% (x_2) & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 1 \\
% (x_3) & 1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
% (z_1) & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
% (i_1) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
% (i_2) & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
% (i_3) & 1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
% \end{matrix} \right]
,
~~~~~~
\Mtrix_{\ssa{c}2} =  \left[ \begin{matrix}
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
 0 & 1 & 0 & 1 & 0 & 1 & 1 & 1 \\
 1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
 1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
\end{matrix} \right]
,
~~~~
\flag_{\ssa{c}2} = \left [ \begin{matrix}
 1 \\
 2 \\
 1 \\
 2 \\
 0 \\
 0 \\
 2 \\
 1 
\end{matrix} \right ]
}
\]
%
%
\[
{{\ssa{c}3}  \triangleq
\begin{array}{l}
    \left[\ssa{ x}_1 \leftarrow \query(1)  \right]^1 ;
    \\
    \left[\ssa{i_1} \leftarrow 1 \right]^2 ; 
    \\
    \ewhile ~ [i < 0]^{3} ,
    \\
    ~\ssa{[ x_3,x_1 ,x_2 ], [i_3, i_1, i_2] }
    ~ \edo
    \\
    ~ \Big( 
    \left[\ssa{ y_1} \leftarrow \query(2) \right]^3; \\
    \left[\ssa{x_2 \leftarrow y_1  + x_3 } \right]^5
    \Big) ; \\
    \left[ \ssa{\assign{z_1}{x_3}} + 2  \right]^{6}
\end{array},
~~~~~~
\avar_{\ssa{c}3} = \left [ \begin{matrix}
\ssa{x}_1 \\
\ssa{i}_1 \\
\ssa{x}_3 \\
\ssa{i}_3 \\
\ssa{z}_1 \\
\end{matrix} \right ]
,~~~~~~
\Mtrix_{\ssa{c}3}  =  \left[ \begin{matrix}
 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 \\
 1 & 0 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 & 0 \\
 0 & 0 & 1 & 0 & 0 \\
\end{matrix} \right]
,~~~~~~
\flag_{\ssa{c}3} = \left [ \begin{matrix}
 1 \\
 0 \\
 2 \\
 2 \\
 0 \\
\end{matrix} \right ]
}
\]
%
We can now look at the if statement.
\[ 
%
 \ssa{c}4 \triangleq
\begin{array}{l}
   	\left[ \ssa{x}_1 \leftarrow \query(1) \right]^1; 
   	\\
   	\left[\ssa{y}_1 \leftarrow \query(2) \right]^2 ; 
   	\\
    \eif \;( \ssa{ x_1 + y_1 == 5} )^3,  \\
    \ssa{[ x_4,x_2,x_3 ],[] ,[y_3,y_1,y_2 ]} 
    \\
    \mathsf{then} ~ \left[ 
    \ssa{x}_2 \leftarrow \query(3) \right]^4 
    \\
    \mathsf{else} ~ \left[ 
    \ssa{x}_3 \leftarrow \query(4) \right]^5 ; 
    \\
    \ssa{y}_2 \leftarrow 2 ) \\
   \left[ \ssa{ z_1 \leftarrow x_4 +y_3 }\right]^6
\end{array},
% \]
% \[
~~~~~~
\avar_{\ssa{c}4} =  \left[ \begin{matrix}
\ssa{x}_1 \\
\ssa{y}_1 \\
\ssa{x}_2 \\
\ssa{x}_3 \\
\ssa{y}_2 \\
\ssa{x}_4 \\
\ssa{y}_3 \\
\ssa{z}_1 \\
\end{matrix} \right], 
~~~~~ 
\Mtrix_{\ssa{c}4} =  \left[ \begin{matrix}
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
\end{matrix} \right], 
~~~~~ 
\flag_{\ssa{c}4} = \left [ \begin{matrix}
 1 \\
 1 \\
 1 \\
 1 \\
 0 \\
 0 \\
 0 \\
 0 \\
\end{matrix} \right ]
\]
%
%
%
%
\subsection{Adaptivity Based on Program Analysis in \THESYSTEM}
%
 \begin{defn}
[Program-Based Dependency Graph].
\label{def:prog-based_graph}
\\
Given a program $\ssa{c}$ with its assigned variables $\avar$ of length $N$, s.t.,
$\Gamma \vdash_{\Mtrix_c, \flag_c} \ssa{c}$, 
its program-based graph 
$G(\ssa{c}) = (\vertxs, \edges, \weights, \flag)$ is. defined as:
\\
\[
\begin{array}{rlcl}
	\text{Vertices} &
	\vertxs & := & \left\{ 
	\ssa{x} \mid
	\ssa{x} = \avar(i); ~ i = 1, \ldots, N
	\right\}
	\\
	\text{Directed Edges} &
	\edges & := & 
	\left\{ 
	(\ssa{x_1}, \ssa{x_2}) 
	% \in \avar \times \avar 
	\mid
		(\ssa{x_1} = \avar(i) \land \ssa{x_2} = \avar(j) \land
		\Mtrix_c(i, j) \geq 1); ~ i,j = 1, \ldots, N
	\right\}
	\\
	\text{Weights} &
	\weights & := &
	\bigcup
	\begin{array}{l}
		\big\{ (v, w) \in \vertxs \times (\mathbb{N} \cup \expr)
		\mid
		v \in \vertxs \land \flag(v) > 0 \land w = \rb(v, c)
		\big\} 
		\\
		\big\{(v, 1)  \in \vertxs \times \{1\} 
		\mid
		v \in \vertxs \land \flag(v) = 0
		\big\}
	\end{array} 
	\\
	\text{Query Flags} &
	\qflag & := & 
	\big\{(\ssa{x}, n)  \in \vertxs \times \{0, 1\} 
	\mid 
	\left\{
	\begin{array} {ll}
	n = 1 & \flag_c(i) = 2
	\\  
	n = 0 & o.w.
	\end{array}
	\right\};
	\ssa{x} = \avar(i); i = 1, \ldots, N
	\big\}
\end{array}
\]
\end{defn} 
%
\begin{defn}[Finite Walk ($k$)].
\\
Given a labeled weighted graph $G = (\vertxs, \edges, \weights, \qflag)$, a \emph{finite walk} $k$ in $G$ is a sequence of edges $(e_1 \ldots e_{n - 1})$ 
for which there is a sequence of vertices $(v_1, \ldots, v_{n})$ such that:
\begin{itemize}
    \item $e_i = (v_{i},v_{i + 1})$ for every $1 \leq i < n$.
    \item every vertex $v \in \vertxs$ appears in this vertices sequence $(v_1, \ldots, v_{n})$ of $k$ at most $W(v)$ times.  
\end{itemize}
$(v_1, \ldots, v_{n})$ is the vertex sequence of this walk.
\\
\jl{
Length of this finite walk $k$ is the number of vertices in its vertex sequence, i.e., $\len(k) = n$.
}
\end{defn}

\jl{Given a labeled weighted graph $G = (\vertxs, \edges, \weights, \qflag)$, 
we use $\walks(G)$ to denote a set containing all finite walks $k$ in $G$;
and $k_{v_1 \to v_2} \in \walks(G)$where $v_1, v_2 \in \vertxs$ denotes the walk from vertex $v_1$ to $v_2$ .
}
% \begin{defn}[walk set ($\walks$)].
% \\
% Given a labeled weighted graph $G = (\vertxs, \edges, \weights, \flag)$, the walk set of $G$, ($\walks(G)$) is defined as a set containing all finite walks $k$ in $G$.
% \end{defn}
%
%
\begin{defn}[Length of Finite Walk w.r.t. Query ($\qlen$)].
\\
Given a labeled weighted graph $G = (\vertxs, \edges, \weights, \qflag)$ and a \emph{finite walk} $k$ in $G$ with its vertex sequence $(v_1, \ldots, v_{n})$, the length of $k$ w.r.t query is defined as:
\[
	\qlen(k) = \len\big(
	v \mid v \in (v_1, \ldots, v_{n}) \land \qflag(v) = 2 \big)
\]
, where $\big(v \mid v \in (v_1, \ldots, v_{n}) \land \qflag(v) = 2 \big)$ is a subsequence of $k$'s vertex sequence.
\end{defn}
%
Given a program $\ssa{c}$, we generate its program-based graph 
$\progG(\ssa{c}) = (\vertxs, \edges, \weights, \qflag)$.
%
Then the adaptivity bound based on program analysis for $\ssa{c}$ is the number of query vertices on a finite walk in $\progG(\ssa{c})$. This finite walk satisfies:
\begin{itemize}
\item the number of query vertices on this walk is maximum
\item the visiting times of each vertex $v$ on this walk is bound by its reachability bound $\weights(v)$.
\end{itemize}
It is formally defined in \ref{def:prog_adapt}.
%
%
\begin{defn}
[{Program-Based Adaptivity}].
\label{def:prog_adapt}
\\
{
Given a program $\ssa{c}$ and its program-based graph 
$\progG(\ssa{c}) = (\vertxs, \edges, \weights, \qflag)$,
%
the program-based adaptivity for $c$ is defined as%
\[
\progA(\ssa{c}) 
:= \max
\left\{ \qlen(k)\ \mid \  k\in \walks(\progG(\ssa{c}))\right \}.
\]
}
\end{defn}  
%
% By specifying the departure and destination vertices $s$ and $t$, the $\pathssearch(\progG, s, t)$ algorithm will 
% give the number of query vertices on a finite walk from $s$ to $t$, which contains the maximum number of query vertices.
% The pseudo-code of $\pathssearch(\progG, s, t)$ algorithm is defined in the Algorithm \ref{alg:adpt_alg}.
% %
% \begin{algorithm}
% \caption{
% {Walk Search Algorithm ($\pathssearch$)}
% \label{alg:adpt_alg}
% }
% \begin{algorithmic}
% \REQUIRE Weighted Directed Graph $G = (\vertxs, \edges, \weights, \flag)$ with a start vertex $s$ and destination vertex $t$ .
% \STATE  {\bf {bfs $(G, s, t)$}:}  
% \STATE \qquad {\bf init} 
% current node: $c = s$, 
% queue: $q = [c]$, 
% vector recoding if the vertex is visited: 
% visited$ = [0]*|\vertxs|$,
% result: $r$
% \STATE \qquad {\bf while} $q$ isn't empty:
% \STATE \qquad \qquad take the vertex from beginning $c= q.pop()$
% \STATE \qquad \qquad mark $c$ as visited, visited $[c] = 1$
% \STATE \qquad \qquad currMinFlow = min($\weights$(c), currMinFlow).
% \STATE \qquad \qquad put all unvisited vertex $v$ having directed edge from c into $q$. 
% \STATE \qquad \qquad if $v$ is visited, then there is a circle in the graph, we update the result $r = r + $currMinFlow
% \RETURN $r$
% \end{algorithmic}
% \end{algorithm}
%
%
\subsection{\todo{Soundness of the \THESYSTEM}}
\jl{
	\begin{thm}[Soundness of the \THESYSTEM].
	Given a program $\ssa{c}$, we have:
	%
	\[
	\progA(\ssa{c}) \geq A(\ssa{c}).
	\]
	\end{thm}
}
\jl{
\begin{proof}
Given a program $\ssa{c}$, 
we construct its program-based graph $\progG(\ssa{c}) = (\vertxs, \edges, \weights, \qflag)$
by Definition~\ref{def:prog-based_graph}
According to the Definition \ref{def:prog_adapt}, we have:
%
\[
	\progA(\ssa{c}) 
	:= \max\left\{ \qlen(k)\ \mid \  k\in \walks(\progG(\ssa{c}))\right \}.
\]
%
According to the Definition \ref{def:trace-based_adapt}, we have the trace-based adaptivity as follows:
$$
A(\ssa{c}) = \max \big 
\{ \len(p) \mid \ssa{m} \in \mathcal{SM},D \in \dbdom ,p \in \paths(\traceG(\ssa{c}, \text{D}, \ssa{m}) \big \} 
$$
%
Then, we need to show:
\[
\max \big 
\{ \len(p) \mid \ssa{m} \in \mathcal{SM},D \in \dbdom ,p \in \paths(\traceG(\ssa{c}, \text{D}, \ssa{m}) \big \} 
\leq
\max\left\{ \qlen(k) \ \mid \  k\in \walks(\progG(\ssa{c}))\right \}
\]
%
It is sufficient to show that:
\[
	\forall p, \ssa{m}, D, ~ s.t., ~ p \in \paths(\traceG(\ssa{c}, \text{D}, \ssa{m}),
	\exists k \in \walks(\progG(\ssa{c})) \land 
	\len(p) \leq \qlen(k)
\]
%
Taking an arbitrary starting memory $m$ and an arbitrary underlying database $D$,
we construct a trace-based graph $\traceG(\ssa{c}, \text{D}, \ssa{m}) = (\vertxs, \edges)$ by the definition \ref{def:trace-based_graph}.
%
\\
%
Let $\midG(\ssa{c},\ssa{m},\text{D}) = \{\midV, \midE, \midF\}$ be the intermediate graph by Definition~\ref{def:midgraph}.
\\
By Lemma~\ref{lem:bie_trace_to_mid}, we know:
\[
	\forall p, \ssa{m}, D, ~ s.t., ~ p \in \paths(\traceG(\ssa{c}, \text{D}, \ssa{m}),
	\exists p' \in \paths(\midG(\ssa{c},\ssa{m},\text{D})) \land 
	\len(p) = \len_q(p')
\]
%
Then it is sufficient to show that:
%
\[
	\forall p, \ssa{m}, D, ~ s.t., ~ p \in \paths(\midG(\ssa{c}, \text{D}, \ssa{m}),
	\exists k \in \walks(\progG(\ssa{c})) \land 
	\qlen(p) \leq \qlen(k)
\]
%
We prove a stronger statement instead:
\[
	\forall p, \ssa{m}, D, ~ s.t., ~ p \in \paths(\midG(\ssa{c}, \text{D}, \ssa{m}),
	\exists k \in \walks(\progG(\ssa{c})) \land 
	\qlen(p) = \qlen(k)	
\]
%
%
By Lemma~\ref{lem:sujv_mid_to_prog}, let $g$ be the surjective function $g: \progV \to \midV$ s.t.:
%
$$
\forall \av \in \midV. ~ \progF(f(\av)) = \midF(\av) 
\land |\kw{image}(f(\av))| \leq W(f(\av)).
$$
%
%
% \item(1) $\len(p_{\av_1 \to \av_2}) = \len(k_{f(\av_1) \to f(\av_2)})$
% %
% \item(2) $\forall \av \in p_{\av_1 \to \av_2}. ~ f(\av) \in k_{f(\av_1) \to f(\av_2)}$
% %
% \item(3) $\forall \av \in p_{\av_1 \to \av_2}. ~ 
% \kw{image}(f(\av)) \cap {p_{\av_1 \to \av_2}}| = \# \{f(\av) \mid f(\av) \in k_{f(\av_1) \to f(\av_2)}\}$
%
Let $\ssa{m}$ and $D$ be an arbitrary memory and database $D$,
taking an arbitrary path $p_{\av_1 \to \av_n} \in \paths(\midG(\ssa{c}, \text{D}, \ssa{m})$ with:
%
\item Edge sequence: $(e, \ldots, e_{n-1})$
%
\item Vertices sequence: $(\av_1, \ldots, \av_n)$.
\\
By Lemma~\ref{lem:sujpathwalk_mid_to_prog}, let $h: \paths(\midG(\ssa{c}, \text{D}, \ssa{m})) \to \walks(\progG(\ssa{c}))$ be the surjective function satisfies:
%
\[
	\forall p_{\av_1 \to \av_n} \in \paths(\midG(\ssa{c}, \text{D}, \ssa{m}))
	\text{ with }
	\left\{
	\begin{array}{ll}
	\mbox{edge sequence:} & (e, \ldots, e_{n-1})
	\\ 
	\mbox{vertices sequence:} & (\av_1, \ldots, \av_n)
	\end{array}
	\right.
\]
%
\[
	\exists k_{f(\av_1) \to f(\av_n)} \in \walks(\progG(\ssa{c}))
	\text{ with }
	\left\{
	\begin{array}{ll}
	\mbox{edge sequence:} & (g(e), \ldots, g(e_{n-1}) 
	\\ 
	\mbox{vertices sequence:} & (f(\av_1), \ldots, f(\av_{n}))
	\end{array}
	\right.
\]
%
We have the walk:
$k_{f(\av_1) \to f(\av_n)} \in \walks(\progG(\ssa{c}))$ with:
%
\item Edges sequence: $(g(e), \ldots, g(e_{n-1}) $
%
\item Vertices sequence: $(f(\av_1), \ldots, f(\av_{n}))$.
\\
It is sufficient to show 
%
\[
	\qlen(p_{\av_1 \to \av_n}) = \qlen(k_{f(\av_1) \to f(\av_n)})
\]
%
Unfold the definition of $\qlen$, it is suffice to show:
\[
\len \big( \av \mid \av \in (\av_1, \ldots, \av_n) \land \midF(\av) = 2 \big) 
= \len \big(f(\av) \mid f(\av) \in (f(\av_1), \ldots, f(\av_{n})) \land \progF(f(\av)\big) = 2)	
~ (a)
\]
%
By Lemma~\ref{lem:sujv_mid_to_prog}, we know:
%
\[
	\forall \av \in \midV. ~ \midF(\av) = \progF(f(\av)) ~(b)
\]
By rewriting $(b)$ in $(a)$, we have this case proved.
%
\\
\todo{
\begin{defn}[Intermediate Graph $\midG$].
	\label{def:midgraph}
	\\
	$\mathcal{AV}$ : Annotated Variables based on program execution
	\\
	Given a program $\ssa{c}$ with its assigned variables $\avar$ of length $N$,
	a database $D$, a starting memory $\ssa{m}$,
	s.t., $\Gamma \vdash_{\Mtrix_c, \flag_c} \ssa{c}$,
	the intermediate graph 
	$\midG(\ssa{c},\ssa{m},\text{D}) = (\vertxs, \edges, \flag)$ is defined as:%
\[
\begin{array}{rlcl}
	\text{Vertices} &
	\vertxs & := & \left\{ 
	\av \in \mathcal{AV} \middle\vert
	\exists \ssa{m'},  w', \qtrace, \vtrace.  ~ s.t., ~  
	\config{\ssa{m} ,\ssa{c}, [], [], []}  \to^{*}  \config{\ssa{m'} , \eskip, \qtrace, \vtrace, w' }
	\land \av \in \vtrace
	\right\}
	\\
	\text{Directed Edges} &
	\edges & := & 
	\left\{ 
	(\av, \av') \in \mathcal{AV} \times \mathcal{AV} 
	~ \middle\vert ~
	\flowsto(\av, \av', \ssa{c},\ssa{m},D) 
	\right\}
	\\
	\text{Flags} &
	\flag & := & 
	\big\{ (\av, n)  \in \vertxs \times \{0, 1, 2\} 
	\mid 
	(\pi_1(\av) = \avar(i) \land n = \flag_c(i)); ~
	i = 1, \ldots, N
	\big\}
\end{array}
\]
\end{defn}
}
%
\\
\todo{
	\begin{lem}[$\vardep$ is Transitive].
	\label{lem:vardep_trans}
	\\
	Given a program $\ssa{c}$, with a starting memory $\ssa{m}$ and a hidden database $D$, s.t., 
	$\config{\ssa{m}, \ssa{c}, [], [], []} \rightarrow^{*} \config{\ssa{m}', \eskip, \qtrace, \vtrace, w} $.
	Then, $\forall \av_1, \av_2, \av_3 \in \vtrace$:
\[
	\Big(\vardep(\av_1, \av_2, \ssa{c}, \ssa{m}, D) \land 
	\vardep(\av_2, \av_3, \ssa{c}, \ssa{m}, D) \Big)
	\implies
	\vardep(\av_1, \av_3, \ssa{c}, \ssa{m}, D)
\]
	\end{lem}
	\begin{subproof}[of Lemma~\ref{lem:vardep_trans}]
	Proof by unfolding and rewriting the Definition~\ref{def:var_dep}.
	\end{subproof}
}
\\
%
\todo{
	\begin{lem}[$\flowsto$ is Transitive ??].
	\label{lem:flowsto_trans}
	\\
	Given a program $\ssa{c}$ with its assigned variables $\avar$ of length $N$. 
	Then $\forall x_1, x_2, x_3 \in \avar$
\[
	\Big(\flowsto(x_1, x_2) \land \flowsto(x_2, x_3) \Big)
	\implies
	\flowsto(x_1, x_3)
\]
	\end{lem}
	\begin{subproof}[of Lemma~\ref{lem:flowsto_trans}]
	Proof by unfolding the Definition~\ref{def:flowsto}.
	\end{subproof}
}
\\
%
\todo{
	\begin{lem}[$\qdep$ Implies $\vardep$].
	\label{lem:querydep_vardep}
	\\
	Given a program $\ssa{c}$, with a starting memory $\ssa{m}$ and a hidden database $D$, s.t., 
	$\config{\ssa{m}, \ssa{c}, [], [], []} \rightarrow^{*} \config{\ssa{m}', \eskip, \qtrace, \vtrace, w} $.
	Then, $\forall \av_1, \av_2 \in \qtrace$
\[
	\qdep(\av_1, \av_2, \ssa{c}, \ssa{m}, D) \implies 
	\vardep(\pi_2(\av_1), \pi_2(\av_2), \ssa{c}, \ssa{m}, D)
\]
	\end{lem}
	\begin{subproof}[of Lemma~\ref{lem:querydep_vardep}]
	Proof by unfolding the Definition~\ref{def:var_dep} and Definition~\ref{def:query_dep}.
	\end{subproof}
}
\\
%
\todo{
	\begin{lem}[$\vardep$ Implies \flowsto].
	\label{lem:vardep_flows}
	\\
	Given a program $\ssa{c}$, with a starting memory $\ssa{m}$ and a hidden database $D$, s.t., 
	$\config{\ssa{m}, \ssa{c}, [], [], []} \rightarrow^{*} \config{\ssa{m}', \eskip, \qtrace, \vtrace, w} $.
	Then, $\forall \av_1, \av_2 \in \vtrace$
\[
	\vardep(\av_1, \av_2, \ssa{c}, \ssa{m}, D) \implies 
	\flowsto(\pi_1(\av_1), \pi_1(\av_2))
\]
	\end{lem}
	\begin{subproof}[of Lemma~\ref{lem:querydep_vardep}]
	Proof by showing contradiction based on the Definition~\ref{def:var_dep} and Definition~\ref{def:flowsto}.
	Let $\av_1, \av_2 \in \vtrace$ be 2 arbitrary annotated variables in the variable trace $\vtrace$,
	s.t., $\vardep(\av_1, \av_2, \ssa{c}, \ssa{m}, D)$.
	\\
	Unfolding the $\vardep$ definition, we have:	
	\end{subproof}
}
\\
%
\todo{
	\begin{lem}[Injective Mapping of vertices from $\traceG$ to $\midG$].
	\label{lem:injv_trace_to_mid}
	\\
	$\traceG(\ssa{c}) = \{\traceV, \traceE\}$
	\\
	$\midG(\ssa{c},\ssa{m},\text{D}) = \{\midV, \midE, \midF\}$
\[
	\exists ~ \kw{injective} ~ f: \mathcal{AQ} \to \mathcal{AV}. 
	~ \forall \av \in \traceV. ~ 
	f(\av) \in \midV \land \midF(f(\av)) = 2
\]
	\end{lem}
\begin{subproof}
Proving by Definition~\ref{def:midgraph} and Definition~\ref{def:prog_adapt}.
\end{subproof}
}
\\
\todo{
	\begin{lem}[One-on-One Mapping from $\edges$ of $\traceG$ to $\paths(\midG)$].
	\label{lem:bie_trace_to_mid}
	\\
	$\traceG(\ssa{c}) = \{\traceV, \traceE\}$
	\\
	$\midG(\ssa{c},\ssa{m},\text{D}) = \{\midV, \midE, \midF\}$
	\\
	An injective function $ f: \traceV \to \midV$ s.t.,
	$\forall \av \in \traceV. ~ \midF(f(\av)) = 2$ 
\[
	\forall e = (\av_1, \av_2) \in \traceE. ~ 
	\exists p_{f(\av_1) \to f(\av_2)} \in \paths(\midG(\ssa{c}, \text{D}, \ssa{m}))
\]
	\end{lem}
\begin{subproof}
Proving by Lemma~\ref{lem:injv_trace_to_mid} and Definition~\ref{def:midgraph} and acyclic property of $\traceG$ and $\midG$.
\end{subproof}
}
\\
\todo{
	\begin{lem}[Surjective Mapping of Vertices from $\midG$ to $\progG$].
	\label{lem:sujv_mid_to_prog}
	\\
	$\midG(\ssa{c},\ssa{m},\text{D}) = \{\midV, \midE, \midF\}$
	\\
	$\progG(\ssa{c}) = \{\progV, \progE, \progF, \progW\}$
	\\
	$\exists ~ \kw{surjective} ~ f: \mathcal{AV} \to \mathcal{SVAR}.$
	%
\[
	\forall \av \in \midV. ~ 
	f(\av) \in \progV \land \progF(f(\av)) = \midF(\av) \land
	|\kw{image}(f(\av))| \leq W(f(\av))
\]
\end{lem}
\begin{subproof}
Proving by Definition~\ref{def:midgraph}.
\end{subproof}
}
\\
\todo{
	\begin{lem}[Surjective Mapping from $\edges$ of $\midG)$ to $\edges$ of $\progG$].
	\label{lem:suje_mid_to_prog}
	\\
	$\midG(\ssa{c},\ssa{m},\text{D}) = \{\midV, \midE, \midF\}$
	\\
	$\progG(\ssa{c}) = \{\progV, \progE, \progF, \progW\}$
	\\
	A surjective function $f: \progV \to \midV$ s.t.,
	$\forall \av \in \midV. ~ \progF(f(\av)) = \midF(\av) \land |\kw{image}(f(\av))| \leq W(f(\av))$
	%
\[
	\exists ~ \kw{surjective} ~ g: \midE \to \progE. ~
	\forall e_{mid} = (\av_1, \av_2) \in \midE. 
	\exists e_{prog} = ({f(\av_1), f(\av_2)}) \in \progE
\]
\end{lem}
\begin{subproof}
Proving by Lemma~\ref{lem:sujv_mid_to_prog}.
\end{subproof}
}
\\
\todo{
	\begin{lem}[Surjective Mapping from $\paths(\midG)$ to $\walks(\progG)$].
	\label{lem:sujpathwalk_mid_to_prog}
	\\
	$\midG(\ssa{c},\ssa{m},\text{D}) = \{\midV, \midE, \midF\}$
	\\
	$\progG(\ssa{c}) = \{\progV, \progE, \progF, \progW\}$
	\\
	A surjective function $f: \progV \to \midV$ s.t.,
	$\forall \av \in \midV. ~ \progF(f(\av)) = \midF(\av) \land |\kw{image}(f(\av))| \leq W(f(\av))$
	\\
	A surjective function $g: \midE \to \progE$ s.t.,
	$\forall e_{mid} = (\av_1, \av_2) \in \midE. 
	\exists e_{prog} = ({f(\av_1) \to f(\av_2)}) \in \progE$
	\\
	$\exists ~ \kw{surjective} ~ h: \paths(\midG(\ssa{c},\ssa{m},\text{D})) \to \walks(\progG(\ssa{c}))$ s.t.:
	%
\[
	\forall p_{\av_1 \to \av_2} \in \paths(\midG(\ssa{c},\ssa{m},\text{D}))
	\text{ with }
	\left\{
	\begin{array}{ll}
	\mbox{edge sequence:} & (e, \ldots, e_{n-1})
	\\ 
	\mbox{vertices sequence:} & (\av_1, \ldots, \av_n)
	\end{array}
	\right.
\]
\[
	\exists k_{f(\av_1) \to f(\av_2)} \in \walks(\progG(\ssa{c}))
	\text{ with }
	\left\{
	\begin{array}{ll}
	\mbox{edge sequence:} & (g(e), \ldots, g(e_{n-1}) 
	\\ 
	\mbox{vertices sequence:} & (f(\av_1), \ldots, f(\av_{n}))
	\end{array}
	\right.
\]
% \item $(e, \ldots, e_{n-1})$, $(\av_1, \ldots, \av_n)$ are the edges sequence and vertices sequence of $p_{\av_1 \to \av_2}$.
% then, 
%  $\len(p_{\av_1 \to \av_2}) = \len(k_{f(\av_1) \to f(\av_2)})$
% %
% \item $\forall \av \in p_{\av_1 \to \av_2}. ~ f(\av) \in k_{f(\av_1) \to f(\av_2)}$
% %
% \item $\forall \av \in p_{\av_1 \to \av_2}. ~ 
% \kw{image}(f(\av)) \cap {p_{\av_1 \to \av_2}}| = \# \{f(\av) \mid f(\av) \in k_{f(\av_1) \to f(\av_2)}\}
% $
\end{lem}
%
\begin{subproof}
Proving by induction on the length of $l = p_{\av_1 \to \av_2} \in \paths(\midG(\ssa{c},\ssa{m},\text{D}))$, and Lemma~\ref{lem:suje_mid_to_prog} and Lemma~\ref{lem:sujv_mid_to_prog}.
\caseL{ $l = 1$: }
\caseL{ $l = l' + 1$, $l' \geq 1$: }
\end{subproof}
}
\end{proof}
%
%
% \begin{lem}
% [Surjective Mapping of Vertices from $\traceG$ to $\progG$ Graph]
% \label{lem:vertexmap}
% \[
% 	\forall \av \in \vertxs. \exists f: \mathcal{AQ} \to \mathcal{SVAR}. 
% 	~ f(\av) \in \vertxs_{prog} \land \flag(f(\av)) = 2
% 	\land |image(f(\av))| \leq \weights(f(\av)) 
% \]
% \end{lem}
% %
% \begin{lem}
% [Mapping from Edges of $\traceG$ to Walks of $\progG$]
% \label{lem:edgewalkmap}
% \[
% 	\forall e = (\av_1, \av_2) \in \edges. \exists k \in \walks(\progG(\ssa{c})). ~ from ~ f(\av_1) ~to~ f(\av_2)
% \]
% \end{lem}
%
}
\section{\todo{Examples}}

\begin{example}[TwoRound Algorithm]
\[
{
TR(k) \triangleq
{
\begin{array}{l}
    \left[i \leftarrow 1 \right]^1 ; \\
    \left[a_1 \leftarrow [] \right]^2; \\
   \ewhile ~ [i < k]^{3}, 0,     ~ \edo ~ \\
    \Big(
     \clabel{x \leftarrow \query() }^4 ; \\
    \clabel{a \leftarrow x :: a }^5  
        \left[i_2 \leftarrow i_3 + 1 \right]^6 
   \Big);\\
    \clabel{l \leftarrow q_{k + 1}(a)}^{7}\\
\end{array}
}
%
~~~~~~~~ \Rightarrow ~~~~~~~
%
TR^{ssa} \triangleq
\begin{array}{l}
    \left[i \leftarrow 1 \right]^1 ; \\
    \left[a_1 \leftarrow [] \right]^2; \\
   \ewhile ~ [i < k]^{3}, 0, 
   [ a_3,a_1,a_2 ] [ i_3,i_1,i_2 ] ~ 
    ~ \edo ~ \\
   \Big( 
     \left[x_1 \leftarrow q \right]^4; \\
    \left[a_2 \leftarrow x_1 :: a_3 \right]^5 
    \left[i_2 \leftarrow i_3 + 1 \right]^6 
    \Big);\\
    \clabel{l \leftarrow q_{k + 1}(a_3)}^{7}\\
\end{array}
}
\]
% %
Adapt($TR$) = 2

{
Using \THESYSTEM, we first generate a assigned variables $G$ from an empty list $[]$ and empty while map $[]$.
 \[[]; []; TR^{ssa} \to G; w  \land w = []\].
 %
 \[G_{k=2} = \left[
  {a_1}^2 , {a_3}^{(2,[2:1])} , x_1^{4} , a_2^{5} ,  i_3^{3} , 
  i_2^{6}, i_1^{2} , l_1^{7} , {l_1}^{(5, [])}   \right] \]
  %
  We denote $a_1^{1}$ short for ${a_1}^{(1, [])}$ and ${a_3}^{(2,1)}$ short for ${a_3}^{(2,[2:1])}$, where the label $(2, 1)$ represents at line number $2$ and in the $1$ st iteration.
  } 
\[
{
M =  \left[ \begin{matrix}
 & a_1^{2} & a_3^{3} & x_1^{4} 
 & a_2^{5}  & i_3^{3} & i_2^{6} & i_1^{2} & l_1^{7}\\
a_1^{2} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
a_3^{3} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
x_1^{4} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
a_2^{5} & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
i_3^{3} & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
i_2^{6} & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
i_1^{2} & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
l_1^{7} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
 \end{matrix} \right] 
~ , V = \left [ \begin{matrix}
a_1^{2} & 2   \\
a_3^{3} & 2  \\
x_1^{4} & 1  \\
a_2^{5} & 2  \\
i_3^{3} & 2  \\
i_2^{6} & 2  \\
i_1^{2} & 2  \\
l_1^{7} & 1 \\
\end{matrix} \right ]
}
\]
\[
{
M =  \left[ \begin{matrix}
 & a_1^{1} & a_3^{(2,1)} & x_1^{(3,1)} & a_2^{(4,1)}  & a_3^{(2,2)} & x_1^{(3,2)} & a_2^{(4,2)} & a_3^{2} & l_1^{5}\\
 a_1^{1} & 0 & 0 & 0 & 0 & 0 & 0 & 0 &0 &0 \\
a_3^{(2,1)} & 1 & 0 & 0 & 0 & 0 & 0 & 0&0&0\\
x_1^{(3,1)} & 0 & 0 & 0 & 0 & 0 & 0& 0& 0 &0\\
a_2^{(4,1)} & 0 & 1 & 1 & 0 & 0 & 0 & 0& 0&0\\
a_3^{(2,2)} & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0&0 \\
x_1^{(3,2)} & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0&0\\
a_2^{(4,2)} & 0 & 0 & 0 & 0 & 1 & 1 & 0& 0&0\\
a_3^{2} & 1 & 0 & 0 & 0 & 0 & 0 & 1& 0&0\\
l_1^{5} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 &0 \\
 \end{matrix} \right] 
~ , V = \left [ \begin{matrix}
a_1^{1} &  0 \\
a_3^{(2,1)} & 0 \\
x_1^{(3,1)} & 1 \\
a_2^{(4,1)} &  0 \\
a_3^{(2,2)} & 0 \\
x_1^{(3,2)} & 1 \\
a_2^{(4,2)} &  0 \\
a_3^{2} &  0 \\
l_1^{5} &  1 \\
\end{matrix} \right ]
}
\]
\newpage

\begin{center}

\todo{
	\begin{tikzpicture}
%%% The nodes represents the k query in the first round
\filldraw[black] (0, 2) circle (2pt) node [anchor=south]{$q_1^{(4, \{3 \to 1\} )}$};
\filldraw[black] (3, 2) circle (2pt) node [anchor=south]{$q_2^{(4, \{3 \to 2\} )}$};
% \filldraw[black] (6, 2) circle (2pt) node [anchor=south]{$q^4_3$};
\filldraw[black] (8, 2) circle (2pt) node [anchor=south]{$\cdots$};
\filldraw[black] (12, 2) circle (2pt) node [anchor=south]{$q_k^{(4, \{3 \to k\} )}$};
%%%%%% The nodes represents the n^k queries in the second round
\filldraw[black] (0, 0) circle (2pt) node [anchor=north]{$q_{k+1,1}^{(7, \emptyset)}$};
\filldraw[black] (3, 0) circle (2pt) node [anchor=north]{$q_{k+1,2}^{(7, \emptyset)}$};
% \filldraw[black] (6, 0) circle (2pt) node [anchor=north]{$q^{3, 7}_{k+1}$};
\filldraw[black] (8, 0) circle (2pt) node [anchor=north]{$\cdots$};
\filldraw[black] (12, 0) circle (2pt) node [anchor=north]{$q_{k+1,n^k}^{(7, \emptyset)}$};
%%%%%% The edges represents their dependency relations GROUP 1
\draw[ thick,->] (0, 0)  -- (0, 1.9) ;
\draw[ thick,->] (0, 0)  -- (2.9, 2) ;
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
\draw[ thick,->] (0, 0)  -- (7.9, 2) ;
\draw[ thick,->] (0, 0)  -- (11.9, 2) ;
%%%%%% The edges represents their dependency relations GROUP 2
\draw[ thick,->] (3, 0)  -- (0.1, 1.8) ;
\draw[ thick,->] (3, 0)  -- (3, 1.9) ;
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
\draw[ thick,->] (3, 0)  -- (7.95, 1.9) ;
\draw[ thick,->] (3, 0)  -- (11.95, 1.9) ;
%%%%%% The edges represents their dependency relations GROUP 3
\draw[ thick,->] (8, 0)  -- (0.1, 1.9) ;
\draw[ thick,->] (8, 0)  -- (3.1, 1.9) ;
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
\draw[ thick,->] (8, 0)  -- (8, 1.9) ;
\draw[ thick,->] (8, 0)  -- (12, 1.85) ;
%%%%%% The edges represents their dependency relations GROUP 4
\draw[ thick,->] (12, 0)  -- (0.1, 2) ;
\draw[ thick,->] (12, 0)  -- (3.1, 2) ;
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
\draw[ thick,->] (12, 0)  -- (8.1, 2) ;
\draw[ thick,->] (12, 0)  -- (12, 1.85) ;
%%%% The longest path representing the adaptivity
\draw[ultra thick, red, ->, dashed] (0.1, 0) -- (0.1, 1.9);
\end{tikzpicture}
}
\end{center}
\end{example}
%
\newpage

%
\begin{example}[Multi-Round Algorithm]
{
\[
MR \triangleq
\begin{array}{l}
    \left[i \leftarrow 1 \right]^1 ; \\
    \left[I \leftarrow [] \right]^2; \\
   \ewhile ~ [i < k]^{3} 
    \ ~ \edo ~ \\ \Big(
    \left[p \leftarrow c \right]^4 ; \\
    \left[a \leftarrow \query(p, I) \right]^5; \\
    \left[I \leftarrow \eupdt( {I}, (a, p))  \right]^6 ; \\
    \left[i \leftarrow i + 1 \right]^7 \\
    \Big) 
\end{array}
%
~~~~ \Rightarrow ~~~
%
MR^{ssa} \triangleq
\begin{array}{l}
    \left[i \leftarrow 1 \right]^1 ; \\
   \left[I \leftarrow [] \right]^2; \\
   \ewhile ~ [i < k]^{3} 0, [I_3,I_1,I_2] \\ 
    \ ~ \edo ~ \\ \Big(
    \left[p_1 \leftarrow c \right]^4 ; \\
    \left[a \leftarrow \query(p_1, I_2) \right]^5; \\
    \left[I_2 \leftarrow \eupdt( {I_3}, (a_1, p_1))  \right]^6;\\
    \left[i \leftarrow i + 1 \right]^7 \\
    \Big) 
\end{array}
\]
}
%
%
%
Adapt($MR$) = k.
\\
%
{
Using \THESYSTEM, we first generate a assigned variables $G$ from an empty list $[]$ and empty whlemap $\emptyset$.
 \[[]; \emptyset; MR^{ssa} \to G; w  \land w = \emptyset\].
 %
 \[
 G_{k=2} = 
 \left[
 i_1^{1}, I_1^2, i_3^3, I_3^3, p_1^4, a_1^5, I_2^6, i_2^7
\right] 
\]
  We denote $I_1^{1}$ short for ${I_1}^{(1,\emptyset)}$ and ${I_3}^{(2,1)}$ short for ${I_3}^{(2,[2:1])}$, where the label $(2, 1)$ represents at line number $2$ and in the $1$ st iteration.
  }
{
	\[
M =  \left[ \begin{matrix}
   i_1^{1} & I_1^2 & i_3^3 & I_3^3 & p_1^4 & a_1^5 & I_2^6 & i_2^7\\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
 \end{matrix} \right] 
~ , V = \left [ \begin{matrix}
i_1^1 & 0 \\
I_1^2 & 0 \\
i_3^3 & 2 \\
I_3^3 & 2 \\
p_1^4 & 2 \\
a_1^5 & 1 \\
I_2^6 & 2 \\
i_2^7 & 2
\end{matrix} \right ]
\]
}

\newpage
\begin{center}
%
\todo{
\begin{tikzpicture}
%%% The nodes represents the k query in the first round
\filldraw[black] (0, 4) circle (2pt) node [anchor=south]{$(q^{5, \{3 \to 1\}}_{1, 1}$};
\filldraw[black] (3, 0) circle (2pt) node [anchor=north]{$q^{5, \{3 \to 2\}}_{2, 3}$};
% \filldraw[black] (6, 0) circle (2pt) node [anchor=north]{$q^{3, 7}_{k+1}$};
% \filldraw[black] (8, 0) circle (2pt) node [anchor=north]{$\cdots$};
\filldraw[black] (12, 0) circle (2pt) node [anchor=north]{$q^{5, \{3 \to 4\}}_{4, 3}$};
\filldraw[black] (8, 2) circle (2pt) node [anchor=south]{$q^{5, (3 \to 3)}_{3, 2}$};
\draw[very thick,->, red] (3, 0)  -- (0, 3.8) ;
%
\draw[very thick,->, red] (8, 2)  -- (3.1, 0.1) ;
\draw[very thick,->] (8, 2)  -- (0.15, 4) ;
% \draw[very thick,->] (8, 0)  -- (3.1, 0) ;
% \draw[very thick,->] (8, 4)  -- (3, 0.2) ;
% %
%%%%%% The edges represents their dependency relations GROUP 4
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
% \draw[very thick,->] (12, 2)  -- (8.1, 2) ;
\draw[very thick,->, red] (12, 0)  -- (8.1, 1.9) ;
\draw[very thick,->] (12, 0)  -- (3.1, 0) ;
\draw[very thick,->] (12, 0)  -- (0.1, 3.9) ;
%
\end{tikzpicture}
}
\end{center}
%
\newpage
%
$\forall k. \forall D$, we have $A(TR^L) = (k - 1)$ given all possible execution traces.
\todo{
\begin{center}
%
\begin{tikzpicture}
%%% The nodes represents the k query in the first round
\filldraw[black] (0, 4) circle (2pt) node [anchor=south]{$q^{1, [(5, 1)]}_1$};
\filldraw[black] (3, 4) circle (2pt) node [anchor=south]{$q^{1, [(5, 2)]}_2$};
% \filldraw[black] (6, 2) circle (2pt) node [anchor=south]{$q^4_3$};
\filldraw[black] (8, 4) circle (2pt) node [anchor=south]{$\cdots$};
\filldraw[black] (12, 4) circle (2pt) node [anchor=south]{$q^{1, (5, k)}_k$};
%%%%%% The nodes represents the n^k queries in the second round
\filldraw[black] (0, 0) circle (2pt) node [anchor=north]{$q^{n!, (5, 1)}_1$};
\filldraw[black] (3, 0) circle (2pt) node [anchor=north]{$q^{n!, (5, 2)}_2$};
% \filldraw[black] (6, 0) circle (2pt) node [anchor=north]{$q^{3, 7}_{k+1}$};
\filldraw[black] (8, 0) circle (2pt) node [anchor=north]{$\cdots$};
\filldraw[black] (12, 0) circle (2pt) node [anchor=north]{$q^{n!, (5, k)}_k$};
%%% The nodes represents the k query in the first round
\filldraw[black] (0, 2) circle (2pt) node [anchor=south]{$q^{\cdots, (5, 1)}_1$};
\filldraw[black] (3, 2) circle (2pt) node [anchor=south]{$q^{\cdots, (5, 2)}_2$};
% \filldraw[black] (6, 2) circle (2pt) node [anchor=south]{$q^4_3$};
\filldraw[black] (8, 2) circle (2pt) node [anchor=south]{$\cdots$};
\filldraw[black] (12, 2) circle (2pt) node [anchor=south]{$q^{\cdots, (5, k)}_k$};
%%%%%% The edges represents their dependency relations GROUP 1
\draw[very thick,->] (3, 2)  -- (0.1, 2) ;
\draw[very thick,->] (3, 0)  -- (0.1, 1.9) ;
\draw[very thick,->] (3, 4)  -- (0.1, 2.1) ;
%
\draw[very thick,->] (3, 2)  -- (0.1, 0.1) ;
\draw[very thick,->] (3, 0)  -- (0.1, 0) ;
\draw[very thick,->] (3, 4)  -- (0, 0.2) ;
%
\draw[very thick,->] (3, 2)  -- (0.1, 3.9) ;
\draw[very thick,->] (3, 0)  -- (0, 3.8) ;
\draw[very thick,->] (3, 4)  -- (0, 4) ;
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
%%%%%% The edges represents their dependency relations GROUP 2
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
\draw[very thick,->] (8, 2)  -- (3.1, 2) ;
\draw[very thick,->] (8, 0)  -- (3.1, 1.9) ;
\draw[very thick,->] (8, 4)  -- (3.1, 2.1) ;
%
\draw[very thick,->] (8, 2)  -- (3.1, 0.1) ;
\draw[very thick,->] (8, 0)  -- (3.1, 0) ;
\draw[very thick,->] (8, 4)  -- (3, 0.2) ;
%
\draw[very thick,->] (8, 2)  -- (3.1, 3.9) ;
\draw[very thick,->] (8, 0)  -- (3, 3.8) ;
\draw[very thick,->] (8, 4)  -- (3.1, 4) ;
%%%%%% The edges represents their dependency relations GROUP 4
% \draw[very thick,->] (0, 0)  -- (6, 2) ;
\draw[very thick,->] (12, 2)  -- (8.1, 2) ;
\draw[very thick,->] (12, 0)  -- (8.1, 1.9) ;
\draw[very thick,->] (12, 4)  -- (8.1, 2.1) ;
%
\draw[very thick,->] (12, 2)  -- (8.1, 0.1) ;
\draw[very thick,->] (12, 0)  -- (8.1, 0) ;
\draw[very thick,->] (12, 4)  -- (8, 0.2) ;
%
\draw[very thick,->] (12, 2)  -- (8.1, 3.9) ;
\draw[very thick,->] (12, 0)  -- (8, 3.8) ;
\draw[very thick,->] (12, 4)  -- (8.1, 4) ;
%
%%%% The longest path representing the adaptivity
\draw[ultra thick, red, ->, dashed] (3, 4.1)  -- (0.1, 4.1);
\draw[ultra thick, red, ->, dashed] (8, 4.1)  -- (3.1, 4.1);
\draw[ultra thick, red, ->, dashed] (12, 4.1)  -- (8.1, 4.1);
\end{tikzpicture}
\end{center}
}
\end{example}
%%
%%
\section{Non Determinism}
%%
\paragraph{Non-Determinism of queries.}
% 
{
When evaluating a query $\query(\qval)$ on a given database $D$, 
in addition to obtain a result $v$ from the database $v = \query(\qval)(D)$,
we assume there is an underlying mechanism that will perform extra manipulations on $v$. 
The mechanism is considered as primitive operations in our language, behaving as black box to programmers.
There are different kinds of mechanisms, 
such as adding noise sampled from certain probabilistic distribution to the result \cite{dwork2015preserving}.
Because of the randomness of the underlying mechanism, the evaluation of a query $\query(\qval)$ is non-deterministic. 
That's the reason, in the Definition \ref{def:query_dep}, given a fixed database $D$, there will be a query domain $\qdom$ where $\query(\qval)(D) $ can be evaluated to different values $v \in \qdom$.
}
\\
{
On the other hand, in the operational semantics rule \textbf{query-v}:
\[
		\inferrule
	{
	\query(\qval) = v
	}
	{
	\config{m, [\assign{x}{\query(\qval)}]^l, t, w} 
	\xrightarrow{} 
	\config{m[ v/ x], \eskip,  (t ++ [(\qval, l, w)],w }
	}
	~\textbf{query-v}
	\]
, we evaluate the query given database $D$ based on an assumption that the underlying mechanism is fixed.
This fixed mechanism only adds constant $0$ to the original result $v$ returned from the database, i.e., $v = \query(\qval)(D)$. 
}
%
\\
%
The Lemma \ref{lem:semidetrm} and \ref{lem:querysemidetrm} formalize this property.
%
\begin{lem}
[Semi-Determinism].
\label{lem:semidetrm}\\
{
for any program $c$ with a starting memory $m$, trace $t$ and while label $w$, 
if program $c$ contains neither  
$[\assign{x}{\query(\qexpr)}]^l$ nor $[\assign{x}{\query(\qval)}]^l$ for any $\qexpr$ and $\qval$, then
%
$$
\bigwedge
\left\{\begin{array}{l}
\config{m, c, t, w} 
\rightarrow^{*} 
\config{m_1, \eskip, t_1, w_1} 
\\ 
\config{m, c, t, w} 
\rightarrow^{*} 
\config{m_2, \eskip, t_2, w_2} 
\end{array}
\right\}
\implies
(m_1 = m_2 \land t_1 = t_2 \land w_1 = w_2)
 $$ 
}
\end{lem}
%
\begin{proof}
{
Proof is obvious by induction on the operational semantics rules.
}
\end{proof}
%
%
\begin{lem}
[Query Semi-Determinism].
\label{lem:querysemidetrm}
\\
{
Given a program $c; \assign{x}{\query(\qexpr)}; c'$ with a starting memory $m$, trace $t$ and while label $w$, 
s.t. $c$ contains neither  
$[\assign{x}{\query(\qexpr)}]^l$ nor $[\assign{x}{\query(\qval)}]^l$ for any $\qexpr$ and $\qval$, then:
%
\[
\bigwedge
\left\{
\begin{array}{l}
\config{m, c; \assign{x}{\query(\qexpr)}; c', t, w} 
\rightarrow^{*} 
\config{m_1, \assign{x}{\query(\qval_1)}; c', t_1, w_1} 
\\
\config{m, c; \assign{x}{\query(\qexpr)}; c', t, w} 
\rightarrow^{*} 
\config{m_2, \assign{x}{\query(\qval_2)}; c', t_2, w_2} 
\end{array}
\right\}
\implies
(\qval_1 = \qval_2 \land m_1 = m_2 \land t_1 = t_2 \land w_1 = w_2)
\]
}
\end{lem}
%
\begin{proof}
{
Proof is obvious by induction on the operational semantics rules.
}
\end{proof}
% %
\section{Analysis of Generalization Error}

\begin{example}[Two Round Algorithm]
\[
TR^H(k) \triangleq
{
\begin{array}{l}
    % \left[j \leftarrow 0 \right]^1 ; \\
    \clabel{a_1 \leftarrow [] }^1; \\
    \eloop ~ [k]^{2} ~ (a_2 \leftarrow f(1, a_1, a_3)) \\
    ~ \edo ~ \\
    \Big(
     \clabel{x_1 \leftarrow \query() }^3 ; \\
    \clabel{a_3 \leftarrow x_1 :: a_2 }^4     \Big);\\
    \clabel{l \leftarrow q_{k + 1}(a_3)}^{5}\\
\end{array}
}
\]
\end{example}
%
\begin{example}[Multi-Round Algorithm]
\[
MR^H \triangleq
\begin{array}{l}
    %  \left[j \leftarrow 0 \right]^1 ; \\
    \left[I_2 \leftarrow [] \right]^1; \\
    \eloop ~ [k]^{2} ~ (I_2 \leftarrow f(2, I_1, I_3)) \\ 
    \ ~ \edo ~ \\ \Big(
    \left[p_1 \leftarrow c \right]^3 ; \\
    \left[a_1 \leftarrow \delta(\query(p, I_2)) \right]^4; \\
    \left[I_3 \leftarrow \eupdt( {I_2}, (a_1, p))  \right]^5
    \Big) 
\end{array}
\]
\end{example}
%
%
By applying different mechanisms $\delta()$ over the queries $\query(\cdot)$, we have different error bounds.
\\
\textbf{Gaussian Mechanism:} $N(0, \sigma)$ \cite{dwork2015preserving}:
\\
Adaptivity $r = 2$: 
$ \sigma = O \left(\frac{\sqrt{r \log(k)}}{\sqrt{n}} \right)$ (also known as expected error);
\\
Adaptivity unknown:
$ \sigma = O\left(\frac{\sqrt[4]{k}}{\sqrt{n}} \right)$;
\\
{Mean Squared Error Bound:} 
$ \frac{1}{2n} \min\limits_{\lambda \in [0, 1]}
\left( \frac{2\rho k n - \ln(1 - \lambda)}{\lambda} 
\right)
+ 2 \mathbb{E}_{Z_i \sim N(0, \frac{1}{2n^2 \rho})}
\left[ \max\limits_{i \in [k]} (Z_i^2) \right]$
%
\\
{Confidence Bounds:} minimize $\tau$ where
$\tau \geq \sqrt{\frac{2}{n \beta}
\min\limits_{\lambda \in [0, 1]}
\left( \frac{2\rho k n - \ln(1 - \lambda)}{\lambda} 
\right)
}$
and 
$\tau \geq \frac{2}{n} \sqrt{\frac{\ln(4n /\beta}{\rho'}}$ with confidence level $1 - \beta$ .
\\
\textbf{$(\epsilon, \delta)-DP$ mechanism}:
\\
Confidence Bounds:
$\tau \geq \sqrt{\frac{48}{n} \ln(4/\beta) }$ with $\epsilon \leq \frac{\tau}{4}$ and $\delta = 
\exp \left(\frac{-4 \ln (8/\beta)}{\tau} \right)$
\\
\textbf{Sample Splitting}: 
\\
Expected Error: $O \left(\frac{\sqrt{k \log(k)}}{\sqrt{n}} \right)$
\\
\textbf{Thresholdout}: $B, \sigma, T, h$ 
\\
Confidence bounds:  
$\tau = \max\limits\left\{ 
\sqrt{\frac{2\zeta }{h \beta}},
2\sigma \ln(\frac{\beta}{2}),
\sqrt{\frac{1}{\beta}} \cdot \left(\sqrt{T^2 + 56\sigma^2} + \sqrt{\frac{\zeta}{4h} } \right)
\right\}
$,
for $\zeta = \min\limits_{\lambda \in [0, 1)}
\left( \frac{2B \ (\sigma^2 h) - \ln(1 - \lambda)}{\lambda} \right)$

\clearpage


% \input{new-algo}



\newpage
\bibliographystyle{plain}
\bibliography{adaptivity.bib}

\end{document}



