%
\begin{example}[Linear Regression Algorithm with Gradient Decent Optimization]
\label{ex:linearregression}
Our $\THESYSTEM$ works well also over the linear regression algorithm with gradient decent optimization method.
\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
    \centering
    {\small
        \[
        \begin{array}{l}
            \kw{lR(k, r)} \triangleq \\
                   \clabel{ a \leftarrow 0}^{0} ; 
                   \clabel{ c \leftarrow 0}^{1} ; 
                    \clabel{\assign{j}{\kw{k}} }^{2} ; \\
                    \ewhile ~ \clabel{j > 0}^{3} ~ \edo ~ \\
                    \Big(
                        \clabel{\assign{da}{\query(-2 * (\chi[1] - (\chi[0]\times a + c)) \times (\chi[0]))} }^{4}  ; \\
                        \clabel{\assign{dc}{\query(-2 * (\chi[1] - (\chi[0]\times a + c)))} }^{5}  ; \\
                        \clabel{\assign{a}{a - \kw{r} * da} }^{6}  ; 
                        \clabel{\assign{c}{c - \kw{r} * dc} }^{7}  ; \\
                     \clabel{\assign{j}{j-1}}^{8} 
                    \Big);
                \end{array}
        \]
        }
     \caption{}
        \end{subfigure}
        \begin{subfigure}{.55\textwidth}
            \begin{centering}
            \begin{tikzpicture}[scale=\textwidth/24cm,samples=200]
    % Variables Initialization
    \draw[] (-6, 1) circle (0pt) node{{ $a^0: {}^1_{0}$}};
    \draw[] (-6, 4) circle (0pt) node{{ $c^1: {}^{1}_{0}$}};
    % Variables Inside the Loop
         \draw[] (0, 10) circle (0pt) node{{ $da^4: {}^{k}_{1}$}};
         \draw[] (0, 7) circle (0pt) node{{ $dc^5: {}^{k}_{0}$}};
         \draw[] (0, 4) circle (0pt) node{{ $a^6: {}^{k}_{0}$}};
         \draw[] (0, 1) circle (0pt) node{{ $c^7: {}^{k}_{0}$}};
         % Counter Variables
         \draw[] (7, 9) circle (0pt) node {{$j^0: {}^{1}_{0}$}};
         \draw[] (7, 6) circle (0pt) node {{ $j^8: {}^{k}_{0}$}};
         %
         % Value Dependency Edges:
         \draw[ thick, -latex,] (0, 1.5)  -- (0, 3.5) ;
         \draw[ thick, -Straight Barb] (1.8, 4.2) arc (220:-100:1);
         \draw[ thick, -Straight Barb] (7.5, 6.5) arc (150:-150:1);
         \draw[ thick, -latex] (6, 6.5)  -- (6, 8.5) ;
         \draw[ thick, -Straight Barb] (1.7, 1.) arc (120:-200:1);
         % Value Dependency Edges on Initial Values:
         \draw[ thick, -latex,] (-2, 1)  -- (-4.5, 1) ;
         \draw[ thick, -latex,] (-2, 4)  -- (-4.5, 4) ;
         %
         \draw[ ultra thick, -latex, densely dotted,] (-1, 1.5)  to  [out=-220,in=220]  (-1, 6.5);
         \draw[ ultra thick, -latex, densely dotted,] (-1, 4.5)  to  [out=-220,in=220]  (-1, 9.5);
         \draw[ ultra thick, -latex, densely dotted,]  (1, 6.2) to  [out=-60,in=60] (0.5, 1.5) ;
         \draw[ ultra thick, -latex, densely dotted,]  (1.2, 9.2)  to  [out=-50,in=50] (0.5, 4.5);
         \draw[ thick,-latex] (1.8, 7)  -- (5.5, 6) ;
         \draw[ thick,-latex] (1.8, 4)  -- (5.5, 6) ;
         \draw[ thick,-latex] (1.8, 1)  -- (5.5, 6) ;
         \draw[ thick,-latex] (1.8, 10)  -- (5.5, 6) ;
         \end{tikzpicture}
         \caption{}
            \end{centering}
            \end{subfigure}
    \vspace{-0.5cm}
    \caption{(a) The linear regression algorithm 
    (b) The estimated dependency graph by $\THESYSTEM$}
    \vspace{-0.5cm}
    \label{fig:linear_regression}
\end{figure}
%
It computes $ \progA(\kw{lR(k, r)}) = k$.
\end{example} 
%
This linear regression algorithm 
% in order to
aims to
find a linear relationship, $y = a \times x + c$ between a dependent variable $y$
and an independent variable $x$, by approximating the 
model parameter $a$ and $c$.
In order to have a good approximation on the model parameter 
$a$ and $c$, 
% corresponding to the observed value in the column $\chi[0]$ in database, 
it sends query to a training data set adaptively in each iteration.
This training data set contains two columns (can extend to higher dimensional data sets), 
the first column contains the observed values of the independent variable $x$ and the
second column for the dependent variable $y$.
$\kw{lR(k, r)}$ is the program of this example written in our language model in Figure~\ref{fig:linear_regression}(a)
with input variables $k$ and $r$.

$\kw{lR(k, r)}$ starts from initializing the linear model parameters and the counter variable in commands $0, 1, 2$,
and then goes into the while iterations.
In each iteration, it computes the differential value w.r.t. parameter
$a$ and $c$,
through two query requests, 
$\query(-2 * (\chi[1] - (\chi[0]\times a + c)) \times (\chi[0]))$ and 
$\query(-2 * (\chi[1] - (\chi[0]\times a + c)))$
in commands of label $4$ and $5$.
Then, it uses these two differential values stored in variables $da$ and $dc$
to update the model parameters $a$ and $c$.
In this sense, the two query requests in each iteration depends on queries in the previous iterations and the intuitive adaptivity rounds is $k$.
%
Its program-based dependency graph, $\progG(\kw{lR(k, r)})$ is shown in Figure~\ref{fig:linear_regression}(b),
and $\traceG(\kw{lR(k, r)})$ is omitted for the same reason as Example~\ref{ex:multipleRounds}.
In Figure~\ref{fig:linear_regression}(b), we omit the edges which are constructed by the transition of $\flowsto$ relation
for concise, but these edges exist in $\traceG(\kw{lR(k, r)})$  because they can be constructed directly by $\vardep$ relation.

Given an input $\trace_0$, there are multiple walks having the same longest query length in 
$\traceG(\kw{lR(k, r)})$,
such as the walks 
$c^7 \to dc^5 : \to c^7 \to \cdots \to dc^5$ and
$a^6 \to da^4  \to a^6 \to \cdots \to da^4 $ along the 
dotted arrows.
The vertices $c^7, dc^5, a^6, da^4$ in the two walks are visited $w_{c^7}, w_{dc^5}, w_{a^6}, w_{da^4}$ respectively.
Though the different weight functions count the execution times of the different corresponding command,
the counts are expected to be the same number, i.e., the loop iterations.
And the loop iterations are indeed the initial value of input $k$ from initial trace $\trace_0$.
In this sense, $A(\kw{lR(k, r)})(\trace_0) = w_{dc^5}(\trace_0) = w_{da^4}(\trace_0)$.
Similar to Example~\ref{ex:multipleRounds}, $\THESYSTEM$ estimates the tight adaptivity bound, $k$ for this example.

%